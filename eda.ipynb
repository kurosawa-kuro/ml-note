{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "uch3chw3wf",
   "source": "# 🏆 CMI BFRB Detection - EDA総括\n\n## 📊 重要な発見事項\n\n### ✅ データ品質\n- **高品質IMUデータ**: 加速度センサーは欠損値0%、信頼性の高いベースライン\n- **部分的センサー故障**: ToF_5とthm_5が5%以上欠損 → 補完戦略必要\n- **時系列連続性**: シーケンス内でギャップなし、50Hz一定サンプリング\n\n### 🎯 ターゲット変数特性\n- **バイナリ分類**: Target/Non-Target = 60/40 → Binary F1は達成可能\n- **マルチクラス分類**: 18ジェスチャー、6:1の不均衡 → Macro F1が困難\n- **時系列構造**: 平均1.4秒の短いシーケンス → ジェスチャー認識タスク\n\n### 👥 参加者データ\n- **完全分離**: 訓練・テスト間で参加者重複なし ✅\n- **均等分布**: 参加者間データ量は比較的バランス良好\n- **全ジェスチャーカバー**: 全参加者が全ジェスチャータイプを実行\n\n### 🔗 センサー相関\n- **モダリティ間低相関**: IMU/ToF/温度は補完的情報を提供\n- **マルチコリニアリティ**: 深刻な問題なし、融合に適している\n\n---\n\n## 🎯 コンペティション戦略\n\n### 📈 目標スコア\n- **現実的目標**: Combined F1 = 0.60-0.65 (銅メダル圏)\n- **Binary F1**: 0.65-0.70 (比較的達成しやすい)\n- **Macro F1**: 0.55-0.65 (18クラス不均衡で困難)\n\n### 🏗️ 推奨アプローチ\n\n#### Phase 1: ベースライン構築 (Week 1)\n1. **GroupKFold CV** セットアップ (participant-based)\n2. **基本特徴量**: IMU magnitude, rolling statistics\n3. **LightGBM** with missing value handling\n4. **目標**: CV 0.50+, LB 0.50+\n\n#### Phase 2: 特徴工学 (Week 2-3)\n1. **FFT spectrum** features for IMU\n2. **ToF PCA** dimensionality reduction  \n3. **Multimodal fusion** features\n4. **1D CNN** on raw sensor streams\n5. **目標**: CV 0.58+, LB 0.57+\n\n#### Phase 3: モデル最適化 (Week 4-5)\n1. **Multi-branch CNN** (IMU/ToF/Thermopile separate)\n2. **Ensemble** multiple models\n3. **Hyperparameter tuning**\n4. **目標**: CV 0.62+, LB 0.60+ (銅メダル)\n\n---\n\n## ⚠️ 重要な注意点\n\n### 🚨 リスク要因\n1. **Macro F1 difficulty**: 18クラス不均衡により0.5以下の可能性\n2. **Sensor 5 missing**: ToF_5/thm_5欠損による情報損失  \n3. **CV-LB gap**: 人ベースGroupKFoldでズレ可能性\n\n### 🛡️ 対策\n1. **クラス重み調整**: Macro F1向上のためfocal loss等\n2. **欠損値戦略**: imputation + availability indicators\n3. **CV robustness**: 複数シード、fold分散監視\n\n---\n\n## 🎯 Next Steps\n\n1. **特徴工学パイプライン** 実装開始\n2. **GroupKFold CV** 環境構築\n3. **ベースラインモデル** (tsfresh + LightGBM)\n4. **進捗モニタリング** システム構築\n\n**期待される成果**: 適切な特徴工学とCV戦略により、銅メダル圏内(top 200)到達可能",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ri4lh023h7",
   "source": "# 10. 🔧 特徴工学推奨事項\nprint(\"🔧 10. FEATURE ENGINEERING RECOMMENDATIONS\")\nprint(\"=\" * 60)\n\nprint(\"📊 Based on the comprehensive EDA analysis, here are the recommended feature engineering strategies:\")\n\nprint(\"\\n🎯 1. IMU FEATURE ENGINEERING\")\nprint(\"=\" * 40)\nprint(\"✅ Magnitude Features:\")\nprint(\"  • acc_magnitude = sqrt(acc_x² + acc_y² + acc_z²)\")\nprint(\"  • rot_magnitude = sqrt(rot_x² + rot_y² + rot_z²)\")\nprint(\"  • Remove gravity component: acc_no_gravity = acc - [0, 0, 9.81]\")\n\nprint(\"\\n✅ Temporal Features:\")\nprint(\"  • Velocity: diff(acc_x), diff(acc_y), diff(acc_z)\")\nprint(\"  • Jerk: diff(diff(acc_x)) - 2nd derivative\")\nprint(\"  • Rolling statistics: mean, std, min, max over windows (5, 10, 20 timesteps)\")\n\nprint(\"\\n✅ Frequency Domain:\")\nprint(\"  • FFT features: spectral energy, dominant frequency, spectral centroid\")\nprint(\"  • Frequency band powers: 0-2Hz, 2-5Hz, 5-10Hz, 10-25Hz\")\nprint(\"  • Spectral entropy and spectral rolloff\")\n\nprint(\"\\n🌡️ 2. THERMOPILE FEATURE ENGINEERING\") \nprint(\"=\" * 40)\nprint(\"✅ Spatial Features:\")\nprint(\"  • Temperature gradients: thm_1 - thm_3, thm_2 - thm_4\")\nprint(\"  • Temperature range: max(thm_1..4) - min(thm_1..4)\")\nprint(\"  • Centroid calculation: weighted average position\")\n\nprint(\"\\n✅ Handle Missing thm_5:\")\nprint(\"  • Create binary indicator: thm_5_available\")\nprint(\"  • Fill with median of thm_1..4 when missing\")\nprint(\"  • Separate model branch for thm_5 vs thm_1..4\")\n\nprint(\"\\n📡 3. TOF FEATURE ENGINEERING\")\nprint(\"=\" * 40)\nprint(\"✅ Dimensionality Reduction:\")\nprint(\"  • PCA on 64 channels → 8-16 components per ToF sensor\")\nprint(\"  • Statistical summaries: mean, std, min, max, median per sensor\")\nprint(\"  • Distance gradients: edge detection on 8x8 ToF array\")\n\nprint(\"\\n✅ Proximity Features:\")\nprint(\"  • Minimum distance per sensor: min(tof_N_v0..63)\")\nprint(\"  • Distance variance: std(tof_N_v0..63)\")\nprint(\"  • Hand-to-face proximity: tof_1 vs tof_3 comparison\")\n\nprint(\"\\n✅ Handle Missing tof_5:\")\nprint(\"  • Binary indicator: tof_5_available\")\nprint(\"  • Zero-fill or interpolate from tof_1..4 spatial patterns\")\n\nprint(\"\\n🔄 4. MULTIMODAL FUSION FEATURES\")\nprint(\"=\" * 40)\nprint(\"✅ Cross-Modal Correlations:\")\nprint(\"  • IMU-Temperature sync: correlation(acc_magnitude, thm_mean)\")\nprint(\"  • Motion-Proximity sync: correlation(acc_jerk, tof_min_distance)\")\nprint(\"  • Activity level: high_motion × high_temperature\")\n\nprint(\"\\n✅ Temporal Alignment:\")\nprint(\"  • Lag features: temperature[t-1], tof[t-1] vs acc[t]\")\nprint(\"  • Lead features: predict next timestep behavior\")\nprint(\"  • Sliding window features: past 5-10 timesteps context\")\n\nprint(\"\\n⏱️ 5. TIME SERIES SPECIFIC FEATURES\")\nprint(\"=\" * 40)\nprint(\"✅ Sequence-Level Features:\")\nprint(\"  • Sequence statistics: length, start/end values, trend\")\nprint(\"  • Phase transitions: count of behavior changes per sequence\")\nprint(\"  • Gesture duration: timesteps in 'Performs gesture' phase\")\n\nprint(\"\\n✅ Temporal Context:\")\nprint(\"  • Position in sequence: timestep / sequence_length\")\nprint(\"  • Time since behavior change\")\nprint(\"  • Behavior transition indicators\")\n\nprint(\"\\n🎭 6. GESTURE-SPECIFIC FEATURES\")\nprint(\"=\" * 40)\nprint(\"✅ BFRB-Relevant Features:\")\nprint(\"  • Repetitive motion detection: autocorrelation, periodicity\")\nprint(\"  • Hand-to-face distance (ToF sensors)\")\nprint(\"  • Fidgeting indicators: high-frequency low-amplitude motion\")\nprint(\"  • Touch detection: temperature spikes + proximity changes\")\n\nprint(\"\\n👥 7. PARTICIPANT-AWARE FEATURES\")\nprint(\"=\" * 40) \nprint(\"✅ Normalization by Demographics:\")\nprint(\"  • Height-normalized features: distances / height\")\nprint(\"  • Age-adjusted motion thresholds\")\nprint(\"  • Handedness-aware spatial features\")\n\nprint(\"\\n✅ Subject-Specific Calibration:\")\nprint(\"  • Z-score normalization per participant\")\nprint(\"  • Baseline subtraction: first N timesteps as reference\")\nprint(\"  • Participant-specific gesture templates\")\n\nprint(\"\\n🏗️ 8. IMPLEMENTATION PRIORITY\")\nprint(\"=\" * 40)\nprint(\"Priority 1 (Essential):\")\nprint(\"  1. IMU magnitude + derivatives (velocity, jerk)\")\nprint(\"  2. Rolling window statistics (mean, std over 5-20 timesteps)\")\nprint(\"  3. Missing value indicators + imputation\")\nprint(\"  4. GroupKFold cross-validation setup\")\n\nprint(\"\\nPriority 2 (High Impact):\")\nprint(\"  5. ToF PCA + statistical summaries\")\nprint(\"  6. Thermopile spatial gradients\")\nprint(\"  7. Sequence-level contextual features\")\nprint(\"  8. FFT spectral features\")\n\nprint(\"\\nPriority 3 (Optimization):\")\nprint(\"  9. Cross-modal correlation features\")\nprint(\"  10. Participant-specific normalization\")\nprint(\"  11. Advanced temporal patterns\")\nprint(\"  12. Gesture-specific domain features\")\n\nprint(\"\\n🎯 Expected Impact on Competition Metrics:\")\nprint(\"  • Binary F1: Should improve from current ~0.60 to 0.65-0.70\")\nprint(\"  • Macro F1: Harder due to class imbalance, expect 0.55-0.65\")\nprint(\"  • Combined Score: Target 0.60-0.68 (bronze medal territory)\")\n\nprint(\"\\n\" + \"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "fj17nr6109k",
   "source": "# 9. ✅ クロスバリデーション戦略検証\nprint(\"✅ 9. CROSS-VALIDATION STRATEGY VALIDATION\")\nprint(\"=\" * 60)\n\n# Close existing connection if it exists and create new one\ntry:\n    conn.close()\nexcept:\n    pass\n\nimport duckdb\nconn = duckdb.connect('/home/wsl/dev/my-study/ml/ml-note/kaggle_datasets.duckdb')\n\n# 参加者データリーク検証\nprint(\"👥 Participant Data Leakage Validation:\")\n\n# 参加者IDの完全性チェック\nparticipant_integrity = conn.execute(\"\"\"\n    SELECT \n        'Train' as dataset,\n        COUNT(DISTINCT subject) as unique_participants,\n        MIN(subject) as min_subject_id,\n        MAX(subject) as max_subject_id\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    \n    UNION ALL\n    \n    SELECT \n        'Test',\n        COUNT(DISTINCT subject),\n        MIN(subject),\n        MAX(subject)\n    FROM \"cmi_detect_behavior_with_sensor_data\".test\n\"\"\").fetchdf()\n\nprint(participant_integrity.to_string(index=False))\n\n# 参加者重複チェック（より詳細）\noverlap_detailed = conn.execute(\"\"\"\n    SELECT \n        tr.subject as train_subject,\n        te.subject as test_subject,\n        'OVERLAP_DETECTED' as status\n    FROM (SELECT DISTINCT subject FROM \"cmi_detect_behavior_with_sensor_data\".train) tr\n    INNER JOIN (SELECT DISTINCT subject FROM \"cmi_detect_behavior_with_sensor_data\".test) te\n    ON tr.subject = te.subject\n\"\"\").fetchdf()\n\nif len(overlap_detailed) > 0:\n    print(f\"⚠️  CRITICAL: Found {len(overlap_detailed)} overlapping participants!\")\n    print(overlap_detailed.to_string(index=False))\nelse:\n    print(\"✅ VERIFIED: No participant overlap between train and test sets\")\n\n# 参加者別データ分布（CV設計用）\nprint(\"\\n📊 Participant Data Distribution for CV Design:\")\nparticipant_distribution = conn.execute(\"\"\"\n    SELECT \n        subject,\n        COUNT(DISTINCT sequence_id) as sequences,\n        COUNT(DISTINCT gesture) as unique_gestures,\n        COUNT(*) as total_timesteps,\n        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".train), 2) as data_percentage\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    GROUP BY subject\n    ORDER BY total_timesteps DESC\n    LIMIT 10\n\"\"\").fetchdf()\n\nprint(\"Top 10 participants by data volume:\")\nprint(participant_distribution.to_string(index=False))\n\n# データ分布の均等性評価\ndistribution_stats = conn.execute(\"\"\"\n    SELECT \n        COUNT(DISTINCT subject) as total_participants,\n        ROUND(AVG(timesteps_per_participant), 1) as avg_timesteps,\n        ROUND(STDDEV(timesteps_per_participant), 1) as std_timesteps,\n        ROUND(MIN(timesteps_per_participant), 1) as min_timesteps,\n        ROUND(MAX(timesteps_per_participant), 1) as max_timesteps,\n        ROUND(MAX(timesteps_per_participant) / MIN(timesteps_per_participant), 1) as imbalance_ratio\n    FROM (\n        SELECT subject, COUNT(*) as timesteps_per_participant\n        FROM \"cmi_detect_behavior_with_sensor_data\".train\n        GROUP BY subject\n    )\n\"\"\").fetchone()\n\nprint(f\"\\n📈 Participant Data Balance:\")\nprint(f\"  Total participants: {distribution_stats[0]}\")\nprint(f\"  Timesteps per participant: mean±std = {distribution_stats[1]}±{distribution_stats[2]}\")\nprint(f\"  Range: {distribution_stats[3]} - {distribution_stats[4]}\")\nprint(f\"  Imbalance ratio: {distribution_stats[5]}:1\")\n\nbalance_status = \"🟢 WELL BALANCED\" if distribution_stats[5] < 5 else \"🟡 MODERATE IMBALANCE\" if distribution_stats[5] < 10 else \"🔴 HIGHLY IMBALANCED\"\nprint(f\"  Status: {balance_status}\")\n\n# ジェスチャー×参加者のカバレッジ分析\nprint(\"\\n🎭 Gesture Coverage by Participant (CV Stratification Check):\")\ngesture_coverage = conn.execute(\"\"\"\n    SELECT \n        gesture,\n        COUNT(DISTINCT subject) as participants_with_gesture,\n        ROUND(COUNT(DISTINCT subject) * 100.0 / 81, 1) as coverage_percentage,\n        COUNT(*) as total_samples\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    GROUP BY gesture\n    ORDER BY participants_with_gesture DESC\n\"\"\").fetchdf()\n\nprint(gesture_coverage.to_string(index=False))\n\n# CV フォールド設計の推奨\nprint(\"\\n🔄 Recommended CV Strategy:\")\n\n# 5-fold GroupKFold シミュレーション\nparticipants_per_fold = distribution_stats[0] / 5\ndata_per_fold = 100 / 5\n\nprint(f\"GroupKFold Configuration:\")\nprint(f\"  • Recommended folds: 5\")\nprint(f\"  • Participants per fold: ~{participants_per_fold:.0f}\")\nprint(f\"  • Expected data per fold: ~{data_per_fold:.0f}%\")\nprint(f\"  • Grouping variable: subject (participant_id)\")\n\n# 潜在的な問題の特定\npotential_issues = []\n\nif distribution_stats[5] > 10:\n    potential_issues.append(\"High participant data imbalance may cause uneven fold sizes\")\n\nif gesture_coverage['coverage_percentage'].min() < 80:\n    potential_issues.append(\"Some gestures appear in <80% of participants - may cause stratification issues\")\n\nif len(potential_issues) > 0:\n    print(f\"\\n⚠️  Potential CV Issues:\")\n    for issue in potential_issues:\n        print(f\"  • {issue}\")\n    print(f\"  • Recommendation: Monitor CV scores variance across folds\")\nelse:\n    print(f\"\\n✅ CV Strategy Looks Robust\")\n\n# 時系列特有の考慮事項\nprint(\"\\n⏰ Time Series Specific Considerations:\")\nprint(\"  ✅ Sequences are independent (no temporal continuity between sequences)\")\nprint(\"  ✅ Participant-level grouping prevents data leakage\")\nprint(\"  ⚠️  Consider sequence-level stratification if needed\")\nprint(\"  📝 Monitor for temporal drift within long sequences\")\n\n# 最終的なCV推奨\nprint(\"\\n🎯 Final CV Recommendation:\")\nprint(\"```python\")\nprint(\"from sklearn.model_selection import GroupKFold\")\nprint(\"\")\nprint(\"# Recommended configuration\")\nprint(\"cv = GroupKFold(n_splits=5)\")\nprint(\"groups = train_data['subject']  # participant IDs\")\nprint(\"\")\nprint(\"# Ensure no participant appears in both train and validation\")\nprint(\"for train_idx, val_idx in cv.split(X, y, groups):\")\nprint(\"    train_subjects = set(groups.iloc[train_idx])\")\nprint(\"    val_subjects = set(groups.iloc[val_idx])\")\nprint(\"    assert len(train_subjects & val_subjects) == 0\")\nprint(\"```\")\n\nprint(\"\\n\" + \"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ugz2uf6n5k",
   "source": "# 8. 📊 センサー融合可視化とマルチモーダル分析\nprint(\"📊 8. SENSOR FUSION VISUALIZATION & MULTIMODAL ANALYSIS\")\nprint(\"=\" * 60)\n\n# 代表的なシーケンスの取得と可視化\nprint(\"🎯 Sample Sequence Analysis for Visualization:\")\n\n# 興味深いシーケンスを選択（異なるジェスチャー）\nsample_sequences = conn.execute(\"\"\"\n    SELECT sequence_id, gesture, COUNT(*) as length\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    WHERE gesture IN ('Text on phone', 'Neck - scratch', 'Wave hello')\n    GROUP BY sequence_id, gesture\n    HAVING COUNT(*) BETWEEN 50 AND 100  -- 中程度の長さ\n    ORDER BY RANDOM()\n    LIMIT 3\n\"\"\").fetchdf()\n\nprint(\"Selected sequences for visualization:\")\nprint(sample_sequences.to_string(index=False))\n\n# マルチモーダルデータの同期性チェック\nprint(\"\\n🔄 Sensor Synchronization Analysis:\")\nsync_analysis = conn.execute(\"\"\"\n    SELECT \n        sequence_id,\n        COUNT(*) as total_timesteps,\n        COUNT(acc_x) as acc_available,\n        COUNT(rot_w) as rot_available,  \n        COUNT(thm_1) as thm_available,\n        COUNT(tof_1_v0) as tof_available,\n        ROUND(COUNT(acc_x) * 100.0 / COUNT(*), 1) as acc_coverage,\n        ROUND(COUNT(thm_1) * 100.0 / COUNT(*), 1) as thm_coverage,\n        ROUND(COUNT(tof_1_v0) * 100.0 / COUNT(*), 1) as tof_coverage\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    WHERE sequence_id IN (\n        SELECT sequence_id FROM \"cmi_detect_behavior_with_sensor_data\".train\n        GROUP BY sequence_id\n        ORDER BY RANDOM()\n        LIMIT 10\n    )\n    GROUP BY sequence_id\n    ORDER BY acc_coverage DESC\n    LIMIT 5\n\"\"\").fetchdf()\n\nprint(sync_analysis.to_string(index=False))\n\n# センサーモダリティ間の情報量分析\nprint(\"\\n📈 Information Content Analysis by Sensor Modality:\")\ninfo_analysis = conn.execute(\"\"\"\n    SELECT \n        'IMU_acceleration' as modality,\n        ROUND(STDDEV(acc_x), 4) as x_std,\n        ROUND(STDDEV(acc_y), 4) as y_std,\n        ROUND(STDDEV(acc_z), 4) as z_std,\n        ROUND(AVG(ABS(acc_x - LAG(acc_x) OVER (ORDER BY sequence_counter))), 4) as x_variability\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    WHERE acc_x IS NOT NULL\n    ORDER BY RANDOM()\n    LIMIT 10000\n\"\"\").fetchone()\n\nprint(f\"IMU Acceleration variability:\")\nprint(f\"  X-axis std: {info_analysis[1]}, variability: {info_analysis[4]}\")\nprint(f\"  Y-axis std: {info_analysis[2]}\")\nprint(f\"  Z-axis std: {info_analysis[3]}\")\n\n# ジェスチャー別のセンサー特性\nprint(\"\\n🎭 Sensor Characteristics by Gesture Type:\")\ngesture_sensor_profile = conn.execute(\"\"\"\n    SELECT \n        gesture,\n        COUNT(*) as samples,\n        ROUND(AVG(ABS(acc_x)), 3) as avg_acc_x_abs,\n        ROUND(AVG(ABS(acc_y)), 3) as avg_acc_y_abs,\n        ROUND(AVG(ABS(acc_z)), 3) as avg_acc_z_abs,\n        ROUND(AVG(thm_1), 2) as avg_thm_1,\n        ROUND(STDDEV(thm_1), 2) as std_thm_1\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    WHERE acc_x IS NOT NULL AND thm_1 IS NOT NULL\n    GROUP BY gesture\n    ORDER BY avg_acc_x_abs DESC\n    LIMIT 8\n\"\"\").fetchdf()\n\nprint(gesture_sensor_profile.to_string(index=False))\n\n# ToFセンサーの距離パターン分析\nprint(\"\\n📡 ToF Distance Pattern Analysis:\")\ntof_pattern = conn.execute(\"\"\"\n    SELECT \n        gesture,\n        COUNT(*) as valid_samples,\n        ROUND(AVG(tof_1_v0), 2) as avg_tof1_v0,\n        ROUND(AVG(tof_2_v0), 2) as avg_tof2_v0,\n        ROUND(AVG(tof_3_v0), 2) as avg_tof3_v0,\n        ROUND(STDDEV(tof_1_v0), 2) as std_tof1_v0,\n        ROUND(AVG(tof_1_v0 - tof_1_v31), 2) as tof1_gradient\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    WHERE tof_1_v0 IS NOT NULL AND tof_2_v0 IS NOT NULL AND tof_3_v0 IS NOT NULL\n    GROUP BY gesture\n    HAVING COUNT(*) > 1000\n    ORDER BY avg_tof1_v0 DESC\n    LIMIT 8\n\"\"\").fetchdf()\n\nprint(tof_pattern.to_string(index=False))\n\n# 実際の可視化データの準備\nprint(\"\\n📊 Preparing Visualization Data:\")\nviz_data = conn.execute(f\"\"\"\n    SELECT \n        sequence_counter,\n        acc_x, acc_y, acc_z,\n        rot_w, rot_x, rot_y, rot_z,\n        thm_1, thm_2, thm_3,\n        tof_1_v0, tof_1_v31, tof_1_v63,\n        gesture, behavior, phase\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    WHERE sequence_id = '{sample_sequences.iloc[0]['sequence_id']}'\n    ORDER BY sequence_counter\n    LIMIT 100\n\"\"\").fetchdf()\n\nprint(f\"Visualization data prepared: {len(viz_data)} timesteps\")\nprint(f\"Gesture: {viz_data['gesture'].iloc[0]}\")\nprint(f\"Data coverage:\")\nprint(f\"  IMU: {viz_data['acc_x'].notna().sum()}/{len(viz_data)} timesteps\")\nprint(f\"  Thermopile: {viz_data['thm_1'].notna().sum()}/{len(viz_data)} timesteps\") \nprint(f\"  ToF: {viz_data['tof_1_v0'].notna().sum()}/{len(viz_data)} timesteps\")\n\n# センサー融合の可能性評価\nprint(\"\\n🔗 Sensor Fusion Potential Assessment:\")\nfusion_metrics = {\n    'temporal_alignment': sync_analysis['acc_coverage'].mean(),\n    'cross_modal_correlation': abs(cross_correlations[0]),  # 前のセルから\n    'complementary_info': 1 - abs(cross_correlations[0]),  # 相関が低いほど補完的\n    'missing_data_overlap': missing_cooccurrence.iloc[0]['percentage']  # 前のセルから\n}\n\nprint(f\"Fusion readiness metrics:\")\nprint(f\"  ✅ Temporal alignment: {fusion_metrics['temporal_alignment']:.1f}%\")\nprint(f\"  🔄 Cross-modal correlation: {fusion_metrics['cross_modal_correlation']:.3f}\")\nprint(f\"  🎯 Complementary information: {fusion_metrics['complementary_info']:.3f}\")\nprint(f\"  ⚠️  Missing data overlap: {fusion_metrics['missing_data_overlap']:.1f}%\")\n\nfusion_score = (fusion_metrics['temporal_alignment']/100 + \n                fusion_metrics['complementary_info'] + \n                (1 - fusion_metrics['missing_data_overlap']/100)) / 3\n\nprint(f\"\\n🏆 Overall Fusion Potential Score: {fusion_score:.2f}/1.0\")\nprint(f\"   {'🟢 EXCELLENT' if fusion_score > 0.8 else '🟡 GOOD' if fusion_score > 0.6 else '🔴 CHALLENGING'}\")\n\nprint(\"\\n\" + \"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "hw7ltqem8i7",
   "source": "# 7. 🔗 特徴相関分析とマルチコリニアリティチェック\nprint(\"🔗 7. FEATURE CORRELATION & MULTICOLLINEARITY ANALYSIS\")\nprint(\"=\" * 60)\n\n# IMUセンサー間の相関分析（サンプルデータ）\nprint(\"🎯 IMU Sensor Correlations (Sample Data):\")\nimu_correlations = conn.execute(\"\"\"\n    SELECT \n        ROUND(CORR(acc_x, acc_y), 3) as acc_x_y_corr,\n        ROUND(CORR(acc_x, acc_z), 3) as acc_x_z_corr,\n        ROUND(CORR(acc_y, acc_z), 3) as acc_y_z_corr,\n        ROUND(CORR(rot_w, rot_x), 3) as rot_w_x_corr,\n        ROUND(CORR(rot_w, rot_y), 3) as rot_w_y_corr,\n        ROUND(CORR(rot_w, rot_z), 3) as rot_w_z_corr\n    FROM (\n        SELECT acc_x, acc_y, acc_z, rot_w, rot_x, rot_y, rot_z\n        FROM \"cmi_detect_behavior_with_sensor_data\".train\n        WHERE acc_x IS NOT NULL AND rot_w IS NOT NULL\n        ORDER BY RANDOM()\n        LIMIT 50000  -- サンプリングして計算負荷を軽減\n    )\n\"\"\").fetchone()\n\nprint(f\"Acceleration correlations:\")\nprint(f\"  acc_x vs acc_y: {imu_correlations[0]}\")\nprint(f\"  acc_x vs acc_z: {imu_correlations[1]}\")\nprint(f\"  acc_y vs acc_z: {imu_correlations[2]}\")\n\nprint(f\"\\nRotation correlations:\")\nprint(f\"  rot_w vs rot_x: {imu_correlations[3]}\")\nprint(f\"  rot_w vs rot_y: {imu_correlations[4]}\")\nprint(f\"  rot_w vs rot_z: {imu_correlations[5]}\")\n\n# 温度センサー間の相関\nprint(\"\\n🌡️ Thermopile Sensor Correlations:\")\nthm_correlations = conn.execute(\"\"\"\n    SELECT \n        ROUND(CORR(thm_1, thm_2), 3) as thm_1_2_corr,\n        ROUND(CORR(thm_1, thm_3), 3) as thm_1_3_corr,\n        ROUND(CORR(thm_1, thm_4), 3) as thm_1_4_corr,\n        ROUND(CORR(thm_2, thm_3), 3) as thm_2_3_corr,\n        ROUND(CORR(thm_2, thm_4), 3) as thm_2_4_corr,\n        ROUND(CORR(thm_3, thm_4), 3) as thm_3_4_corr\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    WHERE thm_1 IS NOT NULL AND thm_2 IS NOT NULL AND thm_3 IS NOT NULL AND thm_4 IS NOT NULL\n\"\"\").fetchone()\n\nprint(f\"thm_1 vs thm_2: {thm_correlations[0]}\")\nprint(f\"thm_1 vs thm_3: {thm_correlations[1]}\")\nprint(f\"thm_1 vs thm_4: {thm_correlations[2]}\")\nprint(f\"thm_2 vs thm_3: {thm_correlations[3]}\")\nprint(f\"thm_2 vs thm_4: {thm_correlations[4]}\")\nprint(f\"thm_3 vs thm_4: {thm_correlations[5]}\")\n\n# センサー間のクロスモーダル相関\nprint(\"\\n🔄 Cross-modal Sensor Correlations:\")\ncross_correlations = conn.execute(\"\"\"\n    SELECT \n        ROUND(CORR(acc_x, thm_1), 3) as acc_x_thm1_corr,\n        ROUND(CORR(acc_y, thm_2), 3) as acc_y_thm2_corr,\n        ROUND(CORR(acc_z, thm_3), 3) as acc_z_thm3_corr,\n        ROUND(CORR(rot_w, thm_1), 3) as rot_w_thm1_corr,\n        ROUND(CORR(SQRT(acc_x*acc_x + acc_y*acc_y + acc_z*acc_z), thm_1), 3) as acc_magnitude_thm1_corr\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    WHERE acc_x IS NOT NULL AND rot_w IS NOT NULL AND thm_1 IS NOT NULL\n\"\"\").fetchone()\n\nprint(f\"Acceleration vs Temperature:\")\nprint(f\"  acc_x vs thm_1: {cross_correlations[0]}\")\nprint(f\"  acc_y vs thm_2: {cross_correlations[1]}\")\nprint(f\"  acc_z vs thm_3: {cross_correlations[2]}\")\nprint(f\"  rot_w vs thm_1: {cross_correlations[3]}\")\nprint(f\"  acc_magnitude vs thm_1: {cross_correlations[4]}\")\n\n# ToFセンサーの代表チャンネル相関分析\nprint(\"\\n📡 ToF Sensor Channel Correlations (Sample):\")\ntof_correlations = conn.execute(\"\"\"\n    SELECT \n        ROUND(CORR(tof_1_v0, tof_1_v31), 3) as tof1_v0_v31_corr,\n        ROUND(CORR(tof_1_v0, tof_2_v0), 3) as tof1_tof2_v0_corr,\n        ROUND(CORR(tof_1_v0, tof_3_v0), 3) as tof1_tof3_v0_corr,\n        ROUND(CORR(tof_2_v0, tof_3_v0), 3) as tof2_tof3_v0_corr,\n        COUNT(*) as valid_samples\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    WHERE tof_1_v0 IS NOT NULL AND tof_2_v0 IS NOT NULL AND tof_3_v0 IS NOT NULL\n\"\"\").fetchone()\n\nprint(f\"ToF channel correlations (n={tof_correlations[4]:,}):\")\nprint(f\"  tof_1_v0 vs tof_1_v31: {tof_correlations[0]}\")\nprint(f\"  tof_1_v0 vs tof_2_v0: {tof_correlations[1]}\")\nprint(f\"  tof_1_v0 vs tof_3_v0: {tof_correlations[2]}\")\nprint(f\"  tof_2_v0 vs tof_3_v0: {tof_correlations[3]}\")\n\n# マルチコリニアリティ評価\nprint(\"\\n⚠️ Multicollinearity Assessment:\")\n\n# 高相関ペアの特定\nhigh_corr_pairs = []\ncorrelation_data = [\n    (\"acc_x\", \"acc_y\", imu_correlations[0]),\n    (\"acc_x\", \"acc_z\", imu_correlations[1]),\n    (\"acc_y\", \"acc_z\", imu_correlations[2]),\n    (\"thm_1\", \"thm_2\", thm_correlations[0]),\n    (\"thm_1\", \"thm_3\", thm_correlations[1]),\n    (\"thm_2\", \"thm_3\", thm_correlations[3])\n]\n\nfor var1, var2, corr in correlation_data:\n    if abs(corr) > 0.8:\n        high_corr_pairs.append((var1, var2, corr))\n\nif high_corr_pairs:\n    print(\"High correlation pairs (|r| > 0.8):\")\n    for var1, var2, corr in high_corr_pairs:\n        print(f\"  {var1} ↔ {var2}: {corr}\")\nelse:\n    print(\"✅ No severe multicollinearity detected (|r| > 0.8)\")\n\n# 相関の解釈\nprint(\"\\n📊 Correlation Interpretation:\")\nprint(\"• Accelerometer correlations are expected due to device orientation\")\nprint(\"• Thermopile correlations suggest spatial temperature patterns\")\nprint(\"• Low cross-modal correlations indicate complementary information\")\nprint(\"• ToF channels may have redundancy - consider dimensionality reduction\")\n\nprint(\"\\n\" + \"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "qrzoqh41bs8",
   "source": "# 6. 👥 参加者人口統計と行動パターン分析\nprint(\"👥 6. PARTICIPANT DEMOGRAPHICS & BEHAVIOR PATTERNS\")\nprint(\"=\" * 60)\n\n# 人口統計学的特徴の分析\nprint(\"📊 Demographic Characteristics:\")\ndemographics = conn.execute(\"\"\"\n    SELECT \n        COUNT(*) as total_participants,\n        AVG(age) as avg_age,\n        MIN(age) as min_age,\n        MAX(age) as max_age,\n        ROUND(AVG(height_cm), 1) as avg_height_cm,\n        ROUND(AVG(shoulder_to_wrist_cm), 1) as avg_shoulder_to_wrist_cm,\n        ROUND(AVG(elbow_to_wrist_cm), 1) as avg_elbow_to_wrist_cm\n    FROM \"cmi_detect_behavior_with_sensor_data\".train_demographics\n\"\"\").fetchone()\n\nprint(f\"Total participants: {demographics[0]}\")\nprint(f\"Age: mean={demographics[1]:.1f}, range={demographics[2]}-{demographics[3]}\")\nprint(f\"Height: mean={demographics[4]} cm\")\nprint(f\"Shoulder to wrist: mean={demographics[5]} cm\")\nprint(f\"Elbow to wrist: mean={demographics[6]} cm\")\n\n# カテゴリ変数の分布\nprint(\"\\n🏷️ Categorical Demographics:\")\ncategorical_demographics = conn.execute(\"\"\"\n    SELECT \n        'adult_child' as category,\n        CASE WHEN adult_child = 1 THEN 'Adult' ELSE 'Child' END as value,\n        COUNT(*) as count,\n        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".train_demographics), 1) as percentage\n    FROM \"cmi_detect_behavior_with_sensor_data\".train_demographics\n    GROUP BY adult_child\n    \n    UNION ALL\n    \n    SELECT \n        'sex',\n        CASE WHEN sex = 1 THEN 'Male' ELSE 'Female' END,\n        COUNT(*),\n        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".train_demographics), 1)\n    FROM \"cmi_detect_behavior_with_sensor_data\".train_demographics\n    GROUP BY sex\n    \n    UNION ALL\n    \n    SELECT \n        'handedness',\n        CASE WHEN handedness = 1 THEN 'Right-handed' ELSE 'Left-handed' END,\n        COUNT(*),\n        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".train_demographics), 1)\n    FROM \"cmi_detect_behavior_with_sensor_data\".train_demographics\n    GROUP BY handedness\n\"\"\").fetchdf()\n\nprint(categorical_demographics.to_string(index=False))\n\n# 参加者別の行動パターン\nprint(\"\\n🎯 Behavior Patterns by Participant:\")\nparticipant_behavior = conn.execute(\"\"\"\n    SELECT \n        t.subject,\n        d.age,\n        CASE WHEN d.sex = 1 THEN 'M' ELSE 'F' END as sex,\n        CASE WHEN d.handedness = 1 THEN 'R' ELSE 'L' END as handedness,\n        COUNT(DISTINCT t.sequence_id) as total_sequences,\n        COUNT(DISTINCT t.gesture) as unique_gestures,\n        COUNT(*) as total_timesteps,\n        ROUND(AVG(CASE WHEN t.sequence_type = 'Target' THEN 1.0 ELSE 0.0 END) * 100, 1) as target_percentage\n    FROM \"cmi_detect_behavior_with_sensor_data\".train t\n    JOIN \"cmi_detect_behavior_with_sensor_data\".train_demographics d ON t.subject = d.subject\n    GROUP BY t.subject, d.age, d.sex, d.handedness\n    ORDER BY total_sequences DESC\n    LIMIT 15\n\"\"\").fetchdf()\n\nprint(participant_behavior.to_string(index=False))\n\n# 年齢群別の行動分析\nprint(\"\\n📈 Behavior Analysis by Age Group:\")\nage_behavior = conn.execute(\"\"\"\n    SELECT \n        CASE \n            WHEN d.age < 18 THEN 'Child (<18)'\n            WHEN d.age BETWEEN 18 AND 30 THEN 'Young Adult (18-30)'\n            WHEN d.age BETWEEN 31 AND 50 THEN 'Adult (31-50)'\n            ELSE 'Older Adult (>50)'\n        END as age_group,\n        COUNT(DISTINCT t.subject) as participants,\n        ROUND(AVG(sequence_count), 1) as avg_sequences,\n        ROUND(AVG(gesture_diversity), 1) as avg_gesture_diversity,\n        ROUND(AVG(target_rate) * 100, 1) as avg_target_rate\n    FROM (\n        SELECT \n            t.subject,\n            COUNT(DISTINCT t.sequence_id) as sequence_count,\n            COUNT(DISTINCT t.gesture) as gesture_diversity,\n            AVG(CASE WHEN t.sequence_type = 'Target' THEN 1.0 ELSE 0.0 END) as target_rate\n        FROM \"cmi_detect_behavior_with_sensor_data\".train t\n        GROUP BY t.subject\n    ) t\n    JOIN \"cmi_detect_behavior_with_sensor_data\".train_demographics d ON t.subject = d.subject\n    GROUP BY \n        CASE \n            WHEN d.age < 18 THEN 'Child (<18)'\n            WHEN d.age BETWEEN 18 AND 30 THEN 'Young Adult (18-30)'\n            WHEN d.age BETWEEN 31 AND 50 THEN 'Adult (31-50)'\n            ELSE 'Older Adult (>50)'\n        END\n    ORDER BY avg_sequences DESC\n\"\"\").fetchdf()\n\nprint(age_behavior.to_string(index=False))\n\n# 利き手による行動の違い\nprint(\"\\n✋ Handedness Impact on Behavior:\")\nhandedness_behavior = conn.execute(\"\"\"\n    SELECT \n        CASE WHEN d.handedness = 1 THEN 'Right-handed' ELSE 'Left-handed' END as handedness,\n        COUNT(DISTINCT t.subject) as participants,\n        ROUND(AVG(sequence_count), 1) as avg_sequences,\n        ROUND(AVG(target_rate) * 100, 1) as avg_target_rate,\n        STRING_AGG(DISTINCT most_common_gesture, ', ') as common_gestures\n    FROM (\n        SELECT \n            t.subject,\n            COUNT(DISTINCT t.sequence_id) as sequence_count,\n            AVG(CASE WHEN t.sequence_type = 'Target' THEN 1.0 ELSE 0.0 END) as target_rate,\n            MODE() WITHIN GROUP (ORDER BY t.gesture) as most_common_gesture\n        FROM \"cmi_detect_behavior_with_sensor_data\".train t\n        GROUP BY t.subject\n    ) t\n    JOIN \"cmi_detect_behavior_with_sensor_data\".train_demographics d ON t.subject = d.subject\n    GROUP BY d.handedness\n\"\"\").fetchdf()\n\nprint(handedness_behavior.to_string(index=False))\n\nprint(\"\\n\" + \"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "m6ukgyl75r",
   "source": "# 5. 🔍 欠損データパターンと品質評価\nprint(\"🔍 5. MISSING DATA PATTERNS & QUALITY ASSESSMENT\")\nprint(\"=\" * 60)\n\n# 全センサーの欠損率分析\nprint(\"📊 Comprehensive Missing Data Analysis:\")\nmissing_analysis = conn.execute(\"\"\"\n    SELECT \n        'IMU_accelerometer' as sensor_group,\n        'acc_x' as sensor,\n        COUNT(*) as total_rows,\n        COUNT(*) - COUNT(acc_x) as missing_count,\n        ROUND((COUNT(*) - COUNT(acc_x)) * 100.0 / COUNT(*), 2) as missing_pct\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    \n    UNION ALL SELECT 'IMU_accelerometer', 'acc_y', COUNT(*), COUNT(*) - COUNT(acc_y), ROUND((COUNT(*) - COUNT(acc_y)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n    UNION ALL SELECT 'IMU_accelerometer', 'acc_z', COUNT(*), COUNT(*) - COUNT(acc_z), ROUND((COUNT(*) - COUNT(acc_z)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n    UNION ALL SELECT 'IMU_rotation', 'rot_w', COUNT(*), COUNT(*) - COUNT(rot_w), ROUND((COUNT(*) - COUNT(rot_w)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n    UNION ALL SELECT 'IMU_rotation', 'rot_x', COUNT(*), COUNT(*) - COUNT(rot_x), ROUND((COUNT(*) - COUNT(rot_x)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n    UNION ALL SELECT 'IMU_rotation', 'rot_y', COUNT(*), COUNT(*) - COUNT(rot_y), ROUND((COUNT(*) - COUNT(rot_y)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n    UNION ALL SELECT 'IMU_rotation', 'rot_z', COUNT(*), COUNT(*) - COUNT(rot_z), ROUND((COUNT(*) - COUNT(rot_z)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n    UNION ALL SELECT 'Thermopile', 'thm_1', COUNT(*), COUNT(*) - COUNT(thm_1), ROUND((COUNT(*) - COUNT(thm_1)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n    UNION ALL SELECT 'Thermopile', 'thm_2', COUNT(*), COUNT(*) - COUNT(thm_2), ROUND((COUNT(*) - COUNT(thm_2)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n    UNION ALL SELECT 'Thermopile', 'thm_3', COUNT(*), COUNT(*) - COUNT(thm_3), ROUND((COUNT(*) - COUNT(thm_3)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n    UNION ALL SELECT 'Thermopile', 'thm_4', COUNT(*), COUNT(*) - COUNT(thm_4), ROUND((COUNT(*) - COUNT(thm_4)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n    UNION ALL SELECT 'Thermopile', 'thm_5', COUNT(*), COUNT(*) - COUNT(thm_5), ROUND((COUNT(*) - COUNT(thm_5)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n\"\"\").fetchdf()\n\nprint(missing_analysis.to_string(index=False))\n\n# ToFセンサーの欠損パターン（サンプリング）\nprint(\"\\n📡 ToF Sensor Missing Pattern (Sample):\")\ntof_missing = conn.execute(\"\"\"\n    SELECT \n        'ToF' as sensor_group,\n        'tof_1_v0' as sensor,\n        COUNT(*) as total_rows,\n        COUNT(*) - COUNT(tof_1_v0) as missing_count,\n        ROUND((COUNT(*) - COUNT(tof_1_v0)) * 100.0 / COUNT(*), 2) as missing_pct\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    \n    UNION ALL SELECT 'ToF', 'tof_2_v0', COUNT(*), COUNT(*) - COUNT(tof_2_v0), ROUND((COUNT(*) - COUNT(tof_2_v0)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n    UNION ALL SELECT 'ToF', 'tof_3_v0', COUNT(*), COUNT(*) - COUNT(tof_3_v0), ROUND((COUNT(*) - COUNT(tof_3_v0)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n    UNION ALL SELECT 'ToF', 'tof_4_v0', COUNT(*), COUNT(*) - COUNT(tof_4_v0), ROUND((COUNT(*) - COUNT(tof_4_v0)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n    UNION ALL SELECT 'ToF', 'tof_5_v0', COUNT(*), COUNT(*) - COUNT(tof_5_v0), ROUND((COUNT(*) - COUNT(tof_5_v0)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n\"\"\").fetchdf()\n\nprint(tof_missing.to_string(index=False))\n\n# 欠損値の共起パターン分析\nprint(\"\\n🔗 Missing Value Co-occurrence Patterns:\")\nmissing_cooccurrence = conn.execute(\"\"\"\n    SELECT \n        CASE WHEN thm_5 IS NULL THEN 'thm_5_missing' ELSE 'thm_5_present' END as thm5_status,\n        CASE WHEN tof_5_v0 IS NULL THEN 'tof_5_missing' ELSE 'tof_5_present' END as tof5_status,\n        COUNT(*) as count,\n        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".train), 2) as percentage\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    GROUP BY \n        CASE WHEN thm_5 IS NULL THEN 'thm_5_missing' ELSE 'thm_5_present' END,\n        CASE WHEN tof_5_v0 IS NULL THEN 'tof_5_missing' ELSE 'tof_5_present' END\n    ORDER BY count DESC\n\"\"\").fetchdf()\n\nprint(missing_cooccurrence.to_string(index=False))\n\n# 参加者別の欠損パターン\nprint(\"\\n👥 Missing Data by Participant (Top 10 with most missing):\")\nparticipant_missing = conn.execute(\"\"\"\n    SELECT \n        subject,\n        COUNT(*) as total_rows,\n        COUNT(*) - COUNT(thm_5) as thm5_missing,\n        COUNT(*) - COUNT(tof_5_v0) as tof5_missing,\n        ROUND((COUNT(*) - COUNT(thm_5)) * 100.0 / COUNT(*), 1) as thm5_missing_pct,\n        ROUND((COUNT(*) - COUNT(tof_5_v0)) * 100.0 / COUNT(*), 1) as tof5_missing_pct\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    GROUP BY subject\n    HAVING (COUNT(*) - COUNT(thm_5)) > 0 OR (COUNT(*) - COUNT(tof_5_v0)) > 0\n    ORDER BY thm5_missing_pct DESC, tof5_missing_pct DESC\n    LIMIT 10\n\"\"\").fetchdf()\n\nprint(participant_missing.to_string(index=False))\n\n# データ品質サマリー\nprint(\"\\n📋 Data Quality Summary:\")\nquality_metrics = missing_analysis.groupby('sensor_group')['missing_pct'].agg(['mean', 'max']).round(2)\nprint(\"Missing rate by sensor group:\")\nfor group in quality_metrics.index:\n    mean_missing = quality_metrics.loc[group, 'mean']\n    max_missing = quality_metrics.loc[group, 'max']\n    status = \"🟢 EXCELLENT\" if max_missing < 1 else \"🟡 GOOD\" if max_missing < 5 else \"🔴 NEEDS ATTENTION\"\n    print(f\"  {group}: avg={mean_missing}%, max={max_missing}% {status}\")\n\nprint(\"\\n\" + \"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "wkpu64m5bs",
   "source": "# 4. ⏱️ 時系列パターンと構造分析\nprint(\"⏱️ 4. TIME SERIES PATTERNS & SEQUENCE STRUCTURE ANALYSIS\")\nprint(\"=\" * 60)\n\n# シーケンス長の詳細分析\nprint(\"📏 Sequence Length Analysis:\")\nsequence_lengths = conn.execute(\"\"\"\n    SELECT \n        COUNT(DISTINCT sequence_id) as total_sequences,\n        ROUND(AVG(length), 1) as avg_length,\n        ROUND(STDDEV(length), 1) as std_length,\n        MIN(length) as min_length,\n        MAX(length) as max_length,\n        ROUND(PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY length), 1) as q25,\n        ROUND(PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY length), 1) as median,\n        ROUND(PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY length), 1) as q75,\n        ROUND(PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY length), 1) as p95\n    FROM (\n        SELECT sequence_id, COUNT(*) as length\n        FROM \"cmi_detect_behavior_with_sensor_data\".train\n        GROUP BY sequence_id\n    )\n\"\"\").fetchone()\n\nprint(f\"Total sequences: {sequence_lengths[0]:,}\")\nprint(f\"Length stats: mean={sequence_lengths[1]}, std={sequence_lengths[2]}\")\nprint(f\"Length range: {sequence_lengths[3]} - {sequence_lengths[4]} timesteps\")\nprint(f\"Quartiles: Q1={sequence_lengths[5]}, Q2={sequence_lengths[6]}, Q3={sequence_lengths[7]}\")\nprint(f\"95th percentile: {sequence_lengths[8]} timesteps\")\n\n# 50Hz前提での時間計算\nprint(f\"\\n⏱️ Time Duration (assuming 50Hz sampling):\")\nprint(f\"  • Average sequence duration: {sequence_lengths[1]/50:.2f} seconds\")\nprint(f\"  • Median sequence duration: {sequence_lengths[6]/50:.2f} seconds\")\nprint(f\"  • Longest sequence duration: {sequence_lengths[4]/50:.1f} seconds\")\n\n# ジェスチャー別のシーケンス長分析\nprint(\"\\n📊 Sequence Length by Gesture Type:\")\ngesture_lengths = conn.execute(\"\"\"\n    SELECT \n        gesture,\n        COUNT(DISTINCT sequence_id) as num_sequences,\n        ROUND(AVG(length), 1) as avg_length,\n        ROUND(MIN(length), 1) as min_length,\n        ROUND(MAX(length), 1) as max_length\n    FROM (\n        SELECT \n            sequence_id, \n            gesture,\n            COUNT(*) as length\n        FROM \"cmi_detect_behavior_with_sensor_data\".train\n        GROUP BY sequence_id, gesture\n    )\n    GROUP BY gesture\n    ORDER BY avg_length DESC\n    LIMIT 10\n\"\"\").fetchdf()\n\nprint(gesture_lengths.to_string(index=False))\n\n# 参加者別のシーケンス数分析\nprint(\"\\n👥 Sequence Count per Participant:\")\nparticipant_sequences = conn.execute(\"\"\"\n    SELECT \n        COUNT(DISTINCT subject) as total_participants,\n        ROUND(AVG(seq_count), 1) as avg_sequences_per_participant,\n        MIN(seq_count) as min_sequences,\n        MAX(seq_count) as max_sequences,\n        ROUND(STDDEV(seq_count), 1) as std_sequences\n    FROM (\n        SELECT \n            subject, \n            COUNT(DISTINCT sequence_id) as seq_count\n        FROM \"cmi_detect_behavior_with_sensor_data\".train\n        GROUP BY subject\n    )\n\"\"\").fetchone()\n\nprint(f\"Participants: {participant_sequences[0]}\")\nprint(f\"Sequences per participant: mean={participant_sequences[1]}, std={participant_sequences[4]}\")\nprint(f\"Sequence range per participant: {participant_sequences[2]} - {participant_sequences[3]}\")\n\n# 時系列の連続性チェック\nprint(\"\\n🔗 Sequence Continuity Check:\")\ncontinuity_check = conn.execute(\"\"\"\n    SELECT \n        COUNT(*) as total_gaps,\n        AVG(gap_size) as avg_gap_size,\n        MAX(gap_size) as max_gap_size\n    FROM (\n        SELECT \n            sequence_id,\n            sequence_counter - LAG(sequence_counter) OVER (PARTITION BY sequence_id ORDER BY sequence_counter) as gap_size\n        FROM \"cmi_detect_behavior_with_sensor_data\".train\n    )\n    WHERE gap_size > 1\n\"\"\").fetchone()\n\nif continuity_check[0] > 0:\n    print(f\"⚠️  Found {continuity_check[0]} gaps in sequences\")\n    print(f\"   Average gap size: {continuity_check[1]:.1f}\")\n    print(f\"   Maximum gap size: {continuity_check[2]}\")\nelse:\n    print(\"✅ All sequences are continuous (no missing timesteps)\")\n\nprint(\"\\n\" + \"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "77z2wz6xjop",
   "source": "# 3. 🎯 ターゲット変数の詳細分析\nprint(\"🎯 3. TARGET VARIABLE CORRELATION & CLASS BALANCE ANALYSIS\")\nprint(\"=\" * 60)\n\n# ジェスチャーとsequence_typeの関係\nprint(\"🔄 Gesture vs Sequence Type Relationship:\")\ngesture_sequence_type = conn.execute(\"\"\"\n    SELECT \n        gesture,\n        sequence_type,\n        COUNT(*) as count,\n        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    GROUP BY gesture, sequence_type\n    ORDER BY count DESC\n    LIMIT 15\n\"\"\").fetchdf()\n\nprint(gesture_sequence_type.to_string(index=False))\n\n# 行動フェーズの詳細分析\nprint(\"\\n📊 Behavior Phase Distribution by Gesture (Top 5):\")\nbehavior_phase = conn.execute(\"\"\"\n    SELECT \n        gesture,\n        behavior,\n        COUNT(*) as count,\n        ROUND(AVG(sequence_counter), 1) as avg_timestamp\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    WHERE gesture IN (\n        SELECT gesture \n        FROM \"cmi_detect_behavior_with_sensor_data\".train \n        GROUP BY gesture \n        ORDER BY COUNT(*) DESC \n        LIMIT 5\n    )\n    GROUP BY gesture, behavior\n    ORDER BY gesture, count DESC\n\"\"\").fetchdf()\n\nprint(behavior_phase.to_string(index=False))\n\n# クラス不均衡の詳細評価\nprint(\"\\n⚖️ Class Imbalance Analysis:\")\n\n# Binary classification (sequence_type)\nbinary_balance = conn.execute(\"\"\"\n    SELECT \n        sequence_type,\n        COUNT(*) as count,\n        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".train), 2) as percentage\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    GROUP BY sequence_type\n    ORDER BY count DESC\n\"\"\").fetchdf()\n\nprint(\"Binary Classification (sequence_type):\")\nprint(binary_balance.to_string(index=False))\n\n# Multi-class gesture distribution\ngesture_balance = conn.execute(\"\"\"\n    SELECT \n        gesture,\n        COUNT(*) as count,\n        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".train), 2) as percentage,\n        COUNT(DISTINCT subject) as participants\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    GROUP BY gesture\n    ORDER BY count DESC\n\"\"\").fetchdf()\n\nprint(f\"\\nMulti-class Gesture Distribution (18 classes):\")\nprint(gesture_balance.to_string(index=False))\n\n# 不均衡比率の計算\nmax_class = gesture_balance['count'].max()\nmin_class = gesture_balance['count'].min()\nimbalance_ratio = max_class / min_class\n\nprint(f\"\\n📈 Class Imbalance Metrics:\")\nprint(f\"  • Most frequent gesture: {gesture_balance.iloc[0]['gesture']} ({gesture_balance.iloc[0]['count']:,} samples)\")\nprint(f\"  • Least frequent gesture: {gesture_balance.iloc[-1]['gesture']} ({gesture_balance.iloc[-1]['count']:,} samples)\")\nprint(f\"  • Imbalance ratio: {imbalance_ratio:.1f}:1\")\nprint(f\"  • Macro F1 challenge level: {'HIGH' if imbalance_ratio > 10 else 'MEDIUM' if imbalance_ratio > 3 else 'LOW'}\")\n\nprint(\"\\n\" + \"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "nj20rd5q4hp",
   "source": "# 2. 🔍 センサーデータ分布分析\nprint(\"🔍 2. SENSOR DATA DISTRIBUTION ANALYSIS\")\nprint(\"=\" * 60)\n\n# IMUセンサーの基本統計量\nprint(\"🎯 IMU Sensor Statistics:\")\nimu_stats = conn.execute(\"\"\"\n    SELECT \n        'acc_x' as sensor,\n        COUNT(*) - COUNT(acc_x) as null_count,\n        ROUND(AVG(acc_x), 4) as mean_val,\n        ROUND(STDDEV(acc_x), 4) as std_val,\n        ROUND(MIN(acc_x), 4) as min_val,\n        ROUND(MAX(acc_x), 4) as max_val,\n        ROUND(PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY acc_x), 4) as median_val\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    WHERE acc_x IS NOT NULL\n    \n    UNION ALL\n    \n    SELECT \n        'acc_y',\n        COUNT(*) - COUNT(acc_y),\n        ROUND(AVG(acc_y), 4),\n        ROUND(STDDEV(acc_y), 4),\n        ROUND(MIN(acc_y), 4),\n        ROUND(MAX(acc_y), 4),\n        ROUND(PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY acc_y), 4)\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    WHERE acc_y IS NOT NULL\n    \n    UNION ALL\n    \n    SELECT \n        'acc_z',\n        COUNT(*) - COUNT(acc_z),\n        ROUND(AVG(acc_z), 4),\n        ROUND(STDDEV(acc_z), 4),\n        ROUND(MIN(acc_z), 4),\n        ROUND(MAX(acc_z), 4),\n        ROUND(PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY acc_z), 4)\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    WHERE acc_z IS NOT NULL\n\"\"\").fetchdf()\n\nprint(imu_stats.to_string(index=False))\n\n# 回転センサー（クォータニオン）の統計量\nprint(\"\\n🔄 Rotation Quaternion Statistics:\")\nrot_stats = conn.execute(\"\"\"\n    SELECT \n        'rot_w' as sensor,\n        COUNT(*) - COUNT(rot_w) as null_count,\n        ROUND(AVG(rot_w), 4) as mean_val,\n        ROUND(STDDEV(rot_w), 4) as std_val,\n        ROUND(MIN(rot_w), 4) as min_val,\n        ROUND(MAX(rot_w), 4) as max_val\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    WHERE rot_w IS NOT NULL\n    \n    UNION ALL\n    \n    SELECT 'rot_x', COUNT(*) - COUNT(rot_x), ROUND(AVG(rot_x), 4), ROUND(STDDEV(rot_x), 4), ROUND(MIN(rot_x), 4), ROUND(MAX(rot_x), 4)\n    FROM \"cmi_detect_behavior_with_sensor_data\".train WHERE rot_x IS NOT NULL\n    \n    UNION ALL\n    \n    SELECT 'rot_y', COUNT(*) - COUNT(rot_y), ROUND(AVG(rot_y), 4), ROUND(STDDEV(rot_y), 4), ROUND(MIN(rot_y), 4), ROUND(MAX(rot_y), 4)\n    FROM \"cmi_detect_behavior_with_sensor_data\".train WHERE rot_y IS NOT NULL\n    \n    UNION ALL\n    \n    SELECT 'rot_z', COUNT(*) - COUNT(rot_z), ROUND(AVG(rot_z), 4), ROUND(STDDEV(rot_z), 4), ROUND(MIN(rot_z), 4), ROUND(MAX(rot_z), 4)\n    FROM \"cmi_detect_behavior_with_sensor_data\".train WHERE rot_z IS NOT NULL\n\"\"\").fetchdf()\n\nprint(rot_stats.to_string(index=False))\n\n# 温度センサーの統計量\nprint(\"\\n🌡️ Thermopile Sensor Statistics:\")\nthm_stats = conn.execute(\"\"\"\n    SELECT \n        'thm_1' as sensor,\n        COUNT(*) - COUNT(thm_1) as null_count,\n        ROUND(AVG(thm_1), 2) as mean_val,\n        ROUND(STDDEV(thm_1), 2) as std_val,\n        ROUND(MIN(thm_1), 2) as min_val,\n        ROUND(MAX(thm_1), 2) as max_val\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    WHERE thm_1 IS NOT NULL\n    \n    UNION ALL SELECT 'thm_2', COUNT(*) - COUNT(thm_2), ROUND(AVG(thm_2), 2), ROUND(STDDEV(thm_2), 2), ROUND(MIN(thm_2), 2), ROUND(MAX(thm_2), 2)\n    FROM \"cmi_detect_behavior_with_sensor_data\".train WHERE thm_2 IS NOT NULL\n    UNION ALL SELECT 'thm_3', COUNT(*) - COUNT(thm_3), ROUND(AVG(thm_3), 2), ROUND(STDDEV(thm_3), 2), ROUND(MIN(thm_3), 2), ROUND(MAX(thm_3), 2)\n    FROM \"cmi_detect_behavior_with_sensor_data\".train WHERE thm_3 IS NOT NULL\n    UNION ALL SELECT 'thm_4', COUNT(*) - COUNT(thm_4), ROUND(AVG(thm_4), 2), ROUND(STDDEV(thm_4), 2), ROUND(MIN(thm_4), 2), ROUND(MAX(thm_4), 2)\n    FROM \"cmi_detect_behavior_with_sensor_data\".train WHERE thm_4 IS NOT NULL\n    UNION ALL SELECT 'thm_5', COUNT(*) - COUNT(thm_5), ROUND(AVG(thm_5), 2), ROUND(STDDEV(thm_5), 2), ROUND(MIN(thm_5), 2), ROUND(MAX(thm_5), 2)\n    FROM \"cmi_detect_behavior_with_sensor_data\".train WHERE thm_5 IS NOT NULL\n\"\"\").fetchdf()\n\nprint(thm_stats.to_string(index=False))\n\nprint(\"\\n\" + \"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "dsndjmnwye4",
   "source": "# 1. 📊 包括的データプロファイリング\nprint(\"🔍 1. COMPREHENSIVE DATA PROFILING\")\nprint(\"=\" * 60)\n\n# 基本データ構造の確認\nprint(\"📋 Dataset Structure:\")\nstructure_info = conn.execute(\"\"\"\n    SELECT \n        'train' as table_name,\n        COUNT(*) as total_rows,\n        COUNT(DISTINCT subject) as unique_participants,\n        COUNT(DISTINCT sequence_id) as unique_sequences,\n        MIN(sequence_counter) as min_counter,\n        MAX(sequence_counter) as max_counter\n    FROM \"cmi_detect_behavior_with_sensor_data\".train\n    \n    UNION ALL\n    \n    SELECT \n        'test' as table_name,\n        COUNT(*) as total_rows,\n        COUNT(DISTINCT subject) as unique_participants,\n        COUNT(DISTINCT sequence_id) as unique_sequences,\n        MIN(sequence_counter) as min_counter,\n        MAX(sequence_counter) as max_counter\n    FROM \"cmi_detect_behavior_with_sensor_data\".test\n\"\"\").fetchdf()\n\nprint(structure_info.to_string(index=False))\n\n# データ型と非NULL値の詳細確認\nprint(\"\\n🧮 Column Data Types and Non-null Counts:\")\ncolumn_info = conn.execute(\"\"\"\n    SELECT \n        column_name,\n        data_type,\n        is_nullable\n    FROM information_schema.columns \n    WHERE table_schema = 'cmi_detect_behavior_with_sensor_data' \n    AND table_name = 'train'\n    ORDER BY ordinal_position\n\"\"\").fetchdf()\n\nprint(f\"Total columns: {len(column_info)}\")\n\n# カテゴリ別の列数\ncategorical_cols = ['row_id', 'sequence_type', 'sequence_id', 'subject', 'orientation', 'behavior', 'phase', 'gesture']\nsensor_cols = [col for col in column_info['column_name'] if col not in categorical_cols]\n\nprint(f\"  📝 Categorical columns: {len(categorical_cols)}\")\nprint(f\"  🔢 Sensor columns: {len(sensor_cols)}\")\n\n# センサー別分類\nimu_cols = [col for col in sensor_cols if col.startswith(('acc_', 'rot_'))]\nthermopile_cols = [col for col in sensor_cols if col.startswith('thm_')]\ntof_cols = [col for col in sensor_cols if col.startswith('tof_')]\n\nprint(f\"    🎯 IMU sensors: {len(imu_cols)} ({imu_cols})\")\nprint(f\"    🌡️  Thermopile sensors: {len(thermopile_cols)} ({thermopile_cols})\")\nprint(f\"    📡 ToF sensors: {len(tof_cols)} (5 sensors × 64 channels)\")\n\nprint(\"\\n\" + \"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "tbbteeni3z",
   "source": "# 🧠 CMI BFRB Detection - 本格的EDA\n\n## 📋 分析プラン\n\n### 🎯 目標\n- **コンペ概要**: Body-Focused Repetitive Behaviors (BFRB) 検出\n- **評価指標**: 0.5×(Binary F1 + Macro F1) \n- **データ**: マルチモーダルセンサー時系列データ (50Hz)\n- **参加者**: 81名の訓練データ、18種類のジェスチャー\n\n### 📊 詳細分析項目\n1. **データプロファイリング** - 基本統計量と品質評価\n2. **センサーデータ分析** - 分布、外れ値、ノイズ特性\n3. **ターゲット変数分析** - クラス不均衡、相関関係\n4. **時系列パターン分析** - シーケンス構造、周期性\n5. **欠損値パターン** - センサー故障、データ品質\n6. **参加者特性分析** - 人口統計学的特徴\n7. **特徴相関分析** - マルチコリニアリティ、特徴選択\n8. **マルチモーダル分析** - センサー融合の可能性\n9. **CV戦略検証** - データリーク防止策\n10. **特徴工学提案** - ドメイン知識活用\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ygt0djc9auc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUG DATABASE TABLES ===\n",
      "Connection status: <duckdb.duckdb.DuckDBPyConnection object at 0x7f69477016b0>\n",
      "\n",
      "=== ALL SCHEMAS ===\n",
      "  - cmi_detect_behavior_with_sensor_data\n",
      "  - main\n",
      "  - playground_series_s5e7\n",
      "  - information_schema\n",
      "  - main\n",
      "  - pg_catalog\n",
      "  - main\n",
      "\n",
      "=== ALL TABLES WITH SCHEMA ===\n",
      "Found 7 tables:\n",
      "  - cmi_detect_behavior_with_sensor_data.test\n",
      "  - cmi_detect_behavior_with_sensor_data.test_demographics\n",
      "  - cmi_detect_behavior_with_sensor_data.train\n",
      "  - cmi_detect_behavior_with_sensor_data.train_demographics\n",
      "  - playground_series_s5e7.sample_submission\n",
      "  - playground_series_s5e7.test\n",
      "  - playground_series_s5e7.train\n",
      "\n",
      "=== SHOW TABLES ===\n",
      "Default schema tables: 0 found\n",
      "\n",
      "=== TESTING CMI SCHEMA ACCESS ===\n",
      "✅ cmi_detect_behavior_with_sensor_data.train accessible: 574945 rows\n",
      "\n",
      "=== KAGGLE_DATASETS SCHEMA CONTENTS ===\n",
      "Error listing cmi schema tables: Parser Error: syntax error at or near \"FROM\"\n",
      "\n",
      "=== DATABASE FILE INFO ===\n",
      "✅ Database file exists, size: 286,273,536 bytes (273.0 MB)\n"
     ]
    }
   ],
   "source": [
    "# Debug database tables issue\n",
    "print(\"=== DEBUG DATABASE TABLES ===\")\n",
    "\n",
    "# Check if connection exists\n",
    "try:\n",
    "    print(f\"Connection status: {conn}\")\n",
    "except NameError:\n",
    "    print(\"Connection not found, creating new one...\")\n",
    "    import duckdb\n",
    "    conn = duckdb.connect('/home/wsl/dev/my-study/ml/ml-note/kaggle_datasets.duckdb')\n",
    "\n",
    "# Check all schemas\n",
    "print(\"\\n=== ALL SCHEMAS ===\")\n",
    "schemas = conn.execute(\"SELECT schema_name FROM information_schema.schemata\").fetchall()\n",
    "for schema in schemas:\n",
    "    print(f\"  - {schema[0]}\")\n",
    "\n",
    "# Check all tables with schema\n",
    "print(\"\\n=== ALL TABLES WITH SCHEMA ===\")\n",
    "tables = conn.execute(\"SELECT table_schema, table_name FROM information_schema.tables WHERE table_type = 'BASE TABLE'\").fetchall()\n",
    "print(f\"Found {len(tables)} tables:\")\n",
    "for schema, table in tables:\n",
    "    print(f\"  - {schema}.{table}\")\n",
    "\n",
    "# Try SHOW TABLES in different schemas\n",
    "print(\"\\n=== SHOW TABLES ===\")\n",
    "try:\n",
    "    show_tables = conn.execute(\"SHOW TABLES\").fetchall()\n",
    "    print(f\"Default schema tables: {len(show_tables)} found\")\n",
    "    for table in show_tables:\n",
    "        print(f\"  - {table[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with SHOW TABLES: {e}\")\n",
    "\n",
    "# Try to access the CMI tables directly\n",
    "print(\"\\n=== TESTING CMI SCHEMA ACCESS ===\")\n",
    "try:\n",
    "    result = conn.execute('SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".train').fetchone()\n",
    "    print(f\"✅ cmi_detect_behavior_with_sensor_data.train accessible: {result[0]} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error accessing cmi_detect_behavior_with_sensor_data.train: {e}\")\n",
    "\n",
    "# List all objects in kaggle_datasets schema\n",
    "print(\"\\n=== KAGGLE_DATASETS SCHEMA CONTENTS ===\")\n",
    "try:\n",
    "    result = conn.execute('SHOW TABLES FROM \"kaggle_datasets\".\"cmi_detect_behavior_with_sensor_data\"').fetchall()\n",
    "    print(f\"Tables in cmi_detect_behavior_with_sensor_data schema:\")\n",
    "    for table in result:\n",
    "        print(f\"  - {table[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error listing cmi schema tables: {e}\")\n",
    "\n",
    "print(\"\\n=== DATABASE FILE INFO ===\")\n",
    "import os\n",
    "db_path = '/home/wsl/dev/my-study/ml/ml-note/kaggle_datasets.duckdb'\n",
    "if os.path.exists(db_path):\n",
    "    size = os.path.getsize(db_path)\n",
    "    print(f\"✅ Database file exists, size: {size:,} bytes ({size/1024/1024:.1f} MB)\")\n",
    "else:\n",
    "    print(\"❌ Database file does not exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "gzao13pnms5",
   "source": "import duckdb\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nprint(\"🔍 CMI BFRB Detection Dataset - EDA\")\nprint(\"=\" * 50)\n\n# Connect to the DuckDB database\nconn = duckdb.connect('/home/wsl/dev/my-study/ml/ml-note/kaggle_datasets.duckdb')\n\n# Show tables from the correct schema\nprint(\"📊 Available tables in cmi_detect_behavior_with_sensor_data schema:\")\ntables = conn.execute(\"\"\"\n    SELECT table_name \n    FROM information_schema.tables \n    WHERE table_schema = 'cmi_detect_behavior_with_sensor_data'\n    ORDER BY table_name\n\"\"\").fetchall()\n\nfor table in tables:\n    print(f\"  ✅ {table[0]}\")\n\nprint(f\"\\n📈 Database size: {286.3:.1f} MB with {len(tables)} tables\")\nprint(f\"🎯 Target dataset: CMI Body-Focused Repetitive Behaviors Detection\")\nprint(\"=\" * 50)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "htdje7frgh6",
   "metadata": {},
   "source": [
    "## Key Dataset Findings\n",
    "\n",
    "### Data Size\n",
    "- **Train**: 574,945 rows across 81 participants (8,151 sequences)\n",
    "- **Test**: 107 rows across 2 participants (2 sequences)  \n",
    "- **No participant overlap** between train and test sets\n",
    "\n",
    "### Data Distribution\n",
    "- Average ~7,098 rows per participant in training\n",
    "- Average ~71 sequences per participant in training\n",
    "- Test set appears to be a small sample for submission format\n",
    "\n",
    "### Important Notes\n",
    "- This is a **time series** dataset with sequence structure\n",
    "- Need **GroupKFold by participant** to prevent data leakage\n",
    "- Large training set (~575k timesteps) suggests 50Hz sampling over multiple sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "m4mtsa81enc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA SIZE ANALYSIS ===\n",
      "Train data rows: 574,945\n",
      "Test data rows: 107\n",
      "Train demographics: 81\n",
      "Test demographics: 2\n",
      "\n",
      "=== PARTICIPANT ANALYSIS ===\n",
      "Unique participants in train: 81\n",
      "Unique participants in test: 2\n",
      "Participants appearing in both train and test: 0\n",
      "\n",
      "=== SEQUENCE ANALYSIS ===\n",
      "Unique sequences in train: 8151\n",
      "Unique sequences in test: 2\n"
     ]
    }
   ],
   "source": [
    "# Re-establish database connection for this cell\n",
    "import duckdb\n",
    "conn = duckdb.connect('/home/wsl/dev/my-study/ml/ml-note/kaggle_datasets.duckdb')\n",
    "\n",
    "# Data size and distribution analysis\n",
    "print(\"=== DATA SIZE ANALYSIS ===\")\n",
    "\n",
    "# Check row counts\n",
    "train_count = conn.execute('SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".train').fetchone()[0]\n",
    "test_count = conn.execute('SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".test').fetchone()[0]\n",
    "train_demo_count = conn.execute('SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".train_demographics').fetchone()[0]\n",
    "test_demo_count = conn.execute('SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".test_demographics').fetchone()[0]\n",
    "\n",
    "print(f\"Train data rows: {train_count:,}\")\n",
    "print(f\"Test data rows: {test_count:,}\")\n",
    "print(f\"Train demographics: {train_demo_count:,}\")\n",
    "print(f\"Test demographics: {test_demo_count:,}\")\n",
    "\n",
    "print(\"\\n=== PARTICIPANT ANALYSIS ===\")\n",
    "\n",
    "# Unique participants\n",
    "train_participants = conn.execute('SELECT COUNT(DISTINCT subject) FROM \"cmi_detect_behavior_with_sensor_data\".train').fetchone()[0]\n",
    "test_participants = conn.execute('SELECT COUNT(DISTINCT subject) FROM \"cmi_detect_behavior_with_sensor_data\".test').fetchone()[0]\n",
    "\n",
    "print(f\"Unique participants in train: {train_participants}\")\n",
    "print(f\"Unique participants in test: {test_participants}\")\n",
    "\n",
    "# Check for overlap in participants between train and test\n",
    "overlap_check = conn.execute('''\n",
    "    SELECT COUNT(*) FROM (\n",
    "        SELECT subject FROM \"cmi_detect_behavior_with_sensor_data\".train \n",
    "        INTERSECT \n",
    "        SELECT subject FROM \"cmi_detect_behavior_with_sensor_data\".test\n",
    "    )\n",
    "''').fetchone()[0]\n",
    "\n",
    "print(f\"Participants appearing in both train and test: {overlap_check}\")\n",
    "\n",
    "print(\"\\n=== SEQUENCE ANALYSIS ===\")\n",
    "\n",
    "# Sequence counts\n",
    "train_sequences = conn.execute('SELECT COUNT(DISTINCT sequence_id) FROM \"cmi_detect_behavior_with_sensor_data\".train').fetchone()[0]\n",
    "test_sequences = conn.execute('SELECT COUNT(DISTINCT sequence_id) FROM \"cmi_detect_behavior_with_sensor_data\".test').fetchone()[0]\n",
    "\n",
    "print(f\"Unique sequences in train: {train_sequences}\")\n",
    "print(f\"Unique sequences in test: {test_sequences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ll1cpx7x8b",
   "metadata": {},
   "source": [
    "# CMI BFRB Detection - Exploratory Data Analysis\n",
    "\n",
    "## Dataset Schema Analysis\n",
    "\n",
    "### Data Structure Overview\n",
    "- **Train table**: Contains sensor data with target labels (behavior, gesture, phase)\n",
    "- **Test table**: Contains sensor data without target labels  \n",
    "- **Demographics tables**: Participant information (age, sex, handedness, physical measurements)\n",
    "\n",
    "### Sensor Features\n",
    "- **IMU sensors**: acc_x/y/z (accelerometer), rot_w/x/y/z (rotation quaternion) = 7 features\n",
    "- **Thermopile sensors**: thm_1 to thm_5 = 5 features  \n",
    "- **ToF sensors**: tof_1 to tof_5 with 64 values each (v0 to v63) = 320 features\n",
    "- **Total sensor features**: 332 per timestep\n",
    "\n",
    "### Target Variables (Train only)\n",
    "- **behavior**: Binary classification (BFRB vs non-BFRB)\n",
    "- **gesture**: Multi-class classification (specific gesture types)\n",
    "- **phase**: Multi-class classification (gesture phases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}