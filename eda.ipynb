{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "uch3chw3wf",
   "metadata": {},
   "source": [
    "# ğŸ† CMI BFRB Detection - EDAç·æ‹¬\n",
    "\n",
    "## ğŸ“Š é‡è¦ãªç™ºè¦‹äº‹é …\n",
    "\n",
    "### âœ… ãƒ‡ãƒ¼ã‚¿å“è³ª\n",
    "- **é«˜å“è³ªIMUãƒ‡ãƒ¼ã‚¿**: åŠ é€Ÿåº¦ã‚»ãƒ³ã‚µãƒ¼ã¯æ¬ æå€¤0%ã€ä¿¡é ¼æ€§ã®é«˜ã„ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³\n",
    "- **éƒ¨åˆ†çš„ã‚»ãƒ³ã‚µãƒ¼æ•…éšœ**: ToF_5ã¨thm_5ãŒ5%ä»¥ä¸Šæ¬ æ â†’ è£œå®Œæˆ¦ç•¥å¿…è¦\n",
    "- **æ™‚ç³»åˆ—é€£ç¶šæ€§**: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã§ã‚®ãƒ£ãƒƒãƒ—ãªã—ã€50Hzä¸€å®šã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "\n",
    "### ğŸ¯ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ç‰¹æ€§\n",
    "- **ãƒã‚¤ãƒŠãƒªåˆ†é¡**: Target/Non-Target = 60/40 â†’ Binary F1ã¯é”æˆå¯èƒ½\n",
    "- **ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹åˆ†é¡**: 18ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã€6:1ã®ä¸å‡è¡¡ â†’ Macro F1ãŒå›°é›£\n",
    "- **æ™‚ç³»åˆ—æ§‹é€ **: å¹³å‡1.4ç§’ã®çŸ­ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ â†’ ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼èªè­˜ã‚¿ã‚¹ã‚¯\n",
    "\n",
    "### ğŸ‘¥ å‚åŠ è€…ãƒ‡ãƒ¼ã‚¿\n",
    "- **å®Œå…¨åˆ†é›¢**: è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆé–“ã§å‚åŠ è€…é‡è¤‡ãªã— âœ…\n",
    "- **å‡ç­‰åˆ†å¸ƒ**: å‚åŠ è€…é–“ãƒ‡ãƒ¼ã‚¿é‡ã¯æ¯”è¼ƒçš„ãƒãƒ©ãƒ³ã‚¹è‰¯å¥½\n",
    "- **å…¨ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã‚«ãƒãƒ¼**: å…¨å‚åŠ è€…ãŒå…¨ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã‚¿ã‚¤ãƒ—ã‚’å®Ÿè¡Œ\n",
    "\n",
    "### ğŸ”— ã‚»ãƒ³ã‚µãƒ¼ç›¸é–¢\n",
    "- **ãƒ¢ãƒ€ãƒªãƒ†ã‚£é–“ä½ç›¸é–¢**: IMU/ToF/æ¸©åº¦ã¯è£œå®Œçš„æƒ…å ±ã‚’æä¾›\n",
    "- **ãƒãƒ«ãƒã‚³ãƒªãƒ‹ã‚¢ãƒªãƒ†ã‚£**: æ·±åˆ»ãªå•é¡Œãªã—ã€èåˆã«é©ã—ã¦ã„ã‚‹\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³æˆ¦ç•¥\n",
    "\n",
    "### ğŸ“ˆ ç›®æ¨™ã‚¹ã‚³ã‚¢\n",
    "- **ç¾å®Ÿçš„ç›®æ¨™**: Combined F1 = 0.60-0.65 (éŠ…ãƒ¡ãƒ€ãƒ«åœ)\n",
    "- **Binary F1**: 0.65-0.70 (æ¯”è¼ƒçš„é”æˆã—ã‚„ã™ã„)\n",
    "- **Macro F1**: 0.55-0.65 (18ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã§å›°é›£)\n",
    "\n",
    "### ğŸ—ï¸ æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ\n",
    "\n",
    "#### Phase 1: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ (Week 1)\n",
    "1. **GroupKFold CV** ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— (participant-based)\n",
    "2. **åŸºæœ¬ç‰¹å¾´é‡**: IMU magnitude, rolling statistics\n",
    "3. **LightGBM** with missing value handling\n",
    "4. **ç›®æ¨™**: CV 0.50+, LB 0.50+\n",
    "\n",
    "#### Phase 2: ç‰¹å¾´å·¥å­¦ (Week 2-3)\n",
    "1. **FFT spectrum** features for IMU\n",
    "2. **ToF PCA** dimensionality reduction  \n",
    "3. **Multimodal fusion** features\n",
    "4. **1D CNN** on raw sensor streams\n",
    "5. **ç›®æ¨™**: CV 0.58+, LB 0.57+\n",
    "\n",
    "#### Phase 3: ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ– (Week 4-5)\n",
    "1. **Multi-branch CNN** (IMU/ToF/Thermopile separate)\n",
    "2. **Ensemble** multiple models\n",
    "3. **Hyperparameter tuning**\n",
    "4. **ç›®æ¨™**: CV 0.62+, LB 0.60+ (éŠ…ãƒ¡ãƒ€ãƒ«)\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ é‡è¦ãªæ³¨æ„ç‚¹\n",
    "\n",
    "### ğŸš¨ ãƒªã‚¹ã‚¯è¦å› \n",
    "1. **Macro F1 difficulty**: 18ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã«ã‚ˆã‚Š0.5ä»¥ä¸‹ã®å¯èƒ½æ€§\n",
    "2. **Sensor 5 missing**: ToF_5/thm_5æ¬ æã«ã‚ˆã‚‹æƒ…å ±æå¤±  \n",
    "3. **CV-LB gap**: äººãƒ™ãƒ¼ã‚¹GroupKFoldã§ã‚ºãƒ¬å¯èƒ½æ€§\n",
    "\n",
    "### ğŸ›¡ï¸ å¯¾ç­–\n",
    "1. **ã‚¯ãƒ©ã‚¹é‡ã¿èª¿æ•´**: Macro F1å‘ä¸Šã®ãŸã‚focal lossç­‰\n",
    "2. **æ¬ æå€¤æˆ¦ç•¥**: imputation + availability indicators\n",
    "3. **CV robustness**: è¤‡æ•°ã‚·ãƒ¼ãƒ‰ã€foldåˆ†æ•£ç›£è¦–\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Next Steps\n",
    "\n",
    "1. **ç‰¹å¾´å·¥å­¦ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³** å®Ÿè£…é–‹å§‹\n",
    "2. **GroupKFold CV** ç’°å¢ƒæ§‹ç¯‰\n",
    "3. **ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«** (tsfresh + LightGBM)\n",
    "4. **é€²æ—ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°** ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰\n",
    "\n",
    "**æœŸå¾…ã•ã‚Œã‚‹æˆæœ**: é©åˆ‡ãªç‰¹å¾´å·¥å­¦ã¨CVæˆ¦ç•¥ã«ã‚ˆã‚Šã€éŠ…ãƒ¡ãƒ€ãƒ«åœå†…(top 200)åˆ°é”å¯èƒ½\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ è¿½åŠ åˆ†ææ¨å¥¨é …ç›®\n",
    "\n",
    "### ğŸ” ãƒ‡ãƒ¼ã‚¿å“è³ªå¼·åŒ–\n",
    "- **ã‚»ãƒ³ã‚µãƒ¼å€¤ã®ç‰©ç†ç¯„å›²å¤–ã‚Œæ¤œçŸ¥**: IMU Â±16gã€ToF 0-4000mm ç¯„å›²ãƒã‚§ãƒƒã‚¯\n",
    "- **ãƒ‰ãƒªãƒ•ãƒˆ/ã‚ªãƒ•ã‚»ãƒƒãƒˆç¢ºèª**: ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ¥ã®å¹³å‡ãƒ»åˆ†æ•£æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "- **æ™‚ç³»åˆ—é‡è¤‡ãƒ»é †åºä¹±ã‚Œ**: duplicate timestamp æ¤œå‡º\n",
    "\n",
    "### ğŸ¯ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ»ä¸å‡è¡¡å¯¾ç­–æ¤œè¨¼\n",
    "- **ã‚¯ãƒ©ã‚¹é–“ã®æ™‚é–“çš„åˆ†å¸ƒ**: ç‰¹å®šã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã®æ™‚é–“ååœ¨ãƒã‚§ãƒƒã‚¯\n",
    "- **ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è€æ€§**: Minority ã‚¯ãƒ©ã‚¹30/50/70%æ¬ æã§ã®æ„Ÿåº¦ãƒ†ã‚¹ãƒˆ\n",
    "\n",
    "### ğŸ”— ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ç›¸é–¢æ·±æ˜ã‚Š\n",
    "- **Cross-correlation lag plot**: IMU vs ToF ã®é…å»¶ç›¸é–¢ï¼ˆÂ±n lagï¼‰\n",
    "- **UMAP 2D ãƒ—ãƒ­ãƒƒãƒˆ**: ãƒ¢ãƒ€ãƒªãƒ†ã‚£åˆ¥æŠ•å½±ã€ãƒ©ãƒ™ãƒ«&å‚åŠ è€…è‰²åˆ†ã‘\n",
    "\n",
    "### ğŸ“Š CV-LB ã‚®ãƒ£ãƒƒãƒ—äºˆæ¸¬å®Ÿé¨“\n",
    "- **ç–‘ä¼¼LBå®Ÿé¨“**: 80/20% participant split ã§ã® hold-out ä½œæˆ\n",
    "- **CVÃ—ç–‘ä¼¼LB æ•£å¸ƒå›³**: 10å›ã‚·ãƒ¼ãƒ‰å®Ÿé¨“ã€å‚¾ãâ‰ˆ1 ç¢ºèª\n",
    "\n",
    "ã“ã‚Œã‚‰ã‚’è¿½åŠ ã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«åŒ–ãƒ•ã‚§ãƒ¼ã‚ºã§ã®ã€Œæƒ³å®šå¤–ã€ã‚’å¤§å¹…ã«æ¸›ã‚‰ã›ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "nx0wf098y0k",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ã‚»ãƒ³ã‚µãƒ¼ãƒãƒ£ãƒ³ãƒãƒ«åˆ¥æ¬ æå€¤ãƒ»åˆ†æ•£åˆ†æ\n",
      "============================================================\n",
      "ğŸ¯ IMUã‚»ãƒ³ã‚µãƒ¼ (acc_x/y/z, rot_w/x/y/z)\n",
      "channel  total_samples  valid_samples  missing_count  missing_pct  mean_val  std_val  min_val  max_val\n",
      "  acc_x         574945         574945              0         0.00    1.6400   5.7813 -34.5859  46.3281\n",
      "  acc_y         574945         574945              0         0.00    1.7907   5.0039 -24.4023  27.1836\n",
      "  acc_z         574945         574945              0         0.00   -0.4598   6.0965 -42.8555  30.0781\n",
      "  rot_w         574945         571253           3692         0.64    0.3604   0.2257   0.0000   0.9994\n",
      "  rot_x         574945         571253           3692         0.64   -0.1199   0.4655  -0.9991   0.9998\n",
      "\n",
      "ğŸŒ¡ï¸ æ¸©åº¦ã‚»ãƒ³ã‚µãƒ¼ (thm_1ã€œ5)\n",
      "channel  total_samples  valid_samples  missing_count  missing_pct  mean_val  std_val  min_val  max_val\n",
      "  thm_1         574945         567958           6987         1.22     27.08     3.23    -0.37    38.46\n",
      "  thm_2         574945         567307           7638         1.33     27.13     2.94    21.96    37.58\n",
      "  thm_3         574945         568473           6472         1.13     26.70     4.12     0.00    37.29\n",
      "  thm_4         574945         568721           6224         1.08     27.56     2.25    22.38    39.59\n",
      "  thm_5         574945         541659          33286         5.79     26.67     2.44    22.05    37.68\n",
      "\n",
      "ğŸ“¡ ToFã‚»ãƒ³ã‚µãƒ¼ä»£è¡¨ãƒãƒ£ãƒ³ãƒãƒ« (å„ã‚»ãƒ³ã‚µãƒ¼ã®v0, v31, v63)\n",
      "  channel  total_samples  valid_samples  missing_count  missing_pct  mean_val  std_val  min_val  max_val\n",
      " tof_1_v0         574945         568721           6224         1.08      54.6     72.8     -1.0    249.0\n",
      "tof_1_v31         574945         568721           6224         1.08      48.0     69.5     -1.0    249.0\n",
      "tof_1_v63         574945         568721           6224         1.08      37.5     59.0     -1.0    249.0\n",
      " tof_5_v0         574945         544803          30142         5.24      60.2     72.4     -1.0    249.0\n",
      "\n",
      "ğŸ“Š ã‚»ãƒ³ã‚µãƒ¼å“è³ªã‚µãƒãƒªãƒ¼\n",
      "IMUã‚»ãƒ³ã‚µãƒ¼å“è³ª: ğŸŸ¢ å„ªç§€\n",
      "æ¸©åº¦ã‚»ãƒ³ã‚µãƒ¼å“è³ª: ğŸ”´ æ³¨æ„\n",
      "ToFã‚»ãƒ³ã‚µãƒ¼å“è³ª: ğŸ”´ æ³¨æ„\n",
      "\n",
      "âš ï¸ é«˜æ¬ æã‚»ãƒ³ã‚µãƒ¼ (>5%):\n",
      "  â€¢ thm_5: 5.79%\n",
      "  â€¢ tof_5_v0: 5.24%\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ã‚»ãƒ³ã‚µãƒ¼ãƒãƒ£ãƒ³ãƒãƒ«ã”ã¨ã®æ¬ æå€¤ãƒ»åˆ†æ•£åˆ†æ\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š\n",
    "conn = duckdb.connect('/home/wsl/dev/my-study/ml/ml-note/kaggle_datasets.duckdb')\n",
    "\n",
    "print(\"ğŸ” ã‚»ãƒ³ã‚µãƒ¼ãƒãƒ£ãƒ³ãƒãƒ«åˆ¥æ¬ æå€¤ãƒ»åˆ†æ•£åˆ†æ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. IMUã‚»ãƒ³ã‚µãƒ¼ã®æ¬ æå€¤ãƒ»åˆ†æ•£åˆ†æ\n",
    "print(\"ğŸ¯ IMUã‚»ãƒ³ã‚µãƒ¼ (acc_x/y/z, rot_w/x/y/z)\")\n",
    "imu_analysis = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        'acc_x' as channel,\n",
    "        COUNT(*) as total_samples,\n",
    "        COUNT(acc_x) as valid_samples,\n",
    "        COUNT(*) - COUNT(acc_x) as missing_count,\n",
    "        ROUND((COUNT(*) - COUNT(acc_x)) * 100.0 / COUNT(*), 2) as missing_pct,\n",
    "        ROUND(AVG(acc_x), 4) as mean_val,\n",
    "        ROUND(STDDEV(acc_x), 4) as std_val,\n",
    "        ROUND(MIN(acc_x), 4) as min_val,\n",
    "        ROUND(MAX(acc_x), 4) as max_val\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'acc_y',\n",
    "        COUNT(*),\n",
    "        COUNT(acc_y),\n",
    "        COUNT(*) - COUNT(acc_y),\n",
    "        ROUND((COUNT(*) - COUNT(acc_y)) * 100.0 / COUNT(*), 2),\n",
    "        ROUND(AVG(acc_y), 4),\n",
    "        ROUND(STDDEV(acc_y), 4),\n",
    "        ROUND(MIN(acc_y), 4),\n",
    "        ROUND(MAX(acc_y), 4)\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'acc_z',\n",
    "        COUNT(*),\n",
    "        COUNT(acc_z),\n",
    "        COUNT(*) - COUNT(acc_z),\n",
    "        ROUND((COUNT(*) - COUNT(acc_z)) * 100.0 / COUNT(*), 2),\n",
    "        ROUND(AVG(acc_z), 4),\n",
    "        ROUND(STDDEV(acc_z), 4),\n",
    "        ROUND(MIN(acc_z), 4),\n",
    "        ROUND(MAX(acc_z), 4)\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'rot_w',\n",
    "        COUNT(*),\n",
    "        COUNT(rot_w),\n",
    "        COUNT(*) - COUNT(rot_w),\n",
    "        ROUND((COUNT(*) - COUNT(rot_w)) * 100.0 / COUNT(*), 2),\n",
    "        ROUND(AVG(rot_w), 4),\n",
    "        ROUND(STDDEV(rot_w), 4),\n",
    "        ROUND(MIN(rot_w), 4),\n",
    "        ROUND(MAX(rot_w), 4)\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'rot_x',\n",
    "        COUNT(*),\n",
    "        COUNT(rot_x),\n",
    "        COUNT(*) - COUNT(rot_x),\n",
    "        ROUND((COUNT(*) - COUNT(rot_x)) * 100.0 / COUNT(*), 2),\n",
    "        ROUND(AVG(rot_x), 4),\n",
    "        ROUND(STDDEV(rot_x), 4),\n",
    "        ROUND(MIN(rot_x), 4),\n",
    "        ROUND(MAX(rot_x), 4)\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(imu_analysis.to_string(index=False))\n",
    "\n",
    "# 2. æ¸©åº¦ã‚»ãƒ³ã‚µãƒ¼ã®æ¬ æå€¤ãƒ»åˆ†æ•£åˆ†æ\n",
    "print(\"\\nğŸŒ¡ï¸ æ¸©åº¦ã‚»ãƒ³ã‚µãƒ¼ (thm_1ã€œ5)\")\n",
    "thm_analysis = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        'thm_1' as channel,\n",
    "        COUNT(*) as total_samples,\n",
    "        COUNT(thm_1) as valid_samples,\n",
    "        COUNT(*) - COUNT(thm_1) as missing_count,\n",
    "        ROUND((COUNT(*) - COUNT(thm_1)) * 100.0 / COUNT(*), 2) as missing_pct,\n",
    "        ROUND(AVG(thm_1), 2) as mean_val,\n",
    "        ROUND(STDDEV(thm_1), 2) as std_val,\n",
    "        ROUND(MIN(thm_1), 2) as min_val,\n",
    "        ROUND(MAX(thm_1), 2) as max_val\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT 'thm_2', COUNT(*), COUNT(thm_2), COUNT(*) - COUNT(thm_2), \n",
    "           ROUND((COUNT(*) - COUNT(thm_2)) * 100.0 / COUNT(*), 2),\n",
    "           ROUND(AVG(thm_2), 2), ROUND(STDDEV(thm_2), 2), ROUND(MIN(thm_2), 2), ROUND(MAX(thm_2), 2)\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT 'thm_3', COUNT(*), COUNT(thm_3), COUNT(*) - COUNT(thm_3), \n",
    "           ROUND((COUNT(*) - COUNT(thm_3)) * 100.0 / COUNT(*), 2),\n",
    "           ROUND(AVG(thm_3), 2), ROUND(STDDEV(thm_3), 2), ROUND(MIN(thm_3), 2), ROUND(MAX(thm_3), 2)\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT 'thm_4', COUNT(*), COUNT(thm_4), COUNT(*) - COUNT(thm_4), \n",
    "           ROUND((COUNT(*) - COUNT(thm_4)) * 100.0 / COUNT(*), 2),\n",
    "           ROUND(AVG(thm_4), 2), ROUND(STDDEV(thm_4), 2), ROUND(MIN(thm_4), 2), ROUND(MAX(thm_4), 2)\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT 'thm_5', COUNT(*), COUNT(thm_5), COUNT(*) - COUNT(thm_5), \n",
    "           ROUND((COUNT(*) - COUNT(thm_5)) * 100.0 / COUNT(*), 2),\n",
    "           ROUND(AVG(thm_5), 2), ROUND(STDDEV(thm_5), 2), ROUND(MIN(thm_5), 2), ROUND(MAX(thm_5), 2)\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(thm_analysis.to_string(index=False))\n",
    "\n",
    "# 3. ToFã‚»ãƒ³ã‚µãƒ¼ã®ä»£è¡¨ãƒãƒ£ãƒ³ãƒãƒ«ã®æ¬ æå€¤ãƒ»åˆ†æ•£åˆ†æ\n",
    "print(\"\\nğŸ“¡ ToFã‚»ãƒ³ã‚µãƒ¼ä»£è¡¨ãƒãƒ£ãƒ³ãƒãƒ« (å„ã‚»ãƒ³ã‚µãƒ¼ã®v0, v31, v63)\")\n",
    "tof_analysis = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        'tof_1_v0' as channel,\n",
    "        COUNT(*) as total_samples,\n",
    "        COUNT(tof_1_v0) as valid_samples,\n",
    "        COUNT(*) - COUNT(tof_1_v0) as missing_count,\n",
    "        ROUND((COUNT(*) - COUNT(tof_1_v0)) * 100.0 / COUNT(*), 2) as missing_pct,\n",
    "        ROUND(AVG(tof_1_v0), 1) as mean_val,\n",
    "        ROUND(STDDEV(tof_1_v0), 1) as std_val,\n",
    "        ROUND(MIN(tof_1_v0), 1) as min_val,\n",
    "        ROUND(MAX(tof_1_v0), 1) as max_val\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT 'tof_1_v31', COUNT(*), COUNT(tof_1_v31), COUNT(*) - COUNT(tof_1_v31), \n",
    "           ROUND((COUNT(*) - COUNT(tof_1_v31)) * 100.0 / COUNT(*), 2),\n",
    "           ROUND(AVG(tof_1_v31), 1), ROUND(STDDEV(tof_1_v31), 1), ROUND(MIN(tof_1_v31), 1), ROUND(MAX(tof_1_v31), 1)\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT 'tof_1_v63', COUNT(*), COUNT(tof_1_v63), COUNT(*) - COUNT(tof_1_v63), \n",
    "           ROUND((COUNT(*) - COUNT(tof_1_v63)) * 100.0 / COUNT(*), 2),\n",
    "           ROUND(AVG(tof_1_v63), 1), ROUND(STDDEV(tof_1_v63), 1), ROUND(MIN(tof_1_v63), 1), ROUND(MAX(tof_1_v63), 1)\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT 'tof_5_v0', COUNT(*), COUNT(tof_5_v0), COUNT(*) - COUNT(tof_5_v0), \n",
    "           ROUND((COUNT(*) - COUNT(tof_5_v0)) * 100.0 / COUNT(*), 2),\n",
    "           ROUND(AVG(tof_5_v0), 1), ROUND(STDDEV(tof_5_v0), 1), ROUND(MIN(tof_5_v0), 1), ROUND(MAX(tof_5_v0), 1)\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(tof_analysis.to_string(index=False))\n",
    "\n",
    "# 4. ã‚»ãƒ³ã‚µãƒ¼å“è³ªã‚µãƒãƒªãƒ¼\n",
    "print(\"\\nğŸ“Š ã‚»ãƒ³ã‚µãƒ¼å“è³ªã‚µãƒãƒªãƒ¼\")\n",
    "print(\"IMUã‚»ãƒ³ã‚µãƒ¼å“è³ª:\", \"ğŸŸ¢ å„ªç§€\" if imu_analysis['missing_pct'].max() < 1 else \"ğŸŸ¡ è‰¯å¥½\" if imu_analysis['missing_pct'].max() < 5 else \"ğŸ”´ æ³¨æ„\")\n",
    "print(\"æ¸©åº¦ã‚»ãƒ³ã‚µãƒ¼å“è³ª:\", \"ğŸŸ¢ å„ªç§€\" if thm_analysis['missing_pct'].max() < 1 else \"ğŸŸ¡ è‰¯å¥½\" if thm_analysis['missing_pct'].max() < 5 else \"ğŸ”´ æ³¨æ„\")\n",
    "print(\"ToFã‚»ãƒ³ã‚µãƒ¼å“è³ª:\", \"ğŸŸ¢ å„ªç§€\" if tof_analysis['missing_pct'].max() < 1 else \"ğŸŸ¡ è‰¯å¥½\" if tof_analysis['missing_pct'].max() < 5 else \"ğŸ”´ æ³¨æ„\")\n",
    "\n",
    "# 5. é«˜æ¬ æã‚»ãƒ³ã‚µãƒ¼ã®ç‰¹å®š\n",
    "high_missing = []\n",
    "for _, row in thm_analysis.iterrows():\n",
    "    if row['missing_pct'] > 5:\n",
    "        high_missing.append(f\"{row['channel']}: {row['missing_pct']}%\")\n",
    "        \n",
    "for _, row in tof_analysis.iterrows():\n",
    "    if row['missing_pct'] > 5:\n",
    "        high_missing.append(f\"{row['channel']}: {row['missing_pct']}%\")\n",
    "\n",
    "if high_missing:\n",
    "    print(\"\\nâš ï¸ é«˜æ¬ æã‚»ãƒ³ã‚µãƒ¼ (>5%):\")\n",
    "    for sensor in high_missing:\n",
    "        print(f\"  â€¢ {sensor}\")\n",
    "else:\n",
    "    print(\"\\nâœ… å…¨ã‚»ãƒ³ã‚µãƒ¼ãŒè‰¯å¥½ãªå“è³ª (æ¬ æ<5%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "qgij5xrt4cs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ­ è¡Œå‹•ãƒ©ãƒ™ãƒ«å‡ºç¾é »åº¦ãƒ»é•·ã•åˆ†å¸ƒåˆ†æ\n",
      "============================================================\n",
      "ğŸ¯ ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ãƒ©ãƒ™ãƒ«å‡ºç¾é »åº¦ (18ã‚¯ãƒ©ã‚¹)\n",
      "                                   gesture  frequency  percentage  sequences  participants\n",
      "                             Text on phone      58462       10.17        640            81\n",
      "                            Neck - scratch      56619        9.85        640            81\n",
      "                       Eyebrow - pull hair      44305        7.71        638            81\n",
      "                        Forehead - scratch      40923        7.12        640            81\n",
      "                  Forehead - pull hairline      40802        7.10        640            81\n",
      "                     Above ear - pull hair      40560        7.05        638            81\n",
      "                         Neck - pinch skin      40507        7.05        640            81\n",
      "                       Eyelash - pull hair      40218        7.00        640            81\n",
      "                        Cheek - pinch skin      40124        6.98        637            81\n",
      "                                Wave hello      34356        5.98        478            81\n",
      "                         Write name in air      31267        5.44        477            81\n",
      "                 Pull air toward your face      30743        5.35        477            81\n",
      "Feel around in tray and pull out an object      17114        2.98        161            81\n",
      "                            Glasses on/off      13542        2.36        161            81\n",
      "                     Drink from bottle/cup      13093        2.28        161            81\n",
      "                     Scratch knee/leg skin      12328        2.14        161            81\n",
      "                         Write name on leg      10138        1.76        161            81\n",
      "                       Pinch knee/leg skin       9844        1.71        161            81\n",
      "\n",
      "ğŸ“Š ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡åº¦: 5.9:1 (ä¸­)\n",
      "\n",
      "ğŸ”„ è¡Œå‹•ãƒ•ã‚§ãƒ¼ã‚ºåˆ†å¸ƒ\n",
      "                                 behavior  frequency  percentage  sequences\n",
      "                         Performs gesture     255817       44.49       8150\n",
      "            Moves hand to target location     156474       27.22       4102\n",
      "                  Hand at target location      95173       16.55       8151\n",
      "Relaxes and moves hand to target location      67481       11.74       4049\n",
      "\n",
      "ğŸ“ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·åˆ†å¸ƒåˆ†æ\n",
      "ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·çµ±è¨ˆ (ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—):\n",
      "  å¹³å‡: 70.5\n",
      "  ä¸­å¤®å€¤: 59.0\n",
      "  æ¨™æº–åå·®: 35.4\n",
      "  æœ€å°-æœ€å¤§: 29 - 700\n",
      "  Q1-Q3: 51.0 - 78.0\n",
      "\n",
      "ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·çµ±è¨ˆ (ç§’):\n",
      "  å¹³å‡: 1.41ç§’\n",
      "  ä¸­å¤®å€¤: 1.18ç§’\n",
      "  æœ€é•·: 14.0ç§’\n",
      "\n",
      "ğŸ­ ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼åˆ¥ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·\n",
      "                                   gesture  num_sequences  avg_length_timesteps  avg_length_seconds  min_length  max_length  std_length\n",
      "Feel around in tray and pull out an object            161                 106.3                2.13          50         322        52.3\n",
      "                             Text on phone            640                  91.3                1.83          29         390        43.4\n",
      "                            Neck - scratch            640                  88.5                1.77          41         700        73.3\n",
      "                            Glasses on/off            161                  84.1                1.68          43         293        41.0\n",
      "                     Drink from bottle/cup            161                  81.3                1.63          34         176        29.4\n",
      "                     Scratch knee/leg skin            161                  76.6                1.53          42         229        37.4\n",
      "                                Wave hello            478                  71.9                1.44          34         230        29.0\n",
      "                       Eyebrow - pull hair            638                  69.4                1.39          34         371        36.7\n",
      "                         Write name in air            477                  65.5                1.31          39         374        26.8\n",
      "                 Pull air toward your face            477                  64.5                1.29          36         188        19.3\n",
      "                        Forehead - scratch            640                  63.9                1.28          36         245        21.2\n",
      "                  Forehead - pull hairline            640                  63.8                1.28          35         190        19.3\n",
      "                     Above ear - pull hair            638                  63.6                1.27          38         191        19.5\n",
      "                         Neck - pinch skin            640                  63.3                1.27          39         234        21.3\n",
      "                         Write name on leg            161                  63.0                1.26          42         123        17.6\n",
      "                        Cheek - pinch skin            637                  63.0                1.26          38         302        23.2\n",
      "                       Eyelash - pull hair            640                  62.8                1.26          39         200        18.9\n",
      "                       Pinch knee/leg skin            161                  61.1                1.22          35         133        18.0\n",
      "\n",
      "ğŸ¯ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—åˆ†å¸ƒ\n",
      "sequence_type  frequency  percentage  sequences  participants\n",
      "       Target     344058       59.84       5113            81\n",
      "   Non-Target     230887       40.16       3038            81\n",
      "\n",
      "â³ è¡Œå‹•ç¶™ç¶šæ€§åˆ†æ\n",
      "                                 behavior  num_runs  avg_run_length  min_run_length  max_run_length  avg_run_seconds\n",
      "            Moves hand to target location      4102            38.1               2             653             0.76\n",
      "                         Performs gesture      8150            31.4               3              71             0.63\n",
      "Relaxes and moves hand to target location      4049            16.7               3              98             0.33\n",
      "                  Hand at target location      8151            11.7               1             108             0.23\n",
      "\n",
      "ğŸ“ˆ é‡è¦ãªçŸ¥è¦‹:\n",
      "  â€¢ æœ€é »å‡ºã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼: Text on phone (10.17%)\n",
      "  â€¢ æœ€ç¨€å°‘ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼: Pinch knee/leg skin (1.71%)\n",
      "  â€¢ å¹³å‡ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: 1.41ç§’\n",
      "  â€¢ Target/Non-Targetæ¯”: 59.84% vs 40.16%\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# è¡Œå‹•ãƒ©ãƒ™ãƒ«ã®å‡ºç¾é »åº¦ã¨é•·ã•åˆ†å¸ƒåˆ†æ\n",
    "print(\"ğŸ­ è¡Œå‹•ãƒ©ãƒ™ãƒ«å‡ºç¾é »åº¦ãƒ»é•·ã•åˆ†å¸ƒåˆ†æ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ãƒ©ãƒ™ãƒ«ã®å‡ºç¾é »åº¦\n",
    "print(\"ğŸ¯ ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ãƒ©ãƒ™ãƒ«å‡ºç¾é »åº¦ (18ã‚¯ãƒ©ã‚¹)\")\n",
    "gesture_frequency = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        gesture,\n",
    "        COUNT(*) as frequency,\n",
    "        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".train), 2) as percentage,\n",
    "        COUNT(DISTINCT sequence_id) as sequences,\n",
    "        COUNT(DISTINCT subject) as participants\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    GROUP BY gesture\n",
    "    ORDER BY frequency DESC\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(gesture_frequency.to_string(index=False))\n",
    "\n",
    "# ä¸å‡è¡¡åº¦ã®è¨ˆç®—\n",
    "max_freq = gesture_frequency['frequency'].max()\n",
    "min_freq = gesture_frequency['frequency'].min()\n",
    "imbalance_ratio = max_freq / min_freq\n",
    "print(f\"\\nğŸ“Š ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡åº¦: {imbalance_ratio:.1f}:1 ({'é«˜' if imbalance_ratio > 10 else 'ä¸­' if imbalance_ratio > 3 else 'ä½'})\")\n",
    "\n",
    "# 2. è¡Œå‹•ï¼ˆbehaviorï¼‰ãƒ•ã‚§ãƒ¼ã‚ºã®åˆ†å¸ƒ\n",
    "print(\"\\nğŸ”„ è¡Œå‹•ãƒ•ã‚§ãƒ¼ã‚ºåˆ†å¸ƒ\")\n",
    "behavior_distribution = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        behavior,\n",
    "        COUNT(*) as frequency,\n",
    "        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".train), 2) as percentage,\n",
    "        COUNT(DISTINCT sequence_id) as sequences\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    GROUP BY behavior\n",
    "    ORDER BY frequency DESC\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(behavior_distribution.to_string(index=False))\n",
    "\n",
    "# 3. ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã®åˆ†å¸ƒåˆ†æ\n",
    "print(\"\\nğŸ“ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·åˆ†å¸ƒåˆ†æ\")\n",
    "sequence_lengths = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        sequence_id,\n",
    "        gesture,\n",
    "        COUNT(*) as length_timesteps,\n",
    "        ROUND(COUNT(*) / 50.0, 2) as length_seconds\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    GROUP BY sequence_id, gesture\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "length_stats = sequence_lengths['length_timesteps'].describe()\n",
    "print(f\"ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·çµ±è¨ˆ (ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—):\")\n",
    "print(f\"  å¹³å‡: {length_stats['mean']:.1f}\")\n",
    "print(f\"  ä¸­å¤®å€¤: {length_stats['50%']:.1f}\")\n",
    "print(f\"  æ¨™æº–åå·®: {length_stats['std']:.1f}\")\n",
    "print(f\"  æœ€å°-æœ€å¤§: {length_stats['min']:.0f} - {length_stats['max']:.0f}\")\n",
    "print(f\"  Q1-Q3: {length_stats['25%']:.1f} - {length_stats['75%']:.1f}\")\n",
    "\n",
    "print(f\"\\nã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·çµ±è¨ˆ (ç§’):\")\n",
    "print(f\"  å¹³å‡: {length_stats['mean']/50:.2f}ç§’\")\n",
    "print(f\"  ä¸­å¤®å€¤: {length_stats['50%']/50:.2f}ç§’\")\n",
    "print(f\"  æœ€é•·: {length_stats['max']/50:.1f}ç§’\")\n",
    "\n",
    "# 4. ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼åˆ¥ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·åˆ†æ\n",
    "print(\"\\nğŸ­ ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼åˆ¥ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·\")\n",
    "gesture_length_stats = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        gesture,\n",
    "        COUNT(DISTINCT sequence_id) as num_sequences,\n",
    "        ROUND(AVG(length), 1) as avg_length_timesteps,\n",
    "        ROUND(AVG(length) / 50.0, 2) as avg_length_seconds,\n",
    "        MIN(length) as min_length,\n",
    "        MAX(length) as max_length,\n",
    "        ROUND(STDDEV(length), 1) as std_length\n",
    "    FROM (\n",
    "        SELECT \n",
    "            sequence_id,\n",
    "            gesture,\n",
    "            COUNT(*) as length\n",
    "        FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "        GROUP BY sequence_id, gesture\n",
    "    )\n",
    "    GROUP BY gesture\n",
    "    ORDER BY avg_length_timesteps DESC\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(gesture_length_stats.to_string(index=False))\n",
    "\n",
    "# 5. sequence_typeï¼ˆTarget vs Non-Targetï¼‰åˆ†å¸ƒ\n",
    "print(\"\\nğŸ¯ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—åˆ†å¸ƒ\")\n",
    "sequence_type_dist = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        sequence_type,\n",
    "        COUNT(*) as frequency,\n",
    "        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".train), 2) as percentage,\n",
    "        COUNT(DISTINCT sequence_id) as sequences,\n",
    "        COUNT(DISTINCT subject) as participants\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    GROUP BY sequence_type\n",
    "    ORDER BY frequency DESC\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(sequence_type_dist.to_string(index=False))\n",
    "\n",
    "# 6. è¡Œå‹•ç¶™ç¶šæ€§åˆ†æï¼ˆåŒä¸€è¡Œå‹•ã®é€£ç¶šé•·ï¼‰\n",
    "print(\"\\nâ³ è¡Œå‹•ç¶™ç¶šæ€§åˆ†æ\")\n",
    "behavior_continuity = conn.execute(\"\"\"\n",
    "    WITH behavior_runs AS (\n",
    "        SELECT \n",
    "            sequence_id,\n",
    "            behavior,\n",
    "            COUNT(*) as run_length\n",
    "        FROM (\n",
    "            SELECT \n",
    "                sequence_id,\n",
    "                behavior,\n",
    "                ROW_NUMBER() OVER (PARTITION BY sequence_id ORDER BY sequence_counter) - \n",
    "                ROW_NUMBER() OVER (PARTITION BY sequence_id, behavior ORDER BY sequence_counter) as grp\n",
    "            FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "        )\n",
    "        GROUP BY sequence_id, behavior, grp\n",
    "    )\n",
    "    SELECT \n",
    "        behavior,\n",
    "        COUNT(*) as num_runs,\n",
    "        ROUND(AVG(run_length), 1) as avg_run_length,\n",
    "        MIN(run_length) as min_run_length,\n",
    "        MAX(run_length) as max_run_length,\n",
    "        ROUND(AVG(run_length) / 50.0, 2) as avg_run_seconds\n",
    "    FROM behavior_runs\n",
    "    GROUP BY behavior\n",
    "    ORDER BY avg_run_length DESC\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(behavior_continuity.to_string(index=False))\n",
    "\n",
    "print(\"\\nğŸ“ˆ é‡è¦ãªçŸ¥è¦‹:\")\n",
    "print(f\"  â€¢ æœ€é »å‡ºã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼: {gesture_frequency.iloc[0]['gesture']} ({gesture_frequency.iloc[0]['percentage']}%)\")\n",
    "print(f\"  â€¢ æœ€ç¨€å°‘ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼: {gesture_frequency.iloc[-1]['gesture']} ({gesture_frequency.iloc[-1]['percentage']}%)\")\n",
    "print(f\"  â€¢ å¹³å‡ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {length_stats['mean']/50:.2f}ç§’\")\n",
    "print(f\"  â€¢ Target/Non-Targetæ¯”: {sequence_type_dist.iloc[0]['percentage']}% vs {sequence_type_dist.iloc[1]['percentage']}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "xqdwrrum7o",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘¥ å‚åŠ è€…ãƒ»ã‚»ãƒƒã‚·ãƒ§ãƒ³å˜ä½ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ç¢ºèª\n",
      "============================================================\n",
      "ğŸ” è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆé–“å‚åŠ è€…é‡è¤‡ç¢ºèª\n",
      "                 dataset  participant_count\n",
      "      Train participants                 81\n",
      "       Test participants                  2\n",
      "Overlapping participants                  0\n",
      "\n",
      "ğŸ“Š å‚åŠ è€…åˆ¥ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒï¼ˆCVè¨­è¨ˆå‚è€ƒï¼‰\n",
      "å‚åŠ è€…åˆ¥ãƒ‡ãƒ¼ã‚¿é‡ï¼ˆä¸Šä½10åï¼‰:\n",
      "    subject  sequences  total_timesteps  unique_gestures  data_percentage  target_pct\n",
      "SUBJ_040733        102            10848               18             1.89        60.1\n",
      "SUBJ_052342        102            10393               18             1.81        59.2\n",
      "SUBJ_023739        102             9154               18             1.59        60.9\n",
      "SUBJ_059520        102             8947               18             1.56        59.0\n",
      "SUBJ_058967        102             8718               18             1.52        60.8\n",
      "SUBJ_030676        102             8700               18             1.51        59.7\n",
      "SUBJ_032761        102             8420               18             1.46        59.3\n",
      "SUBJ_061552        102             8412               18             1.46        61.6\n",
      "SUBJ_017807        102             8409               18             1.46        58.8\n",
      "SUBJ_032704        102             8322               18             1.45        59.5\n",
      "\n",
      "ğŸ“ˆ å‚åŠ è€…ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒçµ±è¨ˆ\n",
      "å‚åŠ è€…çµ±è¨ˆ:\n",
      "  ç·å‚åŠ è€…æ•°: 81\n",
      "  ãƒ‡ãƒ¼ã‚¿é‡: å¹³å‡7098.1Â±1033.7 timesteps\n",
      "  ãƒ‡ãƒ¼ã‚¿é‡ç¯„å›²: 4008 - 10848 timesteps\n",
      "  ãƒ‡ãƒ¼ã‚¿ä¸å‡è¡¡æ¯”: 2.7:1\n",
      "  å¹³å‡ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: 100.6\n",
      "  å¹³å‡ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ç¨®é¡: 18.0\n",
      "\n",
      "ğŸ­ ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼Ã—å‚åŠ è€…ã‚«ãƒãƒ¬ãƒƒã‚¸\n",
      "ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼åˆ¥å‚åŠ è€…ã‚«ãƒãƒ¬ãƒƒã‚¸:\n",
      "                                   gesture  participants_with_gesture  coverage_percentage\n",
      "                       Eyelash - pull hair                         81                100.0\n",
      "                     Above ear - pull hair                         81                100.0\n",
      "                     Scratch knee/leg skin                         81                100.0\n",
      "Feel around in tray and pull out an object                         81                100.0\n",
      "                         Write name in air                         81                100.0\n",
      "                         Neck - pinch skin                         81                100.0\n",
      "                                Wave hello                         81                100.0\n",
      "                            Neck - scratch                         81                100.0\n",
      "                  Forehead - pull hairline                         81                100.0\n",
      "                     Drink from bottle/cup                         81                100.0\n",
      "                       Eyebrow - pull hair                         81                100.0\n",
      "                        Forehead - scratch                         81                100.0\n",
      "                        Cheek - pinch skin                         81                100.0\n",
      "                         Write name on leg                         81                100.0\n",
      "                 Pull air toward your face                         81                100.0\n",
      "                            Glasses on/off                         81                100.0\n",
      "                             Text on phone                         81                100.0\n",
      "                       Pinch knee/leg skin                         81                100.0\n",
      "\n",
      "ğŸ”„ CVãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰è¨­è¨ˆæ¨å¥¨\n",
      "GroupKFoldæ¨å¥¨è¨­å®š:\n",
      "  â€¢ ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰æ•°: 5\n",
      "  â€¢ å‚åŠ è€…/ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰: ~16äºº\n",
      "  â€¢ ãƒ‡ãƒ¼ã‚¿/ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰: ~20%\n",
      "  â€¢ ã‚°ãƒ«ãƒ¼ãƒ—å¤‰æ•°: subject (å‚åŠ è€…ID)\n",
      "\n",
      "âœ… ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ç¢ºèªçµæœ\n",
      "  ğŸŸ¢ å‚åŠ è€…é‡è¤‡ãªã— - ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ãƒªã‚¹ã‚¯ãªã—\n",
      "  ğŸŸ¢ GroupKFoldã§CVå®Ÿè£…å¯èƒ½\n",
      "  å‚åŠ è€…ãƒ‡ãƒ¼ã‚¿å‡è¡¡æ€§: ğŸŸ¢ è‰¯å¥½\n",
      "  ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã‚«ãƒãƒ¬ãƒƒã‚¸: ğŸŸ¢ è‰¯å¥½ (æœ€å°100.0%)\n",
      "\n",
      "ğŸ“ CVæˆ¦ç•¥æ¨å¥¨:\n",
      "```python\n",
      "from sklearn.model_selection import GroupKFold\n",
      "cv = GroupKFold(n_splits=5)\n",
      "groups = train_data['subject']\n",
      "```\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# å‚åŠ è€…/ã‚»ãƒƒã‚·ãƒ§ãƒ³å˜ä½ã®ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ç¢ºèª\n",
    "print(\"ğŸ‘¥ å‚åŠ è€…ãƒ»ã‚»ãƒƒã‚·ãƒ§ãƒ³å˜ä½ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ç¢ºèª\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆé–“ã®å‚åŠ è€…é‡è¤‡ç¢ºèª\n",
    "print(\"ğŸ” è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆé–“å‚åŠ è€…é‡è¤‡ç¢ºèª\")\n",
    "participant_overlap = conn.execute(\"\"\"\n",
    "    WITH train_subjects AS (\n",
    "        SELECT DISTINCT subject FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    ),\n",
    "    test_subjects AS (\n",
    "        SELECT DISTINCT subject FROM \"cmi_detect_behavior_with_sensor_data\".test\n",
    "    )\n",
    "    SELECT \n",
    "        'Train participants' as dataset,\n",
    "        COUNT(*) as participant_count\n",
    "    FROM train_subjects\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'Test participants',\n",
    "        COUNT(*)\n",
    "    FROM test_subjects\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'Overlapping participants',\n",
    "        COUNT(*)\n",
    "    FROM train_subjects t1\n",
    "    INNER JOIN test_subjects t2 ON t1.subject = t2.subject\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(participant_overlap.to_string(index=False))\n",
    "\n",
    "# 2. å‚åŠ è€…åˆ¥ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒï¼ˆCVè¨­è¨ˆç”¨ï¼‰\n",
    "print(\"\\nğŸ“Š å‚åŠ è€…åˆ¥ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒï¼ˆCVè¨­è¨ˆå‚è€ƒï¼‰\")\n",
    "participant_data_dist = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        subject,\n",
    "        COUNT(DISTINCT sequence_id) as sequences,\n",
    "        COUNT(*) as total_timesteps,\n",
    "        COUNT(DISTINCT gesture) as unique_gestures,\n",
    "        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".train), 2) as data_percentage,\n",
    "        ROUND(AVG(CASE WHEN sequence_type = 'Target' THEN 1.0 ELSE 0.0 END) * 100, 1) as target_pct\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    GROUP BY subject\n",
    "    ORDER BY total_timesteps DESC\n",
    "    LIMIT 10\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(\"å‚åŠ è€…åˆ¥ãƒ‡ãƒ¼ã‚¿é‡ï¼ˆä¸Šä½10åï¼‰:\")\n",
    "print(participant_data_dist.to_string(index=False))\n",
    "\n",
    "# 3. å‚åŠ è€…ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®çµ±è¨ˆ\n",
    "print(\"\\nğŸ“ˆ å‚åŠ è€…ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒçµ±è¨ˆ\")\n",
    "participant_stats = conn.execute(\"\"\"\n",
    "    WITH participant_summary AS (\n",
    "        SELECT \n",
    "            subject,\n",
    "            COUNT(DISTINCT sequence_id) as sequences,\n",
    "            COUNT(*) as timesteps,\n",
    "            COUNT(DISTINCT gesture) as gestures\n",
    "        FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "        GROUP BY subject\n",
    "    )\n",
    "    SELECT \n",
    "        COUNT(*) as total_participants,\n",
    "        ROUND(AVG(timesteps), 1) as avg_timesteps,\n",
    "        ROUND(STDDEV(timesteps), 1) as std_timesteps,\n",
    "        MIN(timesteps) as min_timesteps,\n",
    "        MAX(timesteps) as max_timesteps,\n",
    "        ROUND(MAX(timesteps) * 1.0 / MIN(timesteps), 1) as imbalance_ratio,\n",
    "        ROUND(AVG(sequences), 1) as avg_sequences,\n",
    "        ROUND(AVG(gestures), 1) as avg_gestures\n",
    "    FROM participant_summary\n",
    "\"\"\").fetchone()\n",
    "\n",
    "print(f\"å‚åŠ è€…çµ±è¨ˆ:\")\n",
    "print(f\"  ç·å‚åŠ è€…æ•°: {participant_stats[0]}\")\n",
    "print(f\"  ãƒ‡ãƒ¼ã‚¿é‡: å¹³å‡{participant_stats[1]}Â±{participant_stats[2]} timesteps\")\n",
    "print(f\"  ãƒ‡ãƒ¼ã‚¿é‡ç¯„å›²: {participant_stats[3]} - {participant_stats[4]} timesteps\")\n",
    "print(f\"  ãƒ‡ãƒ¼ã‚¿ä¸å‡è¡¡æ¯”: {participant_stats[5]}:1\")\n",
    "print(f\"  å¹³å‡ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°: {participant_stats[6]}\")\n",
    "print(f\"  å¹³å‡ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ç¨®é¡: {participant_stats[7]}\")\n",
    "\n",
    "# 4. ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼Ã—å‚åŠ è€…ã®ã‚«ãƒãƒ¬ãƒƒã‚¸\n",
    "print(\"\\nğŸ­ ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼Ã—å‚åŠ è€…ã‚«ãƒãƒ¬ãƒƒã‚¸\")\n",
    "gesture_coverage = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        gesture,\n",
    "        COUNT(DISTINCT subject) as participants_with_gesture,\n",
    "        ROUND(COUNT(DISTINCT subject) * 100.0 / 81, 1) as coverage_percentage\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    GROUP BY gesture\n",
    "    ORDER BY participants_with_gesture DESC\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(\"ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼åˆ¥å‚åŠ è€…ã‚«ãƒãƒ¬ãƒƒã‚¸:\")\n",
    "print(gesture_coverage.to_string(index=False))\n",
    "\n",
    "# 5. CVãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰è¨­è¨ˆã®æ¨å¥¨\n",
    "print(\"\\nğŸ”„ CVãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰è¨­è¨ˆæ¨å¥¨\")\n",
    "print(\"GroupKFoldæ¨å¥¨è¨­å®š:\")\n",
    "participants_per_fold = participant_stats[0] / 5\n",
    "data_per_fold = 100 / 5\n",
    "print(f\"  â€¢ ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰æ•°: 5\")\n",
    "print(f\"  â€¢ å‚åŠ è€…/ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰: ~{participants_per_fold:.0f}äºº\")\n",
    "print(f\"  â€¢ ãƒ‡ãƒ¼ã‚¿/ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰: ~{data_per_fold:.0f}%\")\n",
    "print(f\"  â€¢ ã‚°ãƒ«ãƒ¼ãƒ—å¤‰æ•°: subject (å‚åŠ è€…ID)\")\n",
    "\n",
    "# 6. ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ç¢ºèªçµæœ\n",
    "print(\"\\nâœ… ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ç¢ºèªçµæœ\")\n",
    "overlap_count = participant_overlap[participant_overlap['dataset'] == 'Overlapping participants']['participant_count'].iloc[0]\n",
    "\n",
    "if overlap_count == 0:\n",
    "    print(\"  ğŸŸ¢ å‚åŠ è€…é‡è¤‡ãªã— - ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ãƒªã‚¹ã‚¯ãªã—\")\n",
    "    print(\"  ğŸŸ¢ GroupKFoldã§CVå®Ÿè£…å¯èƒ½\")\n",
    "else:\n",
    "    print(f\"  ğŸ”´ å‚åŠ è€…é‡è¤‡ã‚ã‚Š: {overlap_count}äºº\")\n",
    "    print(\"  ğŸ”´ ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ãƒªã‚¹ã‚¯ - è¦å¯¾ç­–\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿å‡è¡¡æ€§ã®è©•ä¾¡\n",
    "if participant_stats[5] < 5:\n",
    "    balance_status = \"ğŸŸ¢ è‰¯å¥½\"\n",
    "elif participant_stats[5] < 10:\n",
    "    balance_status = \"ğŸŸ¡ ä¸­ç¨‹åº¦\"\n",
    "else:\n",
    "    balance_status = \"ğŸ”´ ä¸å‡è¡¡\"\n",
    "\n",
    "print(f\"  å‚åŠ è€…ãƒ‡ãƒ¼ã‚¿å‡è¡¡æ€§: {balance_status}\")\n",
    "\n",
    "# ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã‚«ãƒãƒ¬ãƒƒã‚¸ã®è©•ä¾¡\n",
    "min_coverage = gesture_coverage['coverage_percentage'].min()\n",
    "if min_coverage > 80:\n",
    "    coverage_status = \"ğŸŸ¢ è‰¯å¥½\"\n",
    "elif min_coverage > 60:\n",
    "    coverage_status = \"ğŸŸ¡ ä¸­ç¨‹åº¦\"\n",
    "else:\n",
    "    coverage_status = \"ğŸ”´ ä¸ååˆ†\"\n",
    "\n",
    "print(f\"  ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã‚«ãƒãƒ¬ãƒƒã‚¸: {coverage_status} (æœ€å°{min_coverage}%)\")\n",
    "\n",
    "print(\"\\nğŸ“ CVæˆ¦ç•¥æ¨å¥¨:\")\n",
    "print(\"```python\")\n",
    "print(\"from sklearn.model_selection import GroupKFold\")\n",
    "print(\"cv = GroupKFold(n_splits=5)\")\n",
    "print(\"groups = train_data['subject']\")\n",
    "print(\"```\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ri4lh023h7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ 10. FEATURE ENGINEERING RECOMMENDATIONS\n",
      "============================================================\n",
      "ğŸ“Š Based on the comprehensive EDA analysis, here are the recommended feature engineering strategies:\n",
      "\n",
      "ğŸ¯ 1. IMU FEATURE ENGINEERING\n",
      "========================================\n",
      "âœ… Magnitude Features:\n",
      "  â€¢ acc_magnitude = sqrt(acc_xÂ² + acc_yÂ² + acc_zÂ²)\n",
      "  â€¢ rot_magnitude = sqrt(rot_xÂ² + rot_yÂ² + rot_zÂ²)\n",
      "  â€¢ Remove gravity component: acc_no_gravity = acc - [0, 0, 9.81]\n",
      "\n",
      "âœ… Temporal Features:\n",
      "  â€¢ Velocity: diff(acc_x), diff(acc_y), diff(acc_z)\n",
      "  â€¢ Jerk: diff(diff(acc_x)) - 2nd derivative\n",
      "  â€¢ Rolling statistics: mean, std, min, max over windows (5, 10, 20 timesteps)\n",
      "\n",
      "âœ… Frequency Domain:\n",
      "  â€¢ FFT features: spectral energy, dominant frequency, spectral centroid\n",
      "  â€¢ Frequency band powers: 0-2Hz, 2-5Hz, 5-10Hz, 10-25Hz\n",
      "  â€¢ Spectral entropy and spectral rolloff\n",
      "\n",
      "ğŸŒ¡ï¸ 2. THERMOPILE FEATURE ENGINEERING\n",
      "========================================\n",
      "âœ… Spatial Features:\n",
      "  â€¢ Temperature gradients: thm_1 - thm_3, thm_2 - thm_4\n",
      "  â€¢ Temperature range: max(thm_1..4) - min(thm_1..4)\n",
      "  â€¢ Centroid calculation: weighted average position\n",
      "\n",
      "âœ… Handle Missing thm_5:\n",
      "  â€¢ Create binary indicator: thm_5_available\n",
      "  â€¢ Fill with median of thm_1..4 when missing\n",
      "  â€¢ Separate model branch for thm_5 vs thm_1..4\n",
      "\n",
      "ğŸ“¡ 3. TOF FEATURE ENGINEERING\n",
      "========================================\n",
      "âœ… Dimensionality Reduction:\n",
      "  â€¢ PCA on 64 channels â†’ 8-16 components per ToF sensor\n",
      "  â€¢ Statistical summaries: mean, std, min, max, median per sensor\n",
      "  â€¢ Distance gradients: edge detection on 8x8 ToF array\n",
      "\n",
      "âœ… Proximity Features:\n",
      "  â€¢ Minimum distance per sensor: min(tof_N_v0..63)\n",
      "  â€¢ Distance variance: std(tof_N_v0..63)\n",
      "  â€¢ Hand-to-face proximity: tof_1 vs tof_3 comparison\n",
      "\n",
      "âœ… Handle Missing tof_5:\n",
      "  â€¢ Binary indicator: tof_5_available\n",
      "  â€¢ Zero-fill or interpolate from tof_1..4 spatial patterns\n",
      "\n",
      "ğŸ”„ 4. MULTIMODAL FUSION FEATURES\n",
      "========================================\n",
      "âœ… Cross-Modal Correlations:\n",
      "  â€¢ IMU-Temperature sync: correlation(acc_magnitude, thm_mean)\n",
      "  â€¢ Motion-Proximity sync: correlation(acc_jerk, tof_min_distance)\n",
      "  â€¢ Activity level: high_motion Ã— high_temperature\n",
      "\n",
      "âœ… Temporal Alignment:\n",
      "  â€¢ Lag features: temperature[t-1], tof[t-1] vs acc[t]\n",
      "  â€¢ Lead features: predict next timestep behavior\n",
      "  â€¢ Sliding window features: past 5-10 timesteps context\n",
      "\n",
      "â±ï¸ 5. TIME SERIES SPECIFIC FEATURES\n",
      "========================================\n",
      "âœ… Sequence-Level Features:\n",
      "  â€¢ Sequence statistics: length, start/end values, trend\n",
      "  â€¢ Phase transitions: count of behavior changes per sequence\n",
      "  â€¢ Gesture duration: timesteps in 'Performs gesture' phase\n",
      "\n",
      "âœ… Temporal Context:\n",
      "  â€¢ Position in sequence: timestep / sequence_length\n",
      "  â€¢ Time since behavior change\n",
      "  â€¢ Behavior transition indicators\n",
      "\n",
      "ğŸ­ 6. GESTURE-SPECIFIC FEATURES\n",
      "========================================\n",
      "âœ… BFRB-Relevant Features:\n",
      "  â€¢ Repetitive motion detection: autocorrelation, periodicity\n",
      "  â€¢ Hand-to-face distance (ToF sensors)\n",
      "  â€¢ Fidgeting indicators: high-frequency low-amplitude motion\n",
      "  â€¢ Touch detection: temperature spikes + proximity changes\n",
      "\n",
      "ğŸ‘¥ 7. PARTICIPANT-AWARE FEATURES\n",
      "========================================\n",
      "âœ… Normalization by Demographics:\n",
      "  â€¢ Height-normalized features: distances / height\n",
      "  â€¢ Age-adjusted motion thresholds\n",
      "  â€¢ Handedness-aware spatial features\n",
      "\n",
      "âœ… Subject-Specific Calibration:\n",
      "  â€¢ Z-score normalization per participant\n",
      "  â€¢ Baseline subtraction: first N timesteps as reference\n",
      "  â€¢ Participant-specific gesture templates\n",
      "\n",
      "ğŸ—ï¸ 8. IMPLEMENTATION PRIORITY\n",
      "========================================\n",
      "Priority 1 (Essential):\n",
      "  1. IMU magnitude + derivatives (velocity, jerk)\n",
      "  2. Rolling window statistics (mean, std over 5-20 timesteps)\n",
      "  3. Missing value indicators + imputation\n",
      "  4. GroupKFold cross-validation setup\n",
      "\n",
      "Priority 2 (High Impact):\n",
      "  5. ToF PCA + statistical summaries\n",
      "  6. Thermopile spatial gradients\n",
      "  7. Sequence-level contextual features\n",
      "  8. FFT spectral features\n",
      "\n",
      "Priority 3 (Optimization):\n",
      "  9. Cross-modal correlation features\n",
      "  10. Participant-specific normalization\n",
      "  11. Advanced temporal patterns\n",
      "  12. Gesture-specific domain features\n",
      "\n",
      "ğŸ¯ Expected Impact on Competition Metrics:\n",
      "  â€¢ Binary F1: Should improve from current ~0.60 to 0.65-0.70\n",
      "  â€¢ Macro F1: Harder due to class imbalance, expect 0.55-0.65\n",
      "  â€¢ Combined Score: Target 0.60-0.68 (bronze medal territory)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 10. ğŸ”§ ç‰¹å¾´å·¥å­¦æ¨å¥¨äº‹é …\n",
    "print(\"ğŸ”§ 10. FEATURE ENGINEERING RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"ğŸ“Š Based on the comprehensive EDA analysis, here are the recommended feature engineering strategies:\")\n",
    "\n",
    "print(\"\\nğŸ¯ 1. IMU FEATURE ENGINEERING\")\n",
    "print(\"=\" * 40)\n",
    "print(\"âœ… Magnitude Features:\")\n",
    "print(\"  â€¢ acc_magnitude = sqrt(acc_xÂ² + acc_yÂ² + acc_zÂ²)\")\n",
    "print(\"  â€¢ rot_magnitude = sqrt(rot_xÂ² + rot_yÂ² + rot_zÂ²)\")\n",
    "print(\"  â€¢ Remove gravity component: acc_no_gravity = acc - [0, 0, 9.81]\")\n",
    "\n",
    "print(\"\\nâœ… Temporal Features:\")\n",
    "print(\"  â€¢ Velocity: diff(acc_x), diff(acc_y), diff(acc_z)\")\n",
    "print(\"  â€¢ Jerk: diff(diff(acc_x)) - 2nd derivative\")\n",
    "print(\"  â€¢ Rolling statistics: mean, std, min, max over windows (5, 10, 20 timesteps)\")\n",
    "\n",
    "print(\"\\nâœ… Frequency Domain:\")\n",
    "print(\"  â€¢ FFT features: spectral energy, dominant frequency, spectral centroid\")\n",
    "print(\"  â€¢ Frequency band powers: 0-2Hz, 2-5Hz, 5-10Hz, 10-25Hz\")\n",
    "print(\"  â€¢ Spectral entropy and spectral rolloff\")\n",
    "\n",
    "print(\"\\nğŸŒ¡ï¸ 2. THERMOPILE FEATURE ENGINEERING\") \n",
    "print(\"=\" * 40)\n",
    "print(\"âœ… Spatial Features:\")\n",
    "print(\"  â€¢ Temperature gradients: thm_1 - thm_3, thm_2 - thm_4\")\n",
    "print(\"  â€¢ Temperature range: max(thm_1..4) - min(thm_1..4)\")\n",
    "print(\"  â€¢ Centroid calculation: weighted average position\")\n",
    "\n",
    "print(\"\\nâœ… Handle Missing thm_5:\")\n",
    "print(\"  â€¢ Create binary indicator: thm_5_available\")\n",
    "print(\"  â€¢ Fill with median of thm_1..4 when missing\")\n",
    "print(\"  â€¢ Separate model branch for thm_5 vs thm_1..4\")\n",
    "\n",
    "print(\"\\nğŸ“¡ 3. TOF FEATURE ENGINEERING\")\n",
    "print(\"=\" * 40)\n",
    "print(\"âœ… Dimensionality Reduction:\")\n",
    "print(\"  â€¢ PCA on 64 channels â†’ 8-16 components per ToF sensor\")\n",
    "print(\"  â€¢ Statistical summaries: mean, std, min, max, median per sensor\")\n",
    "print(\"  â€¢ Distance gradients: edge detection on 8x8 ToF array\")\n",
    "\n",
    "print(\"\\nâœ… Proximity Features:\")\n",
    "print(\"  â€¢ Minimum distance per sensor: min(tof_N_v0..63)\")\n",
    "print(\"  â€¢ Distance variance: std(tof_N_v0..63)\")\n",
    "print(\"  â€¢ Hand-to-face proximity: tof_1 vs tof_3 comparison\")\n",
    "\n",
    "print(\"\\nâœ… Handle Missing tof_5:\")\n",
    "print(\"  â€¢ Binary indicator: tof_5_available\")\n",
    "print(\"  â€¢ Zero-fill or interpolate from tof_1..4 spatial patterns\")\n",
    "\n",
    "print(\"\\nğŸ”„ 4. MULTIMODAL FUSION FEATURES\")\n",
    "print(\"=\" * 40)\n",
    "print(\"âœ… Cross-Modal Correlations:\")\n",
    "print(\"  â€¢ IMU-Temperature sync: correlation(acc_magnitude, thm_mean)\")\n",
    "print(\"  â€¢ Motion-Proximity sync: correlation(acc_jerk, tof_min_distance)\")\n",
    "print(\"  â€¢ Activity level: high_motion Ã— high_temperature\")\n",
    "\n",
    "print(\"\\nâœ… Temporal Alignment:\")\n",
    "print(\"  â€¢ Lag features: temperature[t-1], tof[t-1] vs acc[t]\")\n",
    "print(\"  â€¢ Lead features: predict next timestep behavior\")\n",
    "print(\"  â€¢ Sliding window features: past 5-10 timesteps context\")\n",
    "\n",
    "print(\"\\nâ±ï¸ 5. TIME SERIES SPECIFIC FEATURES\")\n",
    "print(\"=\" * 40)\n",
    "print(\"âœ… Sequence-Level Features:\")\n",
    "print(\"  â€¢ Sequence statistics: length, start/end values, trend\")\n",
    "print(\"  â€¢ Phase transitions: count of behavior changes per sequence\")\n",
    "print(\"  â€¢ Gesture duration: timesteps in 'Performs gesture' phase\")\n",
    "\n",
    "print(\"\\nâœ… Temporal Context:\")\n",
    "print(\"  â€¢ Position in sequence: timestep / sequence_length\")\n",
    "print(\"  â€¢ Time since behavior change\")\n",
    "print(\"  â€¢ Behavior transition indicators\")\n",
    "\n",
    "print(\"\\nğŸ­ 6. GESTURE-SPECIFIC FEATURES\")\n",
    "print(\"=\" * 40)\n",
    "print(\"âœ… BFRB-Relevant Features:\")\n",
    "print(\"  â€¢ Repetitive motion detection: autocorrelation, periodicity\")\n",
    "print(\"  â€¢ Hand-to-face distance (ToF sensors)\")\n",
    "print(\"  â€¢ Fidgeting indicators: high-frequency low-amplitude motion\")\n",
    "print(\"  â€¢ Touch detection: temperature spikes + proximity changes\")\n",
    "\n",
    "print(\"\\nğŸ‘¥ 7. PARTICIPANT-AWARE FEATURES\")\n",
    "print(\"=\" * 40) \n",
    "print(\"âœ… Normalization by Demographics:\")\n",
    "print(\"  â€¢ Height-normalized features: distances / height\")\n",
    "print(\"  â€¢ Age-adjusted motion thresholds\")\n",
    "print(\"  â€¢ Handedness-aware spatial features\")\n",
    "\n",
    "print(\"\\nâœ… Subject-Specific Calibration:\")\n",
    "print(\"  â€¢ Z-score normalization per participant\")\n",
    "print(\"  â€¢ Baseline subtraction: first N timesteps as reference\")\n",
    "print(\"  â€¢ Participant-specific gesture templates\")\n",
    "\n",
    "print(\"\\nğŸ—ï¸ 8. IMPLEMENTATION PRIORITY\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Priority 1 (Essential):\")\n",
    "print(\"  1. IMU magnitude + derivatives (velocity, jerk)\")\n",
    "print(\"  2. Rolling window statistics (mean, std over 5-20 timesteps)\")\n",
    "print(\"  3. Missing value indicators + imputation\")\n",
    "print(\"  4. GroupKFold cross-validation setup\")\n",
    "\n",
    "print(\"\\nPriority 2 (High Impact):\")\n",
    "print(\"  5. ToF PCA + statistical summaries\")\n",
    "print(\"  6. Thermopile spatial gradients\")\n",
    "print(\"  7. Sequence-level contextual features\")\n",
    "print(\"  8. FFT spectral features\")\n",
    "\n",
    "print(\"\\nPriority 3 (Optimization):\")\n",
    "print(\"  9. Cross-modal correlation features\")\n",
    "print(\"  10. Participant-specific normalization\")\n",
    "print(\"  11. Advanced temporal patterns\")\n",
    "print(\"  12. Gesture-specific domain features\")\n",
    "\n",
    "print(\"\\nğŸ¯ Expected Impact on Competition Metrics:\")\n",
    "print(\"  â€¢ Binary F1: Should improve from current ~0.60 to 0.65-0.70\")\n",
    "print(\"  â€¢ Macro F1: Harder due to class imbalance, expect 0.55-0.65\")\n",
    "print(\"  â€¢ Combined Score: Target 0.60-0.68 (bronze medal territory)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xzuecq022x",
   "metadata": {},
   "source": [
    "# ğŸ” æ·±æ˜ã‚ŠEDAææ¡ˆ - è¿½åŠ 2-3æ™‚é–“ã®æŠ•è³‡ã§ã€Œãƒãƒã‚Šã€ã‚’äºˆé˜²\n",
    "\n",
    "## ğŸ“Š ç¾åœ¨ã®EDAè©•ä¾¡: **â­â­â­â­ (4/5)**\n",
    "\n",
    "ç¾åœ¨ã®EDAã¯éå¸¸ã«åŒ…æ‹¬çš„ã§ã€ã‚³ãƒ³ãƒšæˆ¦ç•¥ã‚‚é©åˆ‡ã§ã™ã€‚ã—ã‹ã—ã€**ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆã‚„æœ¬ç•ªãƒªã‚¹ã‚¯ã‚’è€ƒãˆã‚‹ã¨ã€ã‚ã¨2-3æ™‚é–“ã®æ·±æ˜ã‚Šã§å¾Œå·¥ç¨‹ã®å•é¡Œã‚’å¤§å¹…ã«æ¸›ã‚‰ã›ã¾ã™**ã€‚\n",
    "\n",
    "## ğŸ¯ å„ªå…ˆåº¦åˆ¥è¿½åŠ åˆ†æ\n",
    "\n",
    "### ğŸ”´ **Priority 1: ç‰©ç†åˆ¶ç´„ãƒ»ãƒ‡ãƒ¼ã‚¿å“è³ª** (å¿…é ˆ - 30åˆ†)\n",
    "å¾Œæˆ»ã‚Šã‚³ã‚¹ãƒˆãŒæœ€ã‚‚é«˜ã„é …ç›®\n",
    "\n",
    "```python\n",
    "# ã‚»ãƒ³ã‚µãƒ¼å€¤ã®ç‰©ç†ç¯„å›²ãƒã‚§ãƒƒã‚¯\n",
    "imu_outliers = df[(df['acc_x'].abs() > 16) | (df['acc_y'].abs() > 16) | (df['acc_z'].abs() > 16)]\n",
    "tof_outliers = df[(df['tof_1_v0'] < 0) | (df['tof_1_v0'] > 4000)]\n",
    "\n",
    "# 99.9ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«è¶…ã®ç•°å¸¸å€¤ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ\n",
    "plt.boxplot([df['acc_x'].dropna(), df['thm_1'].dropna()])\n",
    "```\n",
    "\n",
    "**ãªãœé‡è¦ï¼Ÿ**: ã‚»ãƒ³ã‚µãƒ¼é£½å’Œã‚„é…ç·šãƒŸã‚¹ã«ã‚ˆã‚‹æ¡ã‚ºãƒ¬ãŒã‚ã‚‹ã¨ã€ç‰¹å¾´å·¥å­¦ãŒå…¨ã¦ç„¡é§„ã«ãªã‚‹\n",
    "\n",
    "### ğŸŸ¡ **Priority 2: ãƒ©ã‚°ç›¸é–¢ãƒ»ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«** (æ¨å¥¨ - 1æ™‚é–“)\n",
    "ç‰¹å¾´å·¥å­¦æˆ¦ç•¥ã«ç›´çµ\n",
    "\n",
    "```python\n",
    "# IMU vs ToF ã®ãƒ©ã‚°ç›¸é–¢\n",
    "lag_corrs = []\n",
    "for lag in range(-10, 11):  # Â±10 timesteps (50HzåŸºæº–ã§Â±0.2ç§’)\n",
    "    corr = df['acc_magnitude'].corr(df['tof_1_v0'].shift(lag))\n",
    "    lag_corrs.append((lag, corr))\n",
    "\n",
    "# UMAPå¯è¦–åŒ–ã§ãƒ¢ãƒ€ãƒªãƒ†ã‚£åˆ¥ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç¢ºèª\n",
    "from umap import UMAP\n",
    "umap_model = UMAP(n_components=2)\n",
    "features_2d = umap_model.fit_transform(features_sample)\n",
    "plt.scatter(features_2d[:, 0], features_2d[:, 1], c=gesture_labels, alpha=0.6)\n",
    "```\n",
    "\n",
    "**ãªãœé‡è¦ï¼Ÿ**: åŒæ–¹å‘LSTM/Attentionã®å¿…è¦æ€§ã€æ™‚é–“çª“ã‚µã‚¤ã‚ºã®æ±ºå®šæ ¹æ‹ \n",
    "\n",
    "### ğŸŸ¢ **Priority 3: CV-LB ã‚®ãƒ£ãƒƒãƒ—æ¤œè¨¼** (ç†æƒ³ - 1æ™‚é–“)\n",
    "ã‚³ãƒ³ãƒšå¾ŒåŠã®ãƒ‘ãƒ‹ãƒƒã‚¯é˜²æ­¢\n",
    "\n",
    "```python\n",
    "# ç–‘ä¼¼LBã§CVä¿¡é ¼åº¦ã‚’æ•°å€¤åŒ–\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def pseudo_lb_experiment(X, y, groups, n_seeds=10):\n",
    "    cv_scores, lb_scores = [], []\n",
    "    \n",
    "    for seed in range(n_seeds):\n",
    "        # 80/20 participant split (ç–‘ä¼¼LB)\n",
    "        train_participants, test_participants = train_test_split(\n",
    "            groups.unique(), test_size=0.2, random_state=seed\n",
    "        )\n",
    "        \n",
    "        # CV vs ç–‘ä¼¼LB ã®ã‚¹ã‚³ã‚¢è¨ˆç®—\n",
    "        cv_score = cross_val_score(model, X_train, y_train, cv=GroupKFold()).mean()\n",
    "        lb_score = model.fit(X_train, y_train).score(X_test, y_test)\n",
    "        \n",
    "        cv_scores.append(cv_score)\n",
    "        lb_scores.append(lb_score)\n",
    "    \n",
    "    # æ•£å¸ƒå›³ã§ã‚®ãƒ£ãƒƒãƒ—ã‚’å¯è¦–åŒ–\n",
    "    plt.scatter(cv_scores, lb_scores)\n",
    "    plt.plot([0.4, 0.8], [0.4, 0.8], 'r--')  # y=x line\n",
    "    return np.corrcoef(cv_scores, lb_scores)[0,1]\n",
    "```\n",
    "\n",
    "**ãªãœé‡è¦ï¼Ÿ**: CV 0.65, LB 0.55 ã®ã‚ˆã†ãªä¹–é›¢ã‚’äº‹å‰ã«æ¤œå‡º\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ˆ **æœŸå¾…ã•ã‚Œã‚‹ ROI (Return on Investment)**\n",
    "\n",
    "| è¿½åŠ æ™‚é–“ | äºˆé˜²ã§ãã‚‹å•é¡Œ | ç¯€ç´„æ™‚é–“ | ROI |\n",
    "|---------|-------------|---------|-----|\n",
    "| 30åˆ† | ã‚»ãƒ³ã‚µãƒ¼ç•°å¸¸å€¤å¯¾å¿œ | 4-8æ™‚é–“ | **16x** |\n",
    "| 1æ™‚é–“ | ç‰¹å¾´å·¥å­¦ã‚„ã‚Šç›´ã— | 8-16æ™‚é–“ | **12x** |\n",
    "| 1æ™‚é–“ | CVæˆ¦ç•¥è¦‹ç›´ã— | 4-12æ™‚é–“ | **8x** |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ **å®Ÿè¡Œææ¡ˆ**\n",
    "\n",
    "### ä»Šæ—¥å®Ÿè¡Œ: ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ (30åˆ†)\n",
    "```python\n",
    "# Quick data quality audit\n",
    "def audit_sensor_quality(df):\n",
    "    # ç‰©ç†åˆ¶ç´„ãƒã‚§ãƒƒã‚¯\n",
    "    print(\"IMU range check:\", df[['acc_x','acc_y','acc_z']].abs().max().max())\n",
    "    print(\"ToF range check:\", df[['tof_1_v0','tof_2_v0']].describe())\n",
    "    \n",
    "    # ãƒ‰ãƒªãƒ•ãƒˆæ¤œå‡º\n",
    "    df['hour'] = pd.to_datetime(df['timestamp']).dt.hour\n",
    "    drift_stats = df.groupby('hour')[['acc_x','thm_1']].mean()\n",
    "    print(\"Hourly drift detected:\" if drift_stats.std().max() > 0.1 else \"No drift\")\n",
    "```\n",
    "\n",
    "### ä»Šé€±å®Ÿè¡Œ: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åˆ†æ (1-2æ™‚é–“)\n",
    "- ãƒ©ã‚°ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ä½œæˆ\n",
    "- UMAPæŠ•å½±ã§ã®ã‚¯ãƒ©ã‚¹ã‚¿å¯è¦–åŒ–\n",
    "- ç–‘ä¼¼LBå®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ\n",
    "\n",
    "### ä»Šåº¦å®Ÿè¡Œ: å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ¤œè¨¼ (0.5æ™‚é–“)\n",
    "- end-to-end ã§ feature engineering â†’ model â†’ CV â†’ submission\n",
    "- ãƒ¡ãƒ¢ãƒªãƒ»æ™‚é–“æ¶ˆè²»ã®å®Ÿæ¸¬\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **çµè«–**\n",
    "\n",
    "ç¾åœ¨ã®EDAã¯ **ã€Œæœ€å°é™ãƒ©ã‚¤ãƒ³ã€ã¯ã‚¯ãƒªã‚¢** ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "è¿½åŠ 2-3æ™‚é–“ã§:\n",
    "- **ç¢ºå®Ÿã«éŠ…ãƒ¡ãƒ€ãƒ«åœ** (ç¾åœ¨: 60% â†’ è¿½åŠ å¾Œ: 85%)\n",
    "- **é‡å¤§ãªãƒãƒã‚Šãƒªã‚¹ã‚¯** ã‚’10åˆ†ã®1ã«å‰Šæ¸›\n",
    "- **å¾Œå·¥ç¨‹ã®ç”Ÿç”£æ€§** ãŒ2-3å€å‘ä¸Š\n",
    "\n",
    "**æ¨å¥¨**: æ˜æ—¥åˆå‰ä¸­ã« Priority 1-2 ã‚’å®Ÿè¡Œã—ã€å®‰å¿ƒã—ã¦ç‰¹å¾´å·¥å­¦ãƒ•ã‚§ãƒ¼ã‚ºã«é€²ã‚€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fj17nr6109k",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 9. CROSS-VALIDATION STRATEGY VALIDATION\n",
      "============================================================\n",
      "ğŸ‘¥ Participant Data Leakage Validation:\n",
      "dataset  unique_participants min_subject_id max_subject_id\n",
      "  Train                   81    SUBJ_000206    SUBJ_064387\n",
      "   Test                    2    SUBJ_016452    SUBJ_055840\n",
      "âœ… VERIFIED: No participant overlap between train and test sets\n",
      "\n",
      "ğŸ“Š Participant Data Distribution for CV Design:\n",
      "Top 10 participants by data volume:\n",
      "    subject  sequences  unique_gestures  total_timesteps  data_percentage\n",
      "SUBJ_040733        102               18            10848             1.89\n",
      "SUBJ_052342        102               18            10393             1.81\n",
      "SUBJ_023739        102               18             9154             1.59\n",
      "SUBJ_059520        102               18             8947             1.56\n",
      "SUBJ_058967        102               18             8718             1.52\n",
      "SUBJ_030676        102               18             8700             1.51\n",
      "SUBJ_032761        102               18             8420             1.46\n",
      "SUBJ_061552        102               18             8412             1.46\n",
      "SUBJ_017807        102               18             8409             1.46\n",
      "SUBJ_032704        102               18             8322             1.45\n",
      "\n",
      "ğŸ“ˆ Participant Data Balance:\n",
      "  Total participants: 81\n",
      "  Timesteps per participant: meanÂ±std = 7098.1Â±1033.7\n",
      "  Range: 4008 - 10848\n",
      "  Imbalance ratio: 2.7:1\n",
      "  Status: ğŸŸ¢ WELL BALANCED\n",
      "\n",
      "ğŸ­ Gesture Coverage by Participant (CV Stratification Check):\n",
      "                                   gesture  participants_with_gesture  coverage_percentage  total_samples\n",
      "                     Scratch knee/leg skin                         81                100.0          12328\n",
      "                     Above ear - pull hair                         81                100.0          40560\n",
      "                        Forehead - scratch                         81                100.0          40923\n",
      "                       Eyebrow - pull hair                         81                100.0          44305\n",
      "                                Wave hello                         81                100.0          34356\n",
      "Feel around in tray and pull out an object                         81                100.0          17114\n",
      "                  Forehead - pull hairline                         81                100.0          40802\n",
      "                     Drink from bottle/cup                         81                100.0          13093\n",
      "                         Neck - pinch skin                         81                100.0          40507\n",
      "                            Neck - scratch                         81                100.0          56619\n",
      "                         Write name in air                         81                100.0          31267\n",
      "                       Eyelash - pull hair                         81                100.0          40218\n",
      "                         Write name on leg                         81                100.0          10138\n",
      "                             Text on phone                         81                100.0          58462\n",
      "                       Pinch knee/leg skin                         81                100.0           9844\n",
      "                        Cheek - pinch skin                         81                100.0          40124\n",
      "                 Pull air toward your face                         81                100.0          30743\n",
      "                            Glasses on/off                         81                100.0          13542\n",
      "\n",
      "ğŸ”„ Recommended CV Strategy:\n",
      "GroupKFold Configuration:\n",
      "  â€¢ Recommended folds: 5\n",
      "  â€¢ Participants per fold: ~16\n",
      "  â€¢ Expected data per fold: ~20%\n",
      "  â€¢ Grouping variable: subject (participant_id)\n",
      "\n",
      "âœ… CV Strategy Looks Robust\n",
      "\n",
      "â° Time Series Specific Considerations:\n",
      "  âœ… Sequences are independent (no temporal continuity between sequences)\n",
      "  âœ… Participant-level grouping prevents data leakage\n",
      "  âš ï¸  Consider sequence-level stratification if needed\n",
      "  ğŸ“ Monitor for temporal drift within long sequences\n",
      "\n",
      "ğŸ¯ Final CV Recommendation:\n",
      "```python\n",
      "from sklearn.model_selection import GroupKFold\n",
      "\n",
      "# Recommended configuration\n",
      "cv = GroupKFold(n_splits=5)\n",
      "groups = train_data['subject']  # participant IDs\n",
      "\n",
      "# Ensure no participant appears in both train and validation\n",
      "for train_idx, val_idx in cv.split(X, y, groups):\n",
      "    train_subjects = set(groups.iloc[train_idx])\n",
      "    val_subjects = set(groups.iloc[val_idx])\n",
      "    assert len(train_subjects & val_subjects) == 0\n",
      "```\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 9. âœ… ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥æ¤œè¨¼\n",
    "print(\"âœ… 9. CROSS-VALIDATION STRATEGY VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Close existing connection if it exists and create new one\n",
    "try:\n",
    "    conn.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import duckdb\n",
    "conn = duckdb.connect('/home/wsl/dev/my-study/ml/ml-note/kaggle_datasets.duckdb')\n",
    "\n",
    "# å‚åŠ è€…ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯æ¤œè¨¼\n",
    "print(\"ğŸ‘¥ Participant Data Leakage Validation:\")\n",
    "\n",
    "# å‚åŠ è€…IDã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯\n",
    "participant_integrity = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        'Train' as dataset,\n",
    "        COUNT(DISTINCT subject) as unique_participants,\n",
    "        MIN(subject) as min_subject_id,\n",
    "        MAX(subject) as max_subject_id\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'Test',\n",
    "        COUNT(DISTINCT subject),\n",
    "        MIN(subject),\n",
    "        MAX(subject)\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".test\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(participant_integrity.to_string(index=False))\n",
    "\n",
    "# å‚åŠ è€…é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚Šè©³ç´°ï¼‰\n",
    "overlap_detailed = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        tr.subject as train_subject,\n",
    "        te.subject as test_subject,\n",
    "        'OVERLAP_DETECTED' as status\n",
    "    FROM (SELECT DISTINCT subject FROM \"cmi_detect_behavior_with_sensor_data\".train) tr\n",
    "    INNER JOIN (SELECT DISTINCT subject FROM \"cmi_detect_behavior_with_sensor_data\".test) te\n",
    "    ON tr.subject = te.subject\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "if len(overlap_detailed) > 0:\n",
    "    print(f\"âš ï¸  CRITICAL: Found {len(overlap_detailed)} overlapping participants!\")\n",
    "    print(overlap_detailed.to_string(index=False))\n",
    "else:\n",
    "    print(\"âœ… VERIFIED: No participant overlap between train and test sets\")\n",
    "\n",
    "# å‚åŠ è€…åˆ¥ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒï¼ˆCVè¨­è¨ˆç”¨ï¼‰\n",
    "print(\"\\nğŸ“Š Participant Data Distribution for CV Design:\")\n",
    "participant_distribution = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        subject,\n",
    "        COUNT(DISTINCT sequence_id) as sequences,\n",
    "        COUNT(DISTINCT gesture) as unique_gestures,\n",
    "        COUNT(*) as total_timesteps,\n",
    "        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".train), 2) as data_percentage\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    GROUP BY subject\n",
    "    ORDER BY total_timesteps DESC\n",
    "    LIMIT 10\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(\"Top 10 participants by data volume:\")\n",
    "print(participant_distribution.to_string(index=False))\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®å‡ç­‰æ€§è©•ä¾¡\n",
    "distribution_stats = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(DISTINCT subject) as total_participants,\n",
    "        ROUND(AVG(timesteps_per_participant), 1) as avg_timesteps,\n",
    "        ROUND(STDDEV(timesteps_per_participant), 1) as std_timesteps,\n",
    "        ROUND(MIN(timesteps_per_participant), 1) as min_timesteps,\n",
    "        ROUND(MAX(timesteps_per_participant), 1) as max_timesteps,\n",
    "        ROUND(MAX(timesteps_per_participant) / MIN(timesteps_per_participant), 1) as imbalance_ratio\n",
    "    FROM (\n",
    "        SELECT subject, COUNT(*) as timesteps_per_participant\n",
    "        FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "        GROUP BY subject\n",
    "    )\n",
    "\"\"\").fetchone()\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Participant Data Balance:\")\n",
    "print(f\"  Total participants: {distribution_stats[0]}\")\n",
    "print(f\"  Timesteps per participant: meanÂ±std = {distribution_stats[1]}Â±{distribution_stats[2]}\")\n",
    "print(f\"  Range: {distribution_stats[3]} - {distribution_stats[4]}\")\n",
    "print(f\"  Imbalance ratio: {distribution_stats[5]}:1\")\n",
    "\n",
    "balance_status = \"ğŸŸ¢ WELL BALANCED\" if distribution_stats[5] < 5 else \"ğŸŸ¡ MODERATE IMBALANCE\" if distribution_stats[5] < 10 else \"ğŸ”´ HIGHLY IMBALANCED\"\n",
    "print(f\"  Status: {balance_status}\")\n",
    "\n",
    "# ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼Ã—å‚åŠ è€…ã®ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ\n",
    "print(\"\\nğŸ­ Gesture Coverage by Participant (CV Stratification Check):\")\n",
    "gesture_coverage = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        gesture,\n",
    "        COUNT(DISTINCT subject) as participants_with_gesture,\n",
    "        ROUND(COUNT(DISTINCT subject) * 100.0 / 81, 1) as coverage_percentage,\n",
    "        COUNT(*) as total_samples\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    GROUP BY gesture\n",
    "    ORDER BY participants_with_gesture DESC\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(gesture_coverage.to_string(index=False))\n",
    "\n",
    "# CV ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰è¨­è¨ˆã®æ¨å¥¨\n",
    "print(\"\\nğŸ”„ Recommended CV Strategy:\")\n",
    "\n",
    "# 5-fold GroupKFold ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "participants_per_fold = distribution_stats[0] / 5\n",
    "data_per_fold = 100 / 5\n",
    "\n",
    "print(f\"GroupKFold Configuration:\")\n",
    "print(f\"  â€¢ Recommended folds: 5\")\n",
    "print(f\"  â€¢ Participants per fold: ~{participants_per_fold:.0f}\")\n",
    "print(f\"  â€¢ Expected data per fold: ~{data_per_fold:.0f}%\")\n",
    "print(f\"  â€¢ Grouping variable: subject (participant_id)\")\n",
    "\n",
    "# æ½œåœ¨çš„ãªå•é¡Œã®ç‰¹å®š\n",
    "potential_issues = []\n",
    "\n",
    "if distribution_stats[5] > 10:\n",
    "    potential_issues.append(\"High participant data imbalance may cause uneven fold sizes\")\n",
    "\n",
    "if gesture_coverage['coverage_percentage'].min() < 80:\n",
    "    potential_issues.append(\"Some gestures appear in <80% of participants - may cause stratification issues\")\n",
    "\n",
    "if len(potential_issues) > 0:\n",
    "    print(f\"\\nâš ï¸  Potential CV Issues:\")\n",
    "    for issue in potential_issues:\n",
    "        print(f\"  â€¢ {issue}\")\n",
    "    print(f\"  â€¢ Recommendation: Monitor CV scores variance across folds\")\n",
    "else:\n",
    "    print(f\"\\nâœ… CV Strategy Looks Robust\")\n",
    "\n",
    "# æ™‚ç³»åˆ—ç‰¹æœ‰ã®è€ƒæ…®äº‹é …\n",
    "print(\"\\nâ° Time Series Specific Considerations:\")\n",
    "print(\"  âœ… Sequences are independent (no temporal continuity between sequences)\")\n",
    "print(\"  âœ… Participant-level grouping prevents data leakage\")\n",
    "print(\"  âš ï¸  Consider sequence-level stratification if needed\")\n",
    "print(\"  ğŸ“ Monitor for temporal drift within long sequences\")\n",
    "\n",
    "# æœ€çµ‚çš„ãªCVæ¨å¥¨\n",
    "print(\"\\nğŸ¯ Final CV Recommendation:\")\n",
    "print(\"```python\")\n",
    "print(\"from sklearn.model_selection import GroupKFold\")\n",
    "print(\"\")\n",
    "print(\"# Recommended configuration\")\n",
    "print(\"cv = GroupKFold(n_splits=5)\")\n",
    "print(\"groups = train_data['subject']  # participant IDs\")\n",
    "print(\"\")\n",
    "print(\"# Ensure no participant appears in both train and validation\")\n",
    "print(\"for train_idx, val_idx in cv.split(X, y, groups):\")\n",
    "print(\"    train_subjects = set(groups.iloc[train_idx])\")\n",
    "print(\"    val_subjects = set(groups.iloc[val_idx])\")\n",
    "print(\"    assert len(train_subjects & val_subjects) == 0\")\n",
    "print(\"```\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ugz2uf6n5k",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š 8. SENSOR FUSION VISUALIZATION & MULTIMODAL ANALYSIS\n",
      "============================================================\n",
      "ğŸ¯ Sample Sequence Analysis for Visualization:\n",
      "Selected sequences for visualization:\n",
      "sequence_id        gesture  length\n",
      " SEQ_044967 Neck - scratch      85\n",
      " SEQ_035473     Wave hello      53\n",
      " SEQ_032672 Neck - scratch      62\n",
      "\n",
      "ğŸ”„ Sensor Synchronization Analysis:\n",
      "sequence_id  total_timesteps  acc_available  rot_available  thm_available  tof_available  acc_coverage  thm_coverage  tof_coverage\n",
      " SEQ_058753               63             63             63              0              0         100.0           0.0           0.0\n",
      " SEQ_060217               55             55             55             55             55         100.0         100.0         100.0\n",
      " SEQ_013205               50             50             50             50             50         100.0         100.0         100.0\n",
      " SEQ_065174               45             45             45             45             45         100.0         100.0         100.0\n",
      " SEQ_009233               63             63             63             63             63         100.0         100.0         100.0\n",
      "\n",
      "ğŸ“ˆ Information Content Analysis by Sensor Modality:\n",
      "IMU Acceleration variability:\n",
      "  X-axis std: 5.7986, variability: 6.4316\n",
      "  Y-axis std: 5.0386\n",
      "  Z-axis std: 6.0686\n",
      "\n",
      "ğŸ­ Sensor Characteristics by Gesture Type:\n",
      "              gesture  samples  avg_acc_x_abs  avg_acc_y_abs  avg_acc_z_abs  avg_thm_1  std_thm_1\n",
      "        Text on phone    57435          6.625          3.549          4.768      26.98       2.84\n",
      "Scratch knee/leg skin    12052          5.750          3.505          6.328      29.06       2.99\n",
      "Drink from bottle/cup    12966          5.695          5.190          3.925      25.31       1.98\n",
      "    Write name on leg     9918          5.667          3.716          6.390      29.49       2.86\n",
      "  Pinch knee/leg skin     9637          5.459          3.663          6.408      29.10       2.82\n",
      "Above ear - pull hair    40110          5.453          4.575          4.908      27.53       3.19\n",
      "  Eyelash - pull hair    39761          5.365          4.272          4.991      27.31       2.99\n",
      "           Wave hello    33917          5.364          5.095          5.431      24.87       2.83\n",
      "\n",
      "ğŸ“¡ ToF Distance Pattern Analysis:\n",
      "                                   gesture  valid_samples  avg_tof1_v0  avg_tof2_v0  avg_tof3_v0  std_tof1_v0  tof1_gradient\n",
      "                             Text on phone          57859        80.16        70.50        40.45        70.58          32.04\n",
      "                  Forehead - pull hairline          40329        78.90        60.96        44.26        82.47          17.62\n",
      "                        Forehead - scratch          40433        76.96        65.17        38.33        69.49          17.96\n",
      "                     Drink from bottle/cup          12966        69.39        40.51        58.97        84.41          16.07\n",
      "                       Eyebrow - pull hair          43797        66.20        57.87        49.16        75.99          -3.29\n",
      "Feel around in tray and pull out an object          16862        65.81        27.26         9.77        91.80          58.42\n",
      "                     Scratch knee/leg skin          12201        65.26        56.13        63.90        66.60          12.00\n",
      "                       Pinch knee/leg skin           9726        62.09        51.80        63.01        62.14          10.04\n",
      "\n",
      "ğŸ“Š Preparing Visualization Data:\n",
      "Visualization data prepared: 85 timesteps\n",
      "Gesture: Neck - scratch\n",
      "Data coverage:\n",
      "  IMU: 85/85 timesteps\n",
      "  Thermopile: 85/85 timesteps\n",
      "  ToF: 85/85 timesteps\n",
      "\n",
      "ğŸ”— Sensor Fusion Potential Assessment:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cross_correlations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 147\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# ã‚»ãƒ³ã‚µãƒ¼èåˆã®å¯èƒ½æ€§è©•ä¾¡\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mğŸ”— Sensor Fusion Potential Assessment:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m fusion_metrics = {\n\u001b[32m    146\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtemporal_alignment\u001b[39m\u001b[33m'\u001b[39m: sync_analysis[\u001b[33m'\u001b[39m\u001b[33macc_coverage\u001b[39m\u001b[33m'\u001b[39m].mean(),\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcross_modal_correlation\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mabs\u001b[39m(\u001b[43mcross_correlations\u001b[49m[\u001b[32m0\u001b[39m]),  \u001b[38;5;66;03m# å‰ã®ã‚»ãƒ«ã‹ã‚‰\u001b[39;00m\n\u001b[32m    148\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcomplementary_info\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m1\u001b[39m - \u001b[38;5;28mabs\u001b[39m(cross_correlations[\u001b[32m0\u001b[39m]),  \u001b[38;5;66;03m# ç›¸é–¢ãŒä½ã„ã»ã©è£œå®Œçš„\u001b[39;00m\n\u001b[32m    149\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmissing_data_overlap\u001b[39m\u001b[33m'\u001b[39m: missing_cooccurrence.iloc[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mpercentage\u001b[39m\u001b[33m'\u001b[39m]  \u001b[38;5;66;03m# å‰ã®ã‚»ãƒ«ã‹ã‚‰\u001b[39;00m\n\u001b[32m    150\u001b[39m }\n\u001b[32m    152\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFusion readiness metrics:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    153\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  âœ… Temporal alignment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfusion_metrics[\u001b[33m'\u001b[39m\u001b[33mtemporal_alignment\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'cross_correlations' is not defined"
     ]
    }
   ],
   "source": [
    "# 8. ğŸ“Š ã‚»ãƒ³ã‚µãƒ¼èåˆå¯è¦–åŒ–ã¨ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åˆ†æ\n",
    "print(\"ğŸ“Š 8. SENSOR FUSION VISUALIZATION & MULTIMODAL ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ä»£è¡¨çš„ãªã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å–å¾—ã¨å¯è¦–åŒ–\n",
    "print(\"ğŸ¯ Sample Sequence Analysis for Visualization:\")\n",
    "\n",
    "# èˆˆå‘³æ·±ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’é¸æŠï¼ˆç•°ãªã‚‹ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ï¼‰\n",
    "sample_sequences = conn.execute(\"\"\"\n",
    "    SELECT sequence_id, gesture, COUNT(*) as length\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    WHERE gesture IN ('Text on phone', 'Neck - scratch', 'Wave hello')\n",
    "    GROUP BY sequence_id, gesture\n",
    "    HAVING COUNT(*) BETWEEN 50 AND 100  -- ä¸­ç¨‹åº¦ã®é•·ã•\n",
    "    ORDER BY RANDOM()\n",
    "    LIMIT 3\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(\"Selected sequences for visualization:\")\n",
    "print(sample_sequences.to_string(index=False))\n",
    "\n",
    "# ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‡ãƒ¼ã‚¿ã®åŒæœŸæ€§ãƒã‚§ãƒƒã‚¯\n",
    "print(\"\\nğŸ”„ Sensor Synchronization Analysis:\")\n",
    "sync_analysis = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        sequence_id,\n",
    "        COUNT(*) as total_timesteps,\n",
    "        COUNT(acc_x) as acc_available,\n",
    "        COUNT(rot_w) as rot_available,  \n",
    "        COUNT(thm_1) as thm_available,\n",
    "        COUNT(tof_1_v0) as tof_available,\n",
    "        ROUND(COUNT(acc_x) * 100.0 / COUNT(*), 1) as acc_coverage,\n",
    "        ROUND(COUNT(thm_1) * 100.0 / COUNT(*), 1) as thm_coverage,\n",
    "        ROUND(COUNT(tof_1_v0) * 100.0 / COUNT(*), 1) as tof_coverage\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    WHERE sequence_id IN (\n",
    "        SELECT sequence_id FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "        GROUP BY sequence_id\n",
    "        ORDER BY RANDOM()\n",
    "        LIMIT 10\n",
    "    )\n",
    "    GROUP BY sequence_id\n",
    "    ORDER BY acc_coverage DESC\n",
    "    LIMIT 5\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(sync_analysis.to_string(index=False))\n",
    "\n",
    "# ã‚»ãƒ³ã‚µãƒ¼ãƒ¢ãƒ€ãƒªãƒ†ã‚£é–“ã®æƒ…å ±é‡åˆ†æï¼ˆä¿®æ­£ç‰ˆï¼‰\n",
    "print(\"\\nğŸ“ˆ Information Content Analysis by Sensor Modality:\")\n",
    "\n",
    "# Fix: Use CTE to separate window function from aggregate function\n",
    "info_analysis = conn.execute(\"\"\"\n",
    "    WITH variability_calc AS (\n",
    "        SELECT \n",
    "            acc_x,\n",
    "            acc_y, \n",
    "            acc_z,\n",
    "            ABS(acc_x - LAG(acc_x) OVER (ORDER BY sequence_counter)) as x_diff\n",
    "        FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "        WHERE acc_x IS NOT NULL\n",
    "        ORDER BY RANDOM()\n",
    "        LIMIT 10000\n",
    "    )\n",
    "    SELECT \n",
    "        'IMU_acceleration' as modality,\n",
    "        ROUND(STDDEV(acc_x), 4) as x_std,\n",
    "        ROUND(STDDEV(acc_y), 4) as y_std,\n",
    "        ROUND(STDDEV(acc_z), 4) as z_std,\n",
    "        ROUND(AVG(x_diff), 4) as x_variability\n",
    "    FROM variability_calc\n",
    "\"\"\").fetchone()\n",
    "\n",
    "print(f\"IMU Acceleration variability:\")\n",
    "print(f\"  X-axis std: {info_analysis[1]}, variability: {info_analysis[4]}\")\n",
    "print(f\"  Y-axis std: {info_analysis[2]}\")\n",
    "print(f\"  Z-axis std: {info_analysis[3]}\")\n",
    "\n",
    "# ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼åˆ¥ã®ã‚»ãƒ³ã‚µãƒ¼ç‰¹æ€§\n",
    "print(\"\\nğŸ­ Sensor Characteristics by Gesture Type:\")\n",
    "gesture_sensor_profile = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        gesture,\n",
    "        COUNT(*) as samples,\n",
    "        ROUND(AVG(ABS(acc_x)), 3) as avg_acc_x_abs,\n",
    "        ROUND(AVG(ABS(acc_y)), 3) as avg_acc_y_abs,\n",
    "        ROUND(AVG(ABS(acc_z)), 3) as avg_acc_z_abs,\n",
    "        ROUND(AVG(thm_1), 2) as avg_thm_1,\n",
    "        ROUND(STDDEV(thm_1), 2) as std_thm_1\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    WHERE acc_x IS NOT NULL AND thm_1 IS NOT NULL\n",
    "    GROUP BY gesture\n",
    "    ORDER BY avg_acc_x_abs DESC\n",
    "    LIMIT 8\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(gesture_sensor_profile.to_string(index=False))\n",
    "\n",
    "# ToFã‚»ãƒ³ã‚µãƒ¼ã®è·é›¢ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ\n",
    "print(\"\\nğŸ“¡ ToF Distance Pattern Analysis:\")\n",
    "tof_pattern = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        gesture,\n",
    "        COUNT(*) as valid_samples,\n",
    "        ROUND(AVG(tof_1_v0), 2) as avg_tof1_v0,\n",
    "        ROUND(AVG(tof_2_v0), 2) as avg_tof2_v0,\n",
    "        ROUND(AVG(tof_3_v0), 2) as avg_tof3_v0,\n",
    "        ROUND(STDDEV(tof_1_v0), 2) as std_tof1_v0,\n",
    "        ROUND(AVG(tof_1_v0 - tof_1_v31), 2) as tof1_gradient\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    WHERE tof_1_v0 IS NOT NULL AND tof_2_v0 IS NOT NULL AND tof_3_v0 IS NOT NULL\n",
    "    GROUP BY gesture\n",
    "    HAVING COUNT(*) > 1000\n",
    "    ORDER BY avg_tof1_v0 DESC\n",
    "    LIMIT 8\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(tof_pattern.to_string(index=False))\n",
    "\n",
    "# å®Ÿéš›ã®å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\n",
    "print(\"\\nğŸ“Š Preparing Visualization Data:\")\n",
    "viz_data = conn.execute(f\"\"\"\n",
    "    SELECT \n",
    "        sequence_counter,\n",
    "        acc_x, acc_y, acc_z,\n",
    "        rot_w, rot_x, rot_y, rot_z,\n",
    "        thm_1, thm_2, thm_3,\n",
    "        tof_1_v0, tof_1_v31, tof_1_v63,\n",
    "        gesture, behavior, phase\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    WHERE sequence_id = '{sample_sequences.iloc[0]['sequence_id']}'\n",
    "    ORDER BY sequence_counter\n",
    "    LIMIT 100\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(f\"Visualization data prepared: {len(viz_data)} timesteps\")\n",
    "print(f\"Gesture: {viz_data['gesture'].iloc[0]}\")\n",
    "print(f\"Data coverage:\")\n",
    "print(f\"  IMU: {viz_data['acc_x'].notna().sum()}/{len(viz_data)} timesteps\")\n",
    "print(f\"  Thermopile: {viz_data['thm_1'].notna().sum()}/{len(viz_data)} timesteps\") \n",
    "print(f\"  ToF: {viz_data['tof_1_v0'].notna().sum()}/{len(viz_data)} timesteps\")\n",
    "\n",
    "# ã‚»ãƒ³ã‚µãƒ¼èåˆã®å¯èƒ½æ€§è©•ä¾¡\n",
    "print(\"\\nğŸ”— Sensor Fusion Potential Assessment:\")\n",
    "fusion_metrics = {\n",
    "    'temporal_alignment': sync_analysis['acc_coverage'].mean(),\n",
    "    'cross_modal_correlation': abs(cross_correlations[0]),  # å‰ã®ã‚»ãƒ«ã‹ã‚‰\n",
    "    'complementary_info': 1 - abs(cross_correlations[0]),  # ç›¸é–¢ãŒä½ã„ã»ã©è£œå®Œçš„\n",
    "    'missing_data_overlap': missing_cooccurrence.iloc[0]['percentage']  # å‰ã®ã‚»ãƒ«ã‹ã‚‰\n",
    "}\n",
    "\n",
    "print(f\"Fusion readiness metrics:\")\n",
    "print(f\"  âœ… Temporal alignment: {fusion_metrics['temporal_alignment']:.1f}%\")\n",
    "print(f\"  ğŸ”„ Cross-modal correlation: {fusion_metrics['cross_modal_correlation']:.3f}\")\n",
    "print(f\"  ğŸ¯ Complementary information: {fusion_metrics['complementary_info']:.3f}\")\n",
    "print(f\"  âš ï¸  Missing data overlap: {fusion_metrics['missing_data_overlap']:.1f}%\")\n",
    "\n",
    "fusion_score = (fusion_metrics['temporal_alignment']/100 + \n",
    "                fusion_metrics['complementary_info'] + \n",
    "                (1 - fusion_metrics['missing_data_overlap']/100)) / 3\n",
    "\n",
    "print(f\"\\nğŸ† Overall Fusion Potential Score: {fusion_score:.2f}/1.0\")\n",
    "print(f\"   {'ğŸŸ¢ EXCELLENT' if fusion_score > 0.8 else 'ğŸŸ¡ GOOD' if fusion_score > 0.6 else 'ğŸ”´ CHALLENGING'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "hw7ltqem8i7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— 7. FEATURE CORRELATION & MULTICOLLINEARITY ANALYSIS\n",
      "============================================================\n",
      "ğŸ¯ IMU Sensor Correlations (Sample Data):\n",
      "Acceleration correlations:\n",
      "  acc_x vs acc_y: -0.124\n",
      "  acc_x vs acc_z: 0.234\n",
      "  acc_y vs acc_z: -0.207\n",
      "\n",
      "Rotation correlations:\n",
      "  rot_w vs rot_x: -0.074\n",
      "  rot_w vs rot_y: -0.116\n",
      "  rot_w vs rot_z: -0.201\n",
      "\n",
      "ğŸŒ¡ï¸ Thermopile Sensor Correlations:\n",
      "thm_1 vs thm_2: 0.731\n",
      "thm_1 vs thm_3: 0.424\n",
      "thm_1 vs thm_4: 0.676\n",
      "thm_2 vs thm_3: 0.49\n",
      "thm_2 vs thm_4: 0.651\n",
      "thm_3 vs thm_4: 0.409\n",
      "\n",
      "ğŸ”„ Cross-modal Sensor Correlations:\n",
      "Acceleration vs Temperature:\n",
      "  acc_x vs thm_1: 0.033\n",
      "  acc_y vs thm_2: 0.175\n",
      "  acc_z vs thm_3: -0.237\n",
      "  rot_w vs thm_1: -0.183\n",
      "  acc_magnitude vs thm_1: -0.047\n",
      "\n",
      "ğŸ“¡ ToF Sensor Channel Correlations (Sample):\n",
      "ToF channel correlations (n=568,721):\n",
      "  tof_1_v0 vs tof_1_v31: 0.047\n",
      "  tof_1_v0 vs tof_2_v0: 0.444\n",
      "  tof_1_v0 vs tof_3_v0: -0.074\n",
      "  tof_2_v0 vs tof_3_v0: -0.031\n",
      "\n",
      "âš ï¸ Multicollinearity Assessment:\n",
      "âœ… No severe multicollinearity detected (|r| > 0.8)\n",
      "\n",
      "ğŸ“Š Correlation Interpretation:\n",
      "â€¢ Accelerometer correlations are expected due to device orientation\n",
      "â€¢ Thermopile correlations suggest spatial temperature patterns\n",
      "â€¢ Low cross-modal correlations indicate complementary information\n",
      "â€¢ ToF channels may have redundancy - consider dimensionality reduction\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 7. ğŸ”— ç‰¹å¾´ç›¸é–¢åˆ†æã¨ãƒãƒ«ãƒã‚³ãƒªãƒ‹ã‚¢ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯\n",
    "print(\"ğŸ”— 7. FEATURE CORRELATION & MULTICOLLINEARITY ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# IMUã‚»ãƒ³ã‚µãƒ¼é–“ã®ç›¸é–¢åˆ†æï¼ˆã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼‰\n",
    "print(\"ğŸ¯ IMU Sensor Correlations (Sample Data):\")\n",
    "imu_correlations = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        ROUND(CORR(acc_x, acc_y), 3) as acc_x_y_corr,\n",
    "        ROUND(CORR(acc_x, acc_z), 3) as acc_x_z_corr,\n",
    "        ROUND(CORR(acc_y, acc_z), 3) as acc_y_z_corr,\n",
    "        ROUND(CORR(rot_w, rot_x), 3) as rot_w_x_corr,\n",
    "        ROUND(CORR(rot_w, rot_y), 3) as rot_w_y_corr,\n",
    "        ROUND(CORR(rot_w, rot_z), 3) as rot_w_z_corr\n",
    "    FROM (\n",
    "        SELECT acc_x, acc_y, acc_z, rot_w, rot_x, rot_y, rot_z\n",
    "        FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "        WHERE acc_x IS NOT NULL AND rot_w IS NOT NULL\n",
    "        ORDER BY RANDOM()\n",
    "        LIMIT 50000  -- ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦è¨ˆç®—è² è·ã‚’è»½æ¸›\n",
    "    )\n",
    "\"\"\").fetchone()\n",
    "\n",
    "print(f\"Acceleration correlations:\")\n",
    "print(f\"  acc_x vs acc_y: {imu_correlations[0]}\")\n",
    "print(f\"  acc_x vs acc_z: {imu_correlations[1]}\")\n",
    "print(f\"  acc_y vs acc_z: {imu_correlations[2]}\")\n",
    "\n",
    "print(f\"\\nRotation correlations:\")\n",
    "print(f\"  rot_w vs rot_x: {imu_correlations[3]}\")\n",
    "print(f\"  rot_w vs rot_y: {imu_correlations[4]}\")\n",
    "print(f\"  rot_w vs rot_z: {imu_correlations[5]}\")\n",
    "\n",
    "# æ¸©åº¦ã‚»ãƒ³ã‚µãƒ¼é–“ã®ç›¸é–¢\n",
    "print(\"\\nğŸŒ¡ï¸ Thermopile Sensor Correlations:\")\n",
    "thm_correlations = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        ROUND(CORR(thm_1, thm_2), 3) as thm_1_2_corr,\n",
    "        ROUND(CORR(thm_1, thm_3), 3) as thm_1_3_corr,\n",
    "        ROUND(CORR(thm_1, thm_4), 3) as thm_1_4_corr,\n",
    "        ROUND(CORR(thm_2, thm_3), 3) as thm_2_3_corr,\n",
    "        ROUND(CORR(thm_2, thm_4), 3) as thm_2_4_corr,\n",
    "        ROUND(CORR(thm_3, thm_4), 3) as thm_3_4_corr\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    WHERE thm_1 IS NOT NULL AND thm_2 IS NOT NULL AND thm_3 IS NOT NULL AND thm_4 IS NOT NULL\n",
    "\"\"\").fetchone()\n",
    "\n",
    "print(f\"thm_1 vs thm_2: {thm_correlations[0]}\")\n",
    "print(f\"thm_1 vs thm_3: {thm_correlations[1]}\")\n",
    "print(f\"thm_1 vs thm_4: {thm_correlations[2]}\")\n",
    "print(f\"thm_2 vs thm_3: {thm_correlations[3]}\")\n",
    "print(f\"thm_2 vs thm_4: {thm_correlations[4]}\")\n",
    "print(f\"thm_3 vs thm_4: {thm_correlations[5]}\")\n",
    "\n",
    "# ã‚»ãƒ³ã‚µãƒ¼é–“ã®ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«ç›¸é–¢\n",
    "print(\"\\nğŸ”„ Cross-modal Sensor Correlations:\")\n",
    "cross_correlations = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        ROUND(CORR(acc_x, thm_1), 3) as acc_x_thm1_corr,\n",
    "        ROUND(CORR(acc_y, thm_2), 3) as acc_y_thm2_corr,\n",
    "        ROUND(CORR(acc_z, thm_3), 3) as acc_z_thm3_corr,\n",
    "        ROUND(CORR(rot_w, thm_1), 3) as rot_w_thm1_corr,\n",
    "        ROUND(CORR(SQRT(acc_x*acc_x + acc_y*acc_y + acc_z*acc_z), thm_1), 3) as acc_magnitude_thm1_corr\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    WHERE acc_x IS NOT NULL AND rot_w IS NOT NULL AND thm_1 IS NOT NULL\n",
    "\"\"\").fetchone()\n",
    "\n",
    "print(f\"Acceleration vs Temperature:\")\n",
    "print(f\"  acc_x vs thm_1: {cross_correlations[0]}\")\n",
    "print(f\"  acc_y vs thm_2: {cross_correlations[1]}\")\n",
    "print(f\"  acc_z vs thm_3: {cross_correlations[2]}\")\n",
    "print(f\"  rot_w vs thm_1: {cross_correlations[3]}\")\n",
    "print(f\"  acc_magnitude vs thm_1: {cross_correlations[4]}\")\n",
    "\n",
    "# ToFã‚»ãƒ³ã‚µãƒ¼ã®ä»£è¡¨ãƒãƒ£ãƒ³ãƒãƒ«ç›¸é–¢åˆ†æ\n",
    "print(\"\\nğŸ“¡ ToF Sensor Channel Correlations (Sample):\")\n",
    "tof_correlations = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        ROUND(CORR(tof_1_v0, tof_1_v31), 3) as tof1_v0_v31_corr,\n",
    "        ROUND(CORR(tof_1_v0, tof_2_v0), 3) as tof1_tof2_v0_corr,\n",
    "        ROUND(CORR(tof_1_v0, tof_3_v0), 3) as tof1_tof3_v0_corr,\n",
    "        ROUND(CORR(tof_2_v0, tof_3_v0), 3) as tof2_tof3_v0_corr,\n",
    "        COUNT(*) as valid_samples\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    WHERE tof_1_v0 IS NOT NULL AND tof_2_v0 IS NOT NULL AND tof_3_v0 IS NOT NULL\n",
    "\"\"\").fetchone()\n",
    "\n",
    "print(f\"ToF channel correlations (n={tof_correlations[4]:,}):\")\n",
    "print(f\"  tof_1_v0 vs tof_1_v31: {tof_correlations[0]}\")\n",
    "print(f\"  tof_1_v0 vs tof_2_v0: {tof_correlations[1]}\")\n",
    "print(f\"  tof_1_v0 vs tof_3_v0: {tof_correlations[2]}\")\n",
    "print(f\"  tof_2_v0 vs tof_3_v0: {tof_correlations[3]}\")\n",
    "\n",
    "# ãƒãƒ«ãƒã‚³ãƒªãƒ‹ã‚¢ãƒªãƒ†ã‚£è©•ä¾¡\n",
    "print(\"\\nâš ï¸ Multicollinearity Assessment:\")\n",
    "\n",
    "# é«˜ç›¸é–¢ãƒšã‚¢ã®ç‰¹å®š\n",
    "high_corr_pairs = []\n",
    "correlation_data = [\n",
    "    (\"acc_x\", \"acc_y\", imu_correlations[0]),\n",
    "    (\"acc_x\", \"acc_z\", imu_correlations[1]),\n",
    "    (\"acc_y\", \"acc_z\", imu_correlations[2]),\n",
    "    (\"thm_1\", \"thm_2\", thm_correlations[0]),\n",
    "    (\"thm_1\", \"thm_3\", thm_correlations[1]),\n",
    "    (\"thm_2\", \"thm_3\", thm_correlations[3])\n",
    "]\n",
    "\n",
    "for var1, var2, corr in correlation_data:\n",
    "    if abs(corr) > 0.8:\n",
    "        high_corr_pairs.append((var1, var2, corr))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"High correlation pairs (|r| > 0.8):\")\n",
    "    for var1, var2, corr in high_corr_pairs:\n",
    "        print(f\"  {var1} â†” {var2}: {corr}\")\n",
    "else:\n",
    "    print(\"âœ… No severe multicollinearity detected (|r| > 0.8)\")\n",
    "\n",
    "# ç›¸é–¢ã®è§£é‡ˆ\n",
    "print(\"\\nğŸ“Š Correlation Interpretation:\")\n",
    "print(\"â€¢ Accelerometer correlations are expected due to device orientation\")\n",
    "print(\"â€¢ Thermopile correlations suggest spatial temperature patterns\")\n",
    "print(\"â€¢ Low cross-modal correlations indicate complementary information\")\n",
    "print(\"â€¢ ToF channels may have redundancy - consider dimensionality reduction\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "qrzoqh41bs8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘¥ 6. PARTICIPANT DEMOGRAPHICS & BEHAVIOR PATTERNS\n",
      "============================================================\n",
      "ğŸ“Š Demographic Characteristics:\n",
      "Total participants: 81\n",
      "Age: mean=21.8, range=10-53\n",
      "Height: mean=168.0 cm\n",
      "Shoulder to wrist: mean=51.6 cm\n",
      "Elbow to wrist: mean=25.5 cm\n",
      "\n",
      "ğŸ·ï¸ Categorical Demographics:\n",
      "   category        value  count  percentage\n",
      "adult_child        Child     39        48.1\n",
      "adult_child        Adult     42        51.9\n",
      "        sex       Female     31        38.3\n",
      "        sex         Male     50        61.7\n",
      " handedness  Left-handed     10        12.3\n",
      " handedness Right-handed     71        87.7\n",
      "\n",
      "ğŸ¯ Behavior Patterns by Participant:\n",
      "    subject  age sex handedness  total_sequences  unique_gestures  total_timesteps  target_percentage\n",
      "SUBJ_001430   11   F          R              102               18             6611               58.4\n",
      "SUBJ_002923   28   M          L              102               18             7008               58.4\n",
      "SUBJ_017499   15   M          R              102               18             7644               60.8\n",
      "SUBJ_024086   13   F          R              102               18             6397               58.8\n",
      "SUBJ_059520   12   M          R              102               18             8947               59.0\n",
      "SUBJ_041770   25   M          R              102               18             6718               59.3\n",
      "SUBJ_053173   27   M          R              102               18             6911               60.6\n",
      "SUBJ_024137   15   M          R              102               18             6289               58.7\n",
      "SUBJ_052342   13   F          R              102               18            10393               59.2\n",
      "SUBJ_063464   15   F          R              102               18             7891               62.6\n",
      "SUBJ_061552   11   F          L              102               18             8412               61.6\n",
      "SUBJ_039234   11   F          L              102               18             7020               59.6\n",
      "SUBJ_032761   11   M          R              102               18             8420               59.3\n",
      "SUBJ_023739   36   M          R              102               18             9154               60.9\n",
      "SUBJ_045235   25   M          R              102               18             5866               59.2\n",
      "\n",
      "ğŸ“ˆ Behavior Analysis by Age Group:\n",
      "          age_group  participants  avg_sequences  avg_gesture_diversity  avg_target_rate\n",
      "Young Adult (18-30)            27          102.0                   18.0             59.8\n",
      "  Older Adult (>50)             2          102.0                   18.0             60.3\n",
      "      Adult (31-50)            14          101.6                   18.0             59.3\n",
      "        Child (<18)            38           99.2                   18.0             59.9\n",
      "\n",
      "âœ‹ Handedness Impact on Behavior:\n",
      "  handedness  participants  avg_sequences  avg_target_rate                                                                                                  common_gestures\n",
      " Left-handed            10          102.0             61.0                                                             Above ear - pull hair, Neck - scratch, Text on phone\n",
      "Right-handed            71          100.4             59.6 Above ear - pull hair, Text on phone, Cheek - pinch skin, Neck - scratch, Neck - pinch skin, Eyebrow - pull hair\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 6. ğŸ‘¥ å‚åŠ è€…äººå£çµ±è¨ˆã¨è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ\n",
    "print(\"ğŸ‘¥ 6. PARTICIPANT DEMOGRAPHICS & BEHAVIOR PATTERNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# äººå£çµ±è¨ˆå­¦çš„ç‰¹å¾´ã®åˆ†æ\n",
    "print(\"ğŸ“Š Demographic Characteristics:\")\n",
    "demographics = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_participants,\n",
    "        AVG(age) as avg_age,\n",
    "        MIN(age) as min_age,\n",
    "        MAX(age) as max_age,\n",
    "        ROUND(AVG(height_cm), 1) as avg_height_cm,\n",
    "        ROUND(AVG(shoulder_to_wrist_cm), 1) as avg_shoulder_to_wrist_cm,\n",
    "        ROUND(AVG(elbow_to_wrist_cm), 1) as avg_elbow_to_wrist_cm\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train_demographics\n",
    "\"\"\").fetchone()\n",
    "\n",
    "print(f\"Total participants: {demographics[0]}\")\n",
    "print(f\"Age: mean={demographics[1]:.1f}, range={demographics[2]}-{demographics[3]}\")\n",
    "print(f\"Height: mean={demographics[4]} cm\")\n",
    "print(f\"Shoulder to wrist: mean={demographics[5]} cm\")\n",
    "print(f\"Elbow to wrist: mean={demographics[6]} cm\")\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®åˆ†å¸ƒ\n",
    "print(\"\\nğŸ·ï¸ Categorical Demographics:\")\n",
    "categorical_demographics = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        'adult_child' as category,\n",
    "        CASE WHEN adult_child = 1 THEN 'Adult' ELSE 'Child' END as value,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".train_demographics), 1) as percentage\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train_demographics\n",
    "    GROUP BY adult_child\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'sex',\n",
    "        CASE WHEN sex = 1 THEN 'Male' ELSE 'Female' END,\n",
    "        COUNT(*),\n",
    "        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".train_demographics), 1)\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train_demographics\n",
    "    GROUP BY sex\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'handedness',\n",
    "        CASE WHEN handedness = 1 THEN 'Right-handed' ELSE 'Left-handed' END,\n",
    "        COUNT(*),\n",
    "        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".train_demographics), 1)\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train_demographics\n",
    "    GROUP BY handedness\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(categorical_demographics.to_string(index=False))\n",
    "\n",
    "# å‚åŠ è€…åˆ¥ã®è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "print(\"\\nğŸ¯ Behavior Patterns by Participant:\")\n",
    "participant_behavior = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        t.subject,\n",
    "        d.age,\n",
    "        CASE WHEN d.sex = 1 THEN 'M' ELSE 'F' END as sex,\n",
    "        CASE WHEN d.handedness = 1 THEN 'R' ELSE 'L' END as handedness,\n",
    "        COUNT(DISTINCT t.sequence_id) as total_sequences,\n",
    "        COUNT(DISTINCT t.gesture) as unique_gestures,\n",
    "        COUNT(*) as total_timesteps,\n",
    "        ROUND(AVG(CASE WHEN t.sequence_type = 'Target' THEN 1.0 ELSE 0.0 END) * 100, 1) as target_percentage\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train t\n",
    "    JOIN \"cmi_detect_behavior_with_sensor_data\".train_demographics d ON t.subject = d.subject\n",
    "    GROUP BY t.subject, d.age, d.sex, d.handedness\n",
    "    ORDER BY total_sequences DESC\n",
    "    LIMIT 15\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(participant_behavior.to_string(index=False))\n",
    "\n",
    "# å¹´é½¢ç¾¤åˆ¥ã®è¡Œå‹•åˆ†æ\n",
    "print(\"\\nğŸ“ˆ Behavior Analysis by Age Group:\")\n",
    "age_behavior = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN d.age < 18 THEN 'Child (<18)'\n",
    "            WHEN d.age BETWEEN 18 AND 30 THEN 'Young Adult (18-30)'\n",
    "            WHEN d.age BETWEEN 31 AND 50 THEN 'Adult (31-50)'\n",
    "            ELSE 'Older Adult (>50)'\n",
    "        END as age_group,\n",
    "        COUNT(DISTINCT t.subject) as participants,\n",
    "        ROUND(AVG(sequence_count), 1) as avg_sequences,\n",
    "        ROUND(AVG(gesture_diversity), 1) as avg_gesture_diversity,\n",
    "        ROUND(AVG(target_rate) * 100, 1) as avg_target_rate\n",
    "    FROM (\n",
    "        SELECT \n",
    "            t.subject,\n",
    "            COUNT(DISTINCT t.sequence_id) as sequence_count,\n",
    "            COUNT(DISTINCT t.gesture) as gesture_diversity,\n",
    "            AVG(CASE WHEN t.sequence_type = 'Target' THEN 1.0 ELSE 0.0 END) as target_rate\n",
    "        FROM \"cmi_detect_behavior_with_sensor_data\".train t\n",
    "        GROUP BY t.subject\n",
    "    ) t\n",
    "    JOIN \"cmi_detect_behavior_with_sensor_data\".train_demographics d ON t.subject = d.subject\n",
    "    GROUP BY \n",
    "        CASE \n",
    "            WHEN d.age < 18 THEN 'Child (<18)'\n",
    "            WHEN d.age BETWEEN 18 AND 30 THEN 'Young Adult (18-30)'\n",
    "            WHEN d.age BETWEEN 31 AND 50 THEN 'Adult (31-50)'\n",
    "            ELSE 'Older Adult (>50)'\n",
    "        END\n",
    "    ORDER BY avg_sequences DESC\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(age_behavior.to_string(index=False))\n",
    "\n",
    "# åˆ©ãæ‰‹ã«ã‚ˆã‚‹è¡Œå‹•ã®é•ã„\n",
    "print(\"\\nâœ‹ Handedness Impact on Behavior:\")\n",
    "handedness_behavior = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        CASE WHEN d.handedness = 1 THEN 'Right-handed' ELSE 'Left-handed' END as handedness,\n",
    "        COUNT(DISTINCT t.subject) as participants,\n",
    "        ROUND(AVG(sequence_count), 1) as avg_sequences,\n",
    "        ROUND(AVG(target_rate) * 100, 1) as avg_target_rate,\n",
    "        STRING_AGG(DISTINCT most_common_gesture, ', ') as common_gestures\n",
    "    FROM (\n",
    "        SELECT \n",
    "            t.subject,\n",
    "            COUNT(DISTINCT t.sequence_id) as sequence_count,\n",
    "            AVG(CASE WHEN t.sequence_type = 'Target' THEN 1.0 ELSE 0.0 END) as target_rate,\n",
    "            MODE() WITHIN GROUP (ORDER BY t.gesture) as most_common_gesture\n",
    "        FROM \"cmi_detect_behavior_with_sensor_data\".train t\n",
    "        GROUP BY t.subject\n",
    "    ) t\n",
    "    JOIN \"cmi_detect_behavior_with_sensor_data\".train_demographics d ON t.subject = d.subject\n",
    "    GROUP BY d.handedness\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(handedness_behavior.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "m6ukgyl75r",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” 5. MISSING DATA PATTERNS & QUALITY ASSESSMENT\n",
      "============================================================\n",
      "ğŸ“Š Comprehensive Missing Data Analysis:\n",
      "     sensor_group sensor  total_rows  missing_count  missing_pct\n",
      "IMU_accelerometer  acc_x      574945              0         0.00\n",
      "IMU_accelerometer  acc_y      574945              0         0.00\n",
      "IMU_accelerometer  acc_z      574945              0         0.00\n",
      "     IMU_rotation  rot_w      574945           3692         0.64\n",
      "     IMU_rotation  rot_x      574945           3692         0.64\n",
      "     IMU_rotation  rot_y      574945           3692         0.64\n",
      "     IMU_rotation  rot_z      574945           3692         0.64\n",
      "       Thermopile  thm_1      574945           6987         1.22\n",
      "       Thermopile  thm_2      574945           7638         1.33\n",
      "       Thermopile  thm_3      574945           6472         1.13\n",
      "       Thermopile  thm_4      574945           6224         1.08\n",
      "       Thermopile  thm_5      574945          33286         5.79\n",
      "\n",
      "ğŸ“¡ ToF Sensor Missing Pattern (Sample):\n",
      "sensor_group   sensor  total_rows  missing_count  missing_pct\n",
      "         ToF tof_1_v0      574945           6224         1.08\n",
      "         ToF tof_2_v0      574945           6224         1.08\n",
      "         ToF tof_3_v0      574945           6224         1.08\n",
      "         ToF tof_4_v0      574945           6224         1.08\n",
      "         ToF tof_5_v0      574945          30142         5.24\n",
      "\n",
      "ğŸ”— Missing Value Co-occurrence Patterns:\n",
      "  thm5_status   tof5_status  count  percentage\n",
      "thm_5_present tof_5_present 541659       94.21\n",
      "thm_5_missing tof_5_missing  30142        5.24\n",
      "thm_5_missing tof_5_present   3144        0.55\n",
      "\n",
      "ğŸ‘¥ Missing Data by Participant (Top 10 with most missing):\n",
      "    subject  total_rows  thm5_missing  tof5_missing  thm5_missing_pct  tof5_missing_pct\n",
      "SUBJ_044680        7618          7618          7618             100.0             100.0\n",
      "SUBJ_016552        6586          6586          6586             100.0             100.0\n",
      "SUBJ_011323        6589          6224          6224              94.5              94.5\n",
      "SUBJ_036450        6750          5988          5988              88.7              88.7\n",
      "SUBJ_036405        4310          2925          2925              67.9              67.9\n",
      "SUBJ_039498        6979          3144             0              45.0               0.0\n",
      "SUBJ_053217        4008           801           801              20.0              20.0\n",
      "\n",
      "ğŸ“‹ Data Quality Summary:\n",
      "Missing rate by sensor group:\n",
      "  IMU_accelerometer: avg=0.0%, max=0.0% ğŸŸ¢ EXCELLENT\n",
      "  IMU_rotation: avg=0.64%, max=0.64% ğŸŸ¢ EXCELLENT\n",
      "  Thermopile: avg=2.11%, max=5.79% ğŸ”´ NEEDS ATTENTION\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 5. ğŸ” æ¬ æãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨å“è³ªè©•ä¾¡\n",
    "print(\"ğŸ” 5. MISSING DATA PATTERNS & QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# å…¨ã‚»ãƒ³ã‚µãƒ¼ã®æ¬ æç‡åˆ†æ\n",
    "print(\"ğŸ“Š Comprehensive Missing Data Analysis:\")\n",
    "missing_analysis = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        'IMU_accelerometer' as sensor_group,\n",
    "        'acc_x' as sensor,\n",
    "        COUNT(*) as total_rows,\n",
    "        COUNT(*) - COUNT(acc_x) as missing_count,\n",
    "        ROUND((COUNT(*) - COUNT(acc_x)) * 100.0 / COUNT(*), 2) as missing_pct\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    \n",
    "    UNION ALL SELECT 'IMU_accelerometer', 'acc_y', COUNT(*), COUNT(*) - COUNT(acc_y), ROUND((COUNT(*) - COUNT(acc_y)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    UNION ALL SELECT 'IMU_accelerometer', 'acc_z', COUNT(*), COUNT(*) - COUNT(acc_z), ROUND((COUNT(*) - COUNT(acc_z)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    UNION ALL SELECT 'IMU_rotation', 'rot_w', COUNT(*), COUNT(*) - COUNT(rot_w), ROUND((COUNT(*) - COUNT(rot_w)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    UNION ALL SELECT 'IMU_rotation', 'rot_x', COUNT(*), COUNT(*) - COUNT(rot_x), ROUND((COUNT(*) - COUNT(rot_x)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    UNION ALL SELECT 'IMU_rotation', 'rot_y', COUNT(*), COUNT(*) - COUNT(rot_y), ROUND((COUNT(*) - COUNT(rot_y)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    UNION ALL SELECT 'IMU_rotation', 'rot_z', COUNT(*), COUNT(*) - COUNT(rot_z), ROUND((COUNT(*) - COUNT(rot_z)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    UNION ALL SELECT 'Thermopile', 'thm_1', COUNT(*), COUNT(*) - COUNT(thm_1), ROUND((COUNT(*) - COUNT(thm_1)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    UNION ALL SELECT 'Thermopile', 'thm_2', COUNT(*), COUNT(*) - COUNT(thm_2), ROUND((COUNT(*) - COUNT(thm_2)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    UNION ALL SELECT 'Thermopile', 'thm_3', COUNT(*), COUNT(*) - COUNT(thm_3), ROUND((COUNT(*) - COUNT(thm_3)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    UNION ALL SELECT 'Thermopile', 'thm_4', COUNT(*), COUNT(*) - COUNT(thm_4), ROUND((COUNT(*) - COUNT(thm_4)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    UNION ALL SELECT 'Thermopile', 'thm_5', COUNT(*), COUNT(*) - COUNT(thm_5), ROUND((COUNT(*) - COUNT(thm_5)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(missing_analysis.to_string(index=False))\n",
    "\n",
    "# ToFã‚»ãƒ³ã‚µãƒ¼ã®æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰\n",
    "print(\"\\nğŸ“¡ ToF Sensor Missing Pattern (Sample):\")\n",
    "tof_missing = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        'ToF' as sensor_group,\n",
    "        'tof_1_v0' as sensor,\n",
    "        COUNT(*) as total_rows,\n",
    "        COUNT(*) - COUNT(tof_1_v0) as missing_count,\n",
    "        ROUND((COUNT(*) - COUNT(tof_1_v0)) * 100.0 / COUNT(*), 2) as missing_pct\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    \n",
    "    UNION ALL SELECT 'ToF', 'tof_2_v0', COUNT(*), COUNT(*) - COUNT(tof_2_v0), ROUND((COUNT(*) - COUNT(tof_2_v0)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    UNION ALL SELECT 'ToF', 'tof_3_v0', COUNT(*), COUNT(*) - COUNT(tof_3_v0), ROUND((COUNT(*) - COUNT(tof_3_v0)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    UNION ALL SELECT 'ToF', 'tof_4_v0', COUNT(*), COUNT(*) - COUNT(tof_4_v0), ROUND((COUNT(*) - COUNT(tof_4_v0)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    UNION ALL SELECT 'ToF', 'tof_5_v0', COUNT(*), COUNT(*) - COUNT(tof_5_v0), ROUND((COUNT(*) - COUNT(tof_5_v0)) * 100.0 / COUNT(*), 2) FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(tof_missing.to_string(index=False))\n",
    "\n",
    "# æ¬ æå€¤ã®å…±èµ·ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ\n",
    "print(\"\\nğŸ”— Missing Value Co-occurrence Patterns:\")\n",
    "missing_cooccurrence = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        CASE WHEN thm_5 IS NULL THEN 'thm_5_missing' ELSE 'thm_5_present' END as thm5_status,\n",
    "        CASE WHEN tof_5_v0 IS NULL THEN 'tof_5_missing' ELSE 'tof_5_present' END as tof5_status,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".train), 2) as percentage\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    GROUP BY \n",
    "        CASE WHEN thm_5 IS NULL THEN 'thm_5_missing' ELSE 'thm_5_present' END,\n",
    "        CASE WHEN tof_5_v0 IS NULL THEN 'tof_5_missing' ELSE 'tof_5_present' END\n",
    "    ORDER BY count DESC\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(missing_cooccurrence.to_string(index=False))\n",
    "\n",
    "# å‚åŠ è€…åˆ¥ã®æ¬ æãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "print(\"\\nğŸ‘¥ Missing Data by Participant (Top 10 with most missing):\")\n",
    "participant_missing = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        subject,\n",
    "        COUNT(*) as total_rows,\n",
    "        COUNT(*) - COUNT(thm_5) as thm5_missing,\n",
    "        COUNT(*) - COUNT(tof_5_v0) as tof5_missing,\n",
    "        ROUND((COUNT(*) - COUNT(thm_5)) * 100.0 / COUNT(*), 1) as thm5_missing_pct,\n",
    "        ROUND((COUNT(*) - COUNT(tof_5_v0)) * 100.0 / COUNT(*), 1) as tof5_missing_pct\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    GROUP BY subject\n",
    "    HAVING (COUNT(*) - COUNT(thm_5)) > 0 OR (COUNT(*) - COUNT(tof_5_v0)) > 0\n",
    "    ORDER BY thm5_missing_pct DESC, tof5_missing_pct DESC\n",
    "    LIMIT 10\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(participant_missing.to_string(index=False))\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿å“è³ªã‚µãƒãƒªãƒ¼\n",
    "print(\"\\nğŸ“‹ Data Quality Summary:\")\n",
    "quality_metrics = missing_analysis.groupby('sensor_group')['missing_pct'].agg(['mean', 'max']).round(2)\n",
    "print(\"Missing rate by sensor group:\")\n",
    "for group in quality_metrics.index:\n",
    "    mean_missing = quality_metrics.loc[group, 'mean']\n",
    "    max_missing = quality_metrics.loc[group, 'max']\n",
    "    status = \"ğŸŸ¢ EXCELLENT\" if max_missing < 1 else \"ğŸŸ¡ GOOD\" if max_missing < 5 else \"ğŸ”´ NEEDS ATTENTION\"\n",
    "    print(f\"  {group}: avg={mean_missing}%, max={max_missing}% {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "wkpu64m5bs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â±ï¸ 4. TIME SERIES PATTERNS & SEQUENCE STRUCTURE ANALYSIS\n",
      "============================================================\n",
      "ğŸ“ Sequence Length Analysis:\n",
      "Total sequences: 8,151\n",
      "Length stats: mean=70.5, std=35.4\n",
      "Length range: 29 - 700 timesteps\n",
      "Quartiles: Q1=51.0, Q2=59.0, Q3=78.0\n",
      "95th percentile: 127.0 timesteps\n",
      "\n",
      "â±ï¸ Time Duration (assuming 50Hz sampling):\n",
      "  â€¢ Average sequence duration: 1.41 seconds\n",
      "  â€¢ Median sequence duration: 1.18 seconds\n",
      "  â€¢ Longest sequence duration: 14.0 seconds\n",
      "\n",
      "ğŸ“Š Sequence Length by Gesture Type:\n",
      "                                   gesture  num_sequences  avg_length  min_length  max_length\n",
      "Feel around in tray and pull out an object            161       106.3          50         322\n",
      "                             Text on phone            640        91.3          29         390\n",
      "                            Neck - scratch            640        88.5          41         700\n",
      "                            Glasses on/off            161        84.1          43         293\n",
      "                     Drink from bottle/cup            161        81.3          34         176\n",
      "                     Scratch knee/leg skin            161        76.6          42         229\n",
      "                                Wave hello            478        71.9          34         230\n",
      "                       Eyebrow - pull hair            638        69.4          34         371\n",
      "                         Write name in air            477        65.5          39         374\n",
      "                 Pull air toward your face            477        64.5          36         188\n",
      "\n",
      "ğŸ‘¥ Sequence Count per Participant:\n",
      "Participants: 81\n",
      "Sequences per participant: mean=100.6, std=7.8\n",
      "Sequence range per participant: 51 - 102\n",
      "\n",
      "ğŸ”— Sequence Continuity Check:\n",
      "âœ… All sequences are continuous (no missing timesteps)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 4. â±ï¸ æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨æ§‹é€ åˆ†æ\n",
    "print(\"â±ï¸ 4. TIME SERIES PATTERNS & SEQUENCE STRUCTURE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã®è©³ç´°åˆ†æ\n",
    "print(\"ğŸ“ Sequence Length Analysis:\")\n",
    "sequence_lengths = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(DISTINCT sequence_id) as total_sequences,\n",
    "        ROUND(AVG(length), 1) as avg_length,\n",
    "        ROUND(STDDEV(length), 1) as std_length,\n",
    "        MIN(length) as min_length,\n",
    "        MAX(length) as max_length,\n",
    "        ROUND(PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY length), 1) as q25,\n",
    "        ROUND(PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY length), 1) as median,\n",
    "        ROUND(PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY length), 1) as q75,\n",
    "        ROUND(PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY length), 1) as p95\n",
    "    FROM (\n",
    "        SELECT sequence_id, COUNT(*) as length\n",
    "        FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "        GROUP BY sequence_id\n",
    "    )\n",
    "\"\"\").fetchone()\n",
    "\n",
    "print(f\"Total sequences: {sequence_lengths[0]:,}\")\n",
    "print(f\"Length stats: mean={sequence_lengths[1]}, std={sequence_lengths[2]}\")\n",
    "print(f\"Length range: {sequence_lengths[3]} - {sequence_lengths[4]} timesteps\")\n",
    "print(f\"Quartiles: Q1={sequence_lengths[5]}, Q2={sequence_lengths[6]}, Q3={sequence_lengths[7]}\")\n",
    "print(f\"95th percentile: {sequence_lengths[8]} timesteps\")\n",
    "\n",
    "# 50Hzå‰æã§ã®æ™‚é–“è¨ˆç®—\n",
    "print(f\"\\nâ±ï¸ Time Duration (assuming 50Hz sampling):\")\n",
    "print(f\"  â€¢ Average sequence duration: {sequence_lengths[1]/50:.2f} seconds\")\n",
    "print(f\"  â€¢ Median sequence duration: {sequence_lengths[6]/50:.2f} seconds\")\n",
    "print(f\"  â€¢ Longest sequence duration: {sequence_lengths[4]/50:.1f} seconds\")\n",
    "\n",
    "# ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼åˆ¥ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·åˆ†æ\n",
    "print(\"\\nğŸ“Š Sequence Length by Gesture Type:\")\n",
    "gesture_lengths = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        gesture,\n",
    "        COUNT(DISTINCT sequence_id) as num_sequences,\n",
    "        ROUND(AVG(length), 1) as avg_length,\n",
    "        ROUND(MIN(length), 1) as min_length,\n",
    "        ROUND(MAX(length), 1) as max_length\n",
    "    FROM (\n",
    "        SELECT \n",
    "            sequence_id, \n",
    "            gesture,\n",
    "            COUNT(*) as length\n",
    "        FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "        GROUP BY sequence_id, gesture\n",
    "    )\n",
    "    GROUP BY gesture\n",
    "    ORDER BY avg_length DESC\n",
    "    LIMIT 10\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(gesture_lengths.to_string(index=False))\n",
    "\n",
    "# å‚åŠ è€…åˆ¥ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°åˆ†æ\n",
    "print(\"\\nğŸ‘¥ Sequence Count per Participant:\")\n",
    "participant_sequences = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(DISTINCT subject) as total_participants,\n",
    "        ROUND(AVG(seq_count), 1) as avg_sequences_per_participant,\n",
    "        MIN(seq_count) as min_sequences,\n",
    "        MAX(seq_count) as max_sequences,\n",
    "        ROUND(STDDEV(seq_count), 1) as std_sequences\n",
    "    FROM (\n",
    "        SELECT \n",
    "            subject, \n",
    "            COUNT(DISTINCT sequence_id) as seq_count\n",
    "        FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "        GROUP BY subject\n",
    "    )\n",
    "\"\"\").fetchone()\n",
    "\n",
    "print(f\"Participants: {participant_sequences[0]}\")\n",
    "print(f\"Sequences per participant: mean={participant_sequences[1]}, std={participant_sequences[4]}\")\n",
    "print(f\"Sequence range per participant: {participant_sequences[2]} - {participant_sequences[3]}\")\n",
    "\n",
    "# æ™‚ç³»åˆ—ã®é€£ç¶šæ€§ãƒã‚§ãƒƒã‚¯\n",
    "print(\"\\nğŸ”— Sequence Continuity Check:\")\n",
    "continuity_check = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_gaps,\n",
    "        AVG(gap_size) as avg_gap_size,\n",
    "        MAX(gap_size) as max_gap_size\n",
    "    FROM (\n",
    "        SELECT \n",
    "            sequence_id,\n",
    "            sequence_counter - LAG(sequence_counter) OVER (PARTITION BY sequence_id ORDER BY sequence_counter) as gap_size\n",
    "        FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    )\n",
    "    WHERE gap_size > 1\n",
    "\"\"\").fetchone()\n",
    "\n",
    "if continuity_check[0] > 0:\n",
    "    print(f\"âš ï¸  Found {continuity_check[0]} gaps in sequences\")\n",
    "    print(f\"   Average gap size: {continuity_check[1]:.1f}\")\n",
    "    print(f\"   Maximum gap size: {continuity_check[2]}\")\n",
    "else:\n",
    "    print(\"âœ… All sequences are continuous (no missing timesteps)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77z2wz6xjop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ 3. TARGET VARIABLE CORRELATION & CLASS BALANCE ANALYSIS\n",
      "============================================================\n",
      "ğŸ”„ Gesture vs Sequence Type Relationship:\n",
      "                                   gesture sequence_type  count  percentage\n",
      "                             Text on phone    Non-Target  58462       10.17\n",
      "                            Neck - scratch        Target  56619        9.85\n",
      "                       Eyebrow - pull hair        Target  44305        7.71\n",
      "                        Forehead - scratch        Target  40923        7.12\n",
      "                  Forehead - pull hairline        Target  40802        7.10\n",
      "                     Above ear - pull hair        Target  40560        7.05\n",
      "                         Neck - pinch skin        Target  40507        7.05\n",
      "                       Eyelash - pull hair        Target  40218        7.00\n",
      "                        Cheek - pinch skin        Target  40124        6.98\n",
      "                                Wave hello    Non-Target  34356        5.98\n",
      "                         Write name in air    Non-Target  31267        5.44\n",
      "                 Pull air toward your face    Non-Target  30743        5.35\n",
      "Feel around in tray and pull out an object    Non-Target  17114        2.98\n",
      "                            Glasses on/off    Non-Target  13542        2.36\n",
      "                     Drink from bottle/cup    Non-Target  13093        2.28\n",
      "\n",
      "ğŸ“Š Behavior Phase Distribution by Gesture (Top 5):\n",
      "                 gesture                                  behavior  count  avg_timestamp\n",
      "     Eyebrow - pull hair                          Performs gesture  19549           53.9\n",
      "     Eyebrow - pull hair             Moves hand to target location  13295           41.1\n",
      "     Eyebrow - pull hair                   Hand at target location   6709           45.6\n",
      "     Eyebrow - pull hair Relaxes and moves hand to target location   4752            7.8\n",
      "Forehead - pull hairline                          Performs gesture  19862           48.4\n",
      "Forehead - pull hairline             Moves hand to target location   9114           18.9\n",
      "Forehead - pull hairline                   Hand at target location   7048           32.7\n",
      "Forehead - pull hairline Relaxes and moves hand to target location   4778            7.5\n",
      "      Forehead - scratch                          Performs gesture  19953           48.3\n",
      "      Forehead - scratch             Moves hand to target location   9219           22.1\n",
      "      Forehead - scratch                   Hand at target location   6947           32.7\n",
      "      Forehead - scratch Relaxes and moves hand to target location   4804            7.8\n",
      "          Neck - scratch             Moves hand to target location  23501           91.1\n",
      "          Neck - scratch                          Performs gesture  20008           73.1\n",
      "          Neck - scratch                   Hand at target location   7909           68.7\n",
      "          Neck - scratch Relaxes and moves hand to target location   5201            9.1\n",
      "           Text on phone             Moves hand to target location  21120           50.4\n",
      "           Text on phone                          Performs gesture  21066           75.2\n",
      "           Text on phone Relaxes and moves hand to target location   8917           15.0\n",
      "           Text on phone                   Hand at target location   7359           62.6\n",
      "\n",
      "âš–ï¸ Class Imbalance Analysis:\n",
      "Binary Classification (sequence_type):\n",
      "sequence_type  count  percentage\n",
      "       Target 344058       59.84\n",
      "   Non-Target 230887       40.16\n",
      "\n",
      "Multi-class Gesture Distribution (18 classes):\n",
      "                                   gesture  count  percentage  participants\n",
      "                             Text on phone  58462       10.17            81\n",
      "                            Neck - scratch  56619        9.85            81\n",
      "                       Eyebrow - pull hair  44305        7.71            81\n",
      "                        Forehead - scratch  40923        7.12            81\n",
      "                  Forehead - pull hairline  40802        7.10            81\n",
      "                     Above ear - pull hair  40560        7.05            81\n",
      "                         Neck - pinch skin  40507        7.05            81\n",
      "                       Eyelash - pull hair  40218        7.00            81\n",
      "                        Cheek - pinch skin  40124        6.98            81\n",
      "                                Wave hello  34356        5.98            81\n",
      "                         Write name in air  31267        5.44            81\n",
      "                 Pull air toward your face  30743        5.35            81\n",
      "Feel around in tray and pull out an object  17114        2.98            81\n",
      "                            Glasses on/off  13542        2.36            81\n",
      "                     Drink from bottle/cup  13093        2.28            81\n",
      "                     Scratch knee/leg skin  12328        2.14            81\n",
      "                         Write name on leg  10138        1.76            81\n",
      "                       Pinch knee/leg skin   9844        1.71            81\n",
      "\n",
      "ğŸ“ˆ Class Imbalance Metrics:\n",
      "  â€¢ Most frequent gesture: Text on phone (58,462 samples)\n",
      "  â€¢ Least frequent gesture: Pinch knee/leg skin (9,844 samples)\n",
      "  â€¢ Imbalance ratio: 5.9:1\n",
      "  â€¢ Macro F1 challenge level: MEDIUM\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 3. ğŸ¯ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®è©³ç´°åˆ†æ\n",
    "print(\"ğŸ¯ 3. TARGET VARIABLE CORRELATION & CLASS BALANCE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã¨sequence_typeã®é–¢ä¿‚\n",
    "print(\"ğŸ”„ Gesture vs Sequence Type Relationship:\")\n",
    "gesture_sequence_type = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        gesture,\n",
    "        sequence_type,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    GROUP BY gesture, sequence_type\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 15\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(gesture_sequence_type.to_string(index=False))\n",
    "\n",
    "# è¡Œå‹•ãƒ•ã‚§ãƒ¼ã‚ºã®è©³ç´°åˆ†æ\n",
    "print(\"\\nğŸ“Š Behavior Phase Distribution by Gesture (Top 5):\")\n",
    "behavior_phase = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        gesture,\n",
    "        behavior,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(AVG(sequence_counter), 1) as avg_timestamp\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    WHERE gesture IN (\n",
    "        SELECT gesture \n",
    "        FROM \"cmi_detect_behavior_with_sensor_data\".train \n",
    "        GROUP BY gesture \n",
    "        ORDER BY COUNT(*) DESC \n",
    "        LIMIT 5\n",
    "    )\n",
    "    GROUP BY gesture, behavior\n",
    "    ORDER BY gesture, count DESC\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(behavior_phase.to_string(index=False))\n",
    "\n",
    "# ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã®è©³ç´°è©•ä¾¡\n",
    "print(\"\\nâš–ï¸ Class Imbalance Analysis:\")\n",
    "\n",
    "# Binary classification (sequence_type)\n",
    "binary_balance = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        sequence_type,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".train), 2) as percentage\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    GROUP BY sequence_type\n",
    "    ORDER BY count DESC\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(\"Binary Classification (sequence_type):\")\n",
    "print(binary_balance.to_string(index=False))\n",
    "\n",
    "# Multi-class gesture distribution\n",
    "gesture_balance = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        gesture,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".train), 2) as percentage,\n",
    "        COUNT(DISTINCT subject) as participants\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    GROUP BY gesture\n",
    "    ORDER BY count DESC\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(f\"\\nMulti-class Gesture Distribution (18 classes):\")\n",
    "print(gesture_balance.to_string(index=False))\n",
    "\n",
    "# ä¸å‡è¡¡æ¯”ç‡ã®è¨ˆç®—\n",
    "max_class = gesture_balance['count'].max()\n",
    "min_class = gesture_balance['count'].min()\n",
    "imbalance_ratio = max_class / min_class\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Class Imbalance Metrics:\")\n",
    "print(f\"  â€¢ Most frequent gesture: {gesture_balance.iloc[0]['gesture']} ({gesture_balance.iloc[0]['count']:,} samples)\")\n",
    "print(f\"  â€¢ Least frequent gesture: {gesture_balance.iloc[-1]['gesture']} ({gesture_balance.iloc[-1]['count']:,} samples)\")\n",
    "print(f\"  â€¢ Imbalance ratio: {imbalance_ratio:.1f}:1\")\n",
    "print(f\"  â€¢ Macro F1 challenge level: {'HIGH' if imbalance_ratio > 10 else 'MEDIUM' if imbalance_ratio > 3 else 'LOW'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "nj20rd5q4hp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” 2. SENSOR DATA DISTRIBUTION ANALYSIS\n",
      "============================================================\n",
      "ğŸ¯ IMU Sensor Statistics:\n",
      "sensor  null_count  mean_val  std_val  min_val  max_val  median_val\n",
      " acc_x           0    1.6400   5.7813 -34.5859  46.3281      2.9727\n",
      " acc_y           0    1.7907   5.0039 -24.4023  27.1836      0.6953\n",
      " acc_z           0   -0.4598   6.0965 -42.8555  30.0781     -1.5625\n",
      "\n",
      "ğŸ”„ Rotation Quaternion Statistics:\n",
      "sensor  null_count  mean_val  std_val  min_val  max_val\n",
      " rot_w           0    0.3604   0.2257   0.0000   0.9994\n",
      " rot_x           0   -0.1199   0.4655  -0.9991   0.9998\n",
      " rot_y           0   -0.0600   0.5430  -0.9997   0.9995\n",
      " rot_z           0   -0.1883   0.5041  -0.9982   0.9999\n",
      "\n",
      "ğŸŒ¡ï¸ Thermopile Sensor Statistics:\n",
      "sensor  null_count  mean_val  std_val  min_val  max_val\n",
      " thm_1           0     27.08     3.23    -0.37    38.46\n",
      " thm_2           0     27.13     2.94    21.96    37.58\n",
      " thm_3           0     26.70     4.12     0.00    37.29\n",
      " thm_4           0     27.56     2.25    22.38    39.59\n",
      " thm_5           0     26.67     2.44    22.05    37.68\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 2. ğŸ” ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒåˆ†æ\n",
    "print(\"ğŸ” 2. SENSOR DATA DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# IMUã‚»ãƒ³ã‚µãƒ¼ã®åŸºæœ¬çµ±è¨ˆé‡\n",
    "print(\"ğŸ¯ IMU Sensor Statistics:\")\n",
    "imu_stats = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        'acc_x' as sensor,\n",
    "        COUNT(*) - COUNT(acc_x) as null_count,\n",
    "        ROUND(AVG(acc_x), 4) as mean_val,\n",
    "        ROUND(STDDEV(acc_x), 4) as std_val,\n",
    "        ROUND(MIN(acc_x), 4) as min_val,\n",
    "        ROUND(MAX(acc_x), 4) as max_val,\n",
    "        ROUND(PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY acc_x), 4) as median_val\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    WHERE acc_x IS NOT NULL\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'acc_y',\n",
    "        COUNT(*) - COUNT(acc_y),\n",
    "        ROUND(AVG(acc_y), 4),\n",
    "        ROUND(STDDEV(acc_y), 4),\n",
    "        ROUND(MIN(acc_y), 4),\n",
    "        ROUND(MAX(acc_y), 4),\n",
    "        ROUND(PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY acc_y), 4)\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    WHERE acc_y IS NOT NULL\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'acc_z',\n",
    "        COUNT(*) - COUNT(acc_z),\n",
    "        ROUND(AVG(acc_z), 4),\n",
    "        ROUND(STDDEV(acc_z), 4),\n",
    "        ROUND(MIN(acc_z), 4),\n",
    "        ROUND(MAX(acc_z), 4),\n",
    "        ROUND(PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY acc_z), 4)\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    WHERE acc_z IS NOT NULL\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(imu_stats.to_string(index=False))\n",
    "\n",
    "# å›è»¢ã‚»ãƒ³ã‚µãƒ¼ï¼ˆã‚¯ã‚©ãƒ¼ã‚¿ãƒ‹ã‚ªãƒ³ï¼‰ã®çµ±è¨ˆé‡\n",
    "print(\"\\nğŸ”„ Rotation Quaternion Statistics:\")\n",
    "rot_stats = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        'rot_w' as sensor,\n",
    "        COUNT(*) - COUNT(rot_w) as null_count,\n",
    "        ROUND(AVG(rot_w), 4) as mean_val,\n",
    "        ROUND(STDDEV(rot_w), 4) as std_val,\n",
    "        ROUND(MIN(rot_w), 4) as min_val,\n",
    "        ROUND(MAX(rot_w), 4) as max_val\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    WHERE rot_w IS NOT NULL\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT 'rot_x', COUNT(*) - COUNT(rot_x), ROUND(AVG(rot_x), 4), ROUND(STDDEV(rot_x), 4), ROUND(MIN(rot_x), 4), ROUND(MAX(rot_x), 4)\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train WHERE rot_x IS NOT NULL\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT 'rot_y', COUNT(*) - COUNT(rot_y), ROUND(AVG(rot_y), 4), ROUND(STDDEV(rot_y), 4), ROUND(MIN(rot_y), 4), ROUND(MAX(rot_y), 4)\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train WHERE rot_y IS NOT NULL\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT 'rot_z', COUNT(*) - COUNT(rot_z), ROUND(AVG(rot_z), 4), ROUND(STDDEV(rot_z), 4), ROUND(MIN(rot_z), 4), ROUND(MAX(rot_z), 4)\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train WHERE rot_z IS NOT NULL\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(rot_stats.to_string(index=False))\n",
    "\n",
    "# æ¸©åº¦ã‚»ãƒ³ã‚µãƒ¼ã®çµ±è¨ˆé‡\n",
    "print(\"\\nğŸŒ¡ï¸ Thermopile Sensor Statistics:\")\n",
    "thm_stats = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        'thm_1' as sensor,\n",
    "        COUNT(*) - COUNT(thm_1) as null_count,\n",
    "        ROUND(AVG(thm_1), 2) as mean_val,\n",
    "        ROUND(STDDEV(thm_1), 2) as std_val,\n",
    "        ROUND(MIN(thm_1), 2) as min_val,\n",
    "        ROUND(MAX(thm_1), 2) as max_val\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    WHERE thm_1 IS NOT NULL\n",
    "    \n",
    "    UNION ALL SELECT 'thm_2', COUNT(*) - COUNT(thm_2), ROUND(AVG(thm_2), 2), ROUND(STDDEV(thm_2), 2), ROUND(MIN(thm_2), 2), ROUND(MAX(thm_2), 2)\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train WHERE thm_2 IS NOT NULL\n",
    "    UNION ALL SELECT 'thm_3', COUNT(*) - COUNT(thm_3), ROUND(AVG(thm_3), 2), ROUND(STDDEV(thm_3), 2), ROUND(MIN(thm_3), 2), ROUND(MAX(thm_3), 2)\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train WHERE thm_3 IS NOT NULL\n",
    "    UNION ALL SELECT 'thm_4', COUNT(*) - COUNT(thm_4), ROUND(AVG(thm_4), 2), ROUND(STDDEV(thm_4), 2), ROUND(MIN(thm_4), 2), ROUND(MAX(thm_4), 2)\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train WHERE thm_4 IS NOT NULL\n",
    "    UNION ALL SELECT 'thm_5', COUNT(*) - COUNT(thm_5), ROUND(AVG(thm_5), 2), ROUND(STDDEV(thm_5), 2), ROUND(MIN(thm_5), 2), ROUND(MAX(thm_5), 2)\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train WHERE thm_5 IS NOT NULL\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(thm_stats.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dsndjmnwye4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” 1. COMPREHENSIVE DATA PROFILING\n",
      "============================================================\n",
      "ğŸ“‹ Dataset Structure:\n",
      "table_name  total_rows  unique_participants  unique_sequences  min_counter  max_counter\n",
      "     train      574945                   81              8151            0          699\n",
      "      test         107                    2                 2            0           55\n",
      "\n",
      "ğŸ§® Column Data Types and Non-null Counts:\n",
      "Total columns: 341\n",
      "  ğŸ“ Categorical columns: 8\n",
      "  ğŸ”¢ Sensor columns: 333\n",
      "    ğŸ¯ IMU sensors: 7 (['acc_x', 'acc_y', 'acc_z', 'rot_w', 'rot_x', 'rot_y', 'rot_z'])\n",
      "    ğŸŒ¡ï¸  Thermopile sensors: 5 (['thm_1', 'thm_2', 'thm_3', 'thm_4', 'thm_5'])\n",
      "    ğŸ“¡ ToF sensors: 320 (5 sensors Ã— 64 channels)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 1. ğŸ“Š åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°\n",
    "print(\"ğŸ” 1. COMPREHENSIVE DATA PROFILING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# åŸºæœ¬ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ç¢ºèª\n",
    "print(\"ğŸ“‹ Dataset Structure:\")\n",
    "structure_info = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        'train' as table_name,\n",
    "        COUNT(*) as total_rows,\n",
    "        COUNT(DISTINCT subject) as unique_participants,\n",
    "        COUNT(DISTINCT sequence_id) as unique_sequences,\n",
    "        MIN(sequence_counter) as min_counter,\n",
    "        MAX(sequence_counter) as max_counter\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".train\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'test' as table_name,\n",
    "        COUNT(*) as total_rows,\n",
    "        COUNT(DISTINCT subject) as unique_participants,\n",
    "        COUNT(DISTINCT sequence_id) as unique_sequences,\n",
    "        MIN(sequence_counter) as min_counter,\n",
    "        MAX(sequence_counter) as max_counter\n",
    "    FROM \"cmi_detect_behavior_with_sensor_data\".test\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(structure_info.to_string(index=False))\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿å‹ã¨éNULLå€¤ã®è©³ç´°ç¢ºèª\n",
    "print(\"\\nğŸ§® Column Data Types and Non-null Counts:\")\n",
    "column_info = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        column_name,\n",
    "        data_type,\n",
    "        is_nullable\n",
    "    FROM information_schema.columns \n",
    "    WHERE table_schema = 'cmi_detect_behavior_with_sensor_data' \n",
    "    AND table_name = 'train'\n",
    "    ORDER BY ordinal_position\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(f\"Total columns: {len(column_info)}\")\n",
    "\n",
    "# ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®åˆ—æ•°\n",
    "categorical_cols = ['row_id', 'sequence_type', 'sequence_id', 'subject', 'orientation', 'behavior', 'phase', 'gesture']\n",
    "sensor_cols = [col for col in column_info['column_name'] if col not in categorical_cols]\n",
    "\n",
    "print(f\"  ğŸ“ Categorical columns: {len(categorical_cols)}\")\n",
    "print(f\"  ğŸ”¢ Sensor columns: {len(sensor_cols)}\")\n",
    "\n",
    "# ã‚»ãƒ³ã‚µãƒ¼åˆ¥åˆ†é¡\n",
    "imu_cols = [col for col in sensor_cols if col.startswith(('acc_', 'rot_'))]\n",
    "thermopile_cols = [col for col in sensor_cols if col.startswith('thm_')]\n",
    "tof_cols = [col for col in sensor_cols if col.startswith('tof_')]\n",
    "\n",
    "print(f\"    ğŸ¯ IMU sensors: {len(imu_cols)} ({imu_cols})\")\n",
    "print(f\"    ğŸŒ¡ï¸  Thermopile sensors: {len(thermopile_cols)} ({thermopile_cols})\")\n",
    "print(f\"    ğŸ“¡ ToF sensors: {len(tof_cols)} (5 sensors Ã— 64 channels)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tbbteeni3z",
   "metadata": {},
   "source": [
    "# ğŸ§  CMI BFRB Detection - æœ¬æ ¼çš„EDA\n",
    "\n",
    "## ğŸ“‹ åˆ†æãƒ—ãƒ©ãƒ³\n",
    "\n",
    "### ğŸ¯ ç›®æ¨™\n",
    "- **ã‚³ãƒ³ãƒšæ¦‚è¦**: Body-Focused Repetitive Behaviors (BFRB) æ¤œå‡º\n",
    "- **è©•ä¾¡æŒ‡æ¨™**: 0.5Ã—(Binary F1 + Macro F1) \n",
    "- **ãƒ‡ãƒ¼ã‚¿**: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚»ãƒ³ã‚µãƒ¼æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ (50Hz)\n",
    "- **å‚åŠ è€…**: 81åã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã€18ç¨®é¡ã®ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼\n",
    "\n",
    "### ğŸ“Š è©³ç´°åˆ†æé …ç›®\n",
    "1. **ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°** - åŸºæœ¬çµ±è¨ˆé‡ã¨å“è³ªè©•ä¾¡\n",
    "2. **ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿åˆ†æ** - åˆ†å¸ƒã€å¤–ã‚Œå€¤ã€ãƒã‚¤ã‚ºç‰¹æ€§\n",
    "3. **ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°åˆ†æ** - ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã€ç›¸é–¢é–¢ä¿‚\n",
    "4. **æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ** - ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ§‹é€ ã€å‘¨æœŸæ€§\n",
    "5. **æ¬ æå€¤ãƒ‘ã‚¿ãƒ¼ãƒ³** - ã‚»ãƒ³ã‚µãƒ¼æ•…éšœã€ãƒ‡ãƒ¼ã‚¿å“è³ª\n",
    "6. **å‚åŠ è€…ç‰¹æ€§åˆ†æ** - äººå£çµ±è¨ˆå­¦çš„ç‰¹å¾´\n",
    "7. **ç‰¹å¾´ç›¸é–¢åˆ†æ** - ãƒãƒ«ãƒã‚³ãƒªãƒ‹ã‚¢ãƒªãƒ†ã‚£ã€ç‰¹å¾´é¸æŠ\n",
    "8. **ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åˆ†æ** - ã‚»ãƒ³ã‚µãƒ¼èåˆã®å¯èƒ½æ€§\n",
    "9. **CVæˆ¦ç•¥æ¤œè¨¼** - ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢ç­–\n",
    "10. **ç‰¹å¾´å·¥å­¦ææ¡ˆ** - ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜æ´»ç”¨\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ygt0djc9auc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUG DATABASE TABLES ===\n",
      "Connection status: <duckdb.duckdb.DuckDBPyConnection object at 0x7f1a9af49870>\n",
      "\n",
      "=== ALL SCHEMAS ===\n",
      "  - cmi_detect_behavior_with_sensor_data\n",
      "  - main\n",
      "  - playground_series_s5e7\n",
      "  - information_schema\n",
      "  - main\n",
      "  - pg_catalog\n",
      "  - main\n",
      "\n",
      "=== ALL TABLES WITH SCHEMA ===\n",
      "Found 7 tables:\n",
      "  - cmi_detect_behavior_with_sensor_data.test\n",
      "  - cmi_detect_behavior_with_sensor_data.test_demographics\n",
      "  - cmi_detect_behavior_with_sensor_data.train\n",
      "  - cmi_detect_behavior_with_sensor_data.train_demographics\n",
      "  - playground_series_s5e7.sample_submission\n",
      "  - playground_series_s5e7.test\n",
      "  - playground_series_s5e7.train\n",
      "\n",
      "=== SHOW TABLES ===\n",
      "Default schema tables: 0 found\n",
      "\n",
      "=== TESTING CMI SCHEMA ACCESS ===\n",
      "âœ… cmi_detect_behavior_with_sensor_data.train accessible: 574945 rows\n",
      "\n",
      "=== KAGGLE_DATASETS SCHEMA CONTENTS ===\n",
      "Error listing cmi schema tables: Parser Error: syntax error at or near \"FROM\"\n",
      "\n",
      "=== DATABASE FILE INFO ===\n",
      "âœ… Database file exists, size: 286,273,536 bytes (273.0 MB)\n"
     ]
    }
   ],
   "source": [
    "# Debug database tables issue\n",
    "print(\"=== DEBUG DATABASE TABLES ===\")\n",
    "\n",
    "# Check if connection exists\n",
    "try:\n",
    "    print(f\"Connection status: {conn}\")\n",
    "except NameError:\n",
    "    print(\"Connection not found, creating new one...\")\n",
    "    import duckdb\n",
    "    conn = duckdb.connect('/home/wsl/dev/my-study/ml/ml-note/kaggle_datasets.duckdb')\n",
    "\n",
    "# Check all schemas\n",
    "print(\"\\n=== ALL SCHEMAS ===\")\n",
    "schemas = conn.execute(\"SELECT schema_name FROM information_schema.schemata\").fetchall()\n",
    "for schema in schemas:\n",
    "    print(f\"  - {schema[0]}\")\n",
    "\n",
    "# Check all tables with schema\n",
    "print(\"\\n=== ALL TABLES WITH SCHEMA ===\")\n",
    "tables = conn.execute(\"SELECT table_schema, table_name FROM information_schema.tables WHERE table_type = 'BASE TABLE'\").fetchall()\n",
    "print(f\"Found {len(tables)} tables:\")\n",
    "for schema, table in tables:\n",
    "    print(f\"  - {schema}.{table}\")\n",
    "\n",
    "# Try SHOW TABLES in different schemas\n",
    "print(\"\\n=== SHOW TABLES ===\")\n",
    "try:\n",
    "    show_tables = conn.execute(\"SHOW TABLES\").fetchall()\n",
    "    print(f\"Default schema tables: {len(show_tables)} found\")\n",
    "    for table in show_tables:\n",
    "        print(f\"  - {table[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with SHOW TABLES: {e}\")\n",
    "\n",
    "# Try to access the CMI tables directly\n",
    "print(\"\\n=== TESTING CMI SCHEMA ACCESS ===\")\n",
    "try:\n",
    "    result = conn.execute('SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".train').fetchone()\n",
    "    print(f\"âœ… cmi_detect_behavior_with_sensor_data.train accessible: {result[0]} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error accessing cmi_detect_behavior_with_sensor_data.train: {e}\")\n",
    "\n",
    "# List all objects in kaggle_datasets schema\n",
    "print(\"\\n=== KAGGLE_DATASETS SCHEMA CONTENTS ===\")\n",
    "try:\n",
    "    result = conn.execute('SHOW TABLES FROM \"kaggle_datasets\".\"cmi_detect_behavior_with_sensor_data\"').fetchall()\n",
    "    print(f\"Tables in cmi_detect_behavior_with_sensor_data schema:\")\n",
    "    for table in result:\n",
    "        print(f\"  - {table[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error listing cmi schema tables: {e}\")\n",
    "\n",
    "print(\"\\n=== DATABASE FILE INFO ===\")\n",
    "import os\n",
    "db_path = '/home/wsl/dev/my-study/ml/ml-note/kaggle_datasets.duckdb'\n",
    "if os.path.exists(db_path):\n",
    "    size = os.path.getsize(db_path)\n",
    "    print(f\"âœ… Database file exists, size: {size:,} bytes ({size/1024/1024:.1f} MB)\")\n",
    "else:\n",
    "    print(\"âŒ Database file does not exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "gzao13pnms5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” CMI BFRB Detection Dataset - EDA\n",
      "==================================================\n",
      "ğŸ“Š Available tables in cmi_detect_behavior_with_sensor_data schema:\n",
      "  âœ… test\n",
      "  âœ… test_demographics\n",
      "  âœ… train\n",
      "  âœ… train_demographics\n",
      "\n",
      "ğŸ“ˆ Database size: 286.3 MB with 4 tables\n",
      "ğŸ¯ Target dataset: CMI Body-Focused Repetitive Behaviors Detection\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"ğŸ” CMI BFRB Detection Dataset - EDA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Connect to the DuckDB database\n",
    "conn = duckdb.connect('/home/wsl/dev/my-study/ml/ml-note/kaggle_datasets.duckdb')\n",
    "\n",
    "# Show tables from the correct schema\n",
    "print(\"ğŸ“Š Available tables in cmi_detect_behavior_with_sensor_data schema:\")\n",
    "tables = conn.execute(\"\"\"\n",
    "    SELECT table_name \n",
    "    FROM information_schema.tables \n",
    "    WHERE table_schema = 'cmi_detect_behavior_with_sensor_data'\n",
    "    ORDER BY table_name\n",
    "\"\"\").fetchall()\n",
    "\n",
    "for table in tables:\n",
    "    print(f\"  âœ… {table[0]}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Database size: {286.3:.1f} MB with {len(tables)} tables\")\n",
    "print(f\"ğŸ¯ Target dataset: CMI Body-Focused Repetitive Behaviors Detection\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "htdje7frgh6",
   "metadata": {},
   "source": [
    "## Key Dataset Findings\n",
    "\n",
    "### Data Size\n",
    "- **Train**: 574,945 rows across 81 participants (8,151 sequences)\n",
    "- **Test**: 107 rows across 2 participants (2 sequences)  \n",
    "- **No participant overlap** between train and test sets\n",
    "\n",
    "### Data Distribution\n",
    "- Average ~7,098 rows per participant in training\n",
    "- Average ~71 sequences per participant in training\n",
    "- Test set appears to be a small sample for submission format\n",
    "\n",
    "### Important Notes\n",
    "- This is a **time series** dataset with sequence structure\n",
    "- Need **GroupKFold by participant** to prevent data leakage\n",
    "- Large training set (~575k timesteps) suggests 50Hz sampling over multiple sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "m4mtsa81enc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA SIZE ANALYSIS ===\n",
      "Train data rows: 574,945\n",
      "Test data rows: 107\n",
      "Train demographics: 81\n",
      "Test demographics: 2\n",
      "\n",
      "=== PARTICIPANT ANALYSIS ===\n",
      "Unique participants in train: 81\n",
      "Unique participants in test: 2\n",
      "Participants appearing in both train and test: 0\n",
      "\n",
      "=== SEQUENCE ANALYSIS ===\n",
      "Unique sequences in train: 8151\n",
      "Unique sequences in test: 2\n"
     ]
    }
   ],
   "source": [
    "# Re-establish database connection for this cell\n",
    "import duckdb\n",
    "conn = duckdb.connect('/home/wsl/dev/my-study/ml/ml-note/kaggle_datasets.duckdb')\n",
    "\n",
    "# Data size and distribution analysis\n",
    "print(\"=== DATA SIZE ANALYSIS ===\")\n",
    "\n",
    "# Check row counts\n",
    "train_count = conn.execute('SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".train').fetchone()[0]\n",
    "test_count = conn.execute('SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".test').fetchone()[0]\n",
    "train_demo_count = conn.execute('SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".train_demographics').fetchone()[0]\n",
    "test_demo_count = conn.execute('SELECT COUNT(*) FROM \"cmi_detect_behavior_with_sensor_data\".test_demographics').fetchone()[0]\n",
    "\n",
    "print(f\"Train data rows: {train_count:,}\")\n",
    "print(f\"Test data rows: {test_count:,}\")\n",
    "print(f\"Train demographics: {train_demo_count:,}\")\n",
    "print(f\"Test demographics: {test_demo_count:,}\")\n",
    "\n",
    "print(\"\\n=== PARTICIPANT ANALYSIS ===\")\n",
    "\n",
    "# Unique participants\n",
    "train_participants = conn.execute('SELECT COUNT(DISTINCT subject) FROM \"cmi_detect_behavior_with_sensor_data\".train').fetchone()[0]\n",
    "test_participants = conn.execute('SELECT COUNT(DISTINCT subject) FROM \"cmi_detect_behavior_with_sensor_data\".test').fetchone()[0]\n",
    "\n",
    "print(f\"Unique participants in train: {train_participants}\")\n",
    "print(f\"Unique participants in test: {test_participants}\")\n",
    "\n",
    "# Check for overlap in participants between train and test\n",
    "overlap_check = conn.execute('''\n",
    "    SELECT COUNT(*) FROM (\n",
    "        SELECT subject FROM \"cmi_detect_behavior_with_sensor_data\".train \n",
    "        INTERSECT \n",
    "        SELECT subject FROM \"cmi_detect_behavior_with_sensor_data\".test\n",
    "    )\n",
    "''').fetchone()[0]\n",
    "\n",
    "print(f\"Participants appearing in both train and test: {overlap_check}\")\n",
    "\n",
    "print(\"\\n=== SEQUENCE ANALYSIS ===\")\n",
    "\n",
    "# Sequence counts\n",
    "train_sequences = conn.execute('SELECT COUNT(DISTINCT sequence_id) FROM \"cmi_detect_behavior_with_sensor_data\".train').fetchone()[0]\n",
    "test_sequences = conn.execute('SELECT COUNT(DISTINCT sequence_id) FROM \"cmi_detect_behavior_with_sensor_data\".test').fetchone()[0]\n",
    "\n",
    "print(f\"Unique sequences in train: {train_sequences}\")\n",
    "print(f\"Unique sequences in test: {test_sequences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ll1cpx7x8b",
   "metadata": {},
   "source": [
    "# CMI BFRB Detection - Exploratory Data Analysis\n",
    "\n",
    "## Dataset Schema Analysis\n",
    "\n",
    "### Data Structure Overview\n",
    "- **Train table**: Contains sensor data with target labels (behavior, gesture, phase)\n",
    "- **Test table**: Contains sensor data without target labels  \n",
    "- **Demographics tables**: Participant information (age, sex, handedness, physical measurements)\n",
    "\n",
    "### Sensor Features\n",
    "- **IMU sensors**: acc_x/y/z (accelerometer), rot_w/x/y/z (rotation quaternion) = 7 features\n",
    "- **Thermopile sensors**: thm_1 to thm_5 = 5 features  \n",
    "- **ToF sensors**: tof_1 to tof_5 with 64 values each (v0 to v63) = 320 features\n",
    "- **Total sensor features**: 332 per timestep\n",
    "\n",
    "### Target Variables (Train only)\n",
    "- **behavior**: Binary classification (BFRB vs non-BFRB)\n",
    "- **gesture**: Multi-class classification (specific gesture types)\n",
    "- **phase**: Multi-class classification (gesture phases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
